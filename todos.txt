[GENERAL]
1. Check if this works with Deno; 	

[v0.3]

REMINDERS [2.]: 
	1. You can document inside-functions and inside-classes!
	2. When the good-ol' inner JSDoc fails one after all [or when "shadowing" the old inner JSDoc], one can do: 

		const UnrecognizedFunction = whatever()

		/**
		 * I DOCUMENT, THEREFORE I AM! 
		 * @param {T} x - bababababa, i am a parameter
		 * ...
		 */
		export const ToBeRecognizedFunction = UnrecognizedFunction as (x: T) => TOUT


4. [Part of the docuementation effort] Add the JSDoc; 
	Let it be brief (very), and work like: 

		1. Contains a short (SHORT) description of the given class/method; 
			WITHOUT the arguments' types! 
		2. Contains a reference to the proper documentation; 
	
	The only benefit of actually having the thing is the ability to reference the "big docs"

6. FIX the 'tests/run.ts' to be able to run ONLY SPECIFIED LOCATIONS: 
	6.1. introduce a path-map [for names shortcuts used from command-line]; 
	6.2. allow multiple names to be used; 
		6.2.1. Write a simple parser for this format [use plain JavaScript - not the library itself]; 

5. Source code TODO-s: 
	16. Individually type ['function IGenerateAStream(): __T__'] EACH function for generation of classes for Stream-creation; 
		Good Example: LimitedStream

	14. FIX THE confusing EXPORTS: 

		Sometimes, one'll encounter artifacts during imports such as: 

			import {X} from "..."
			const A = X.A.A 

		Fix this, by means of using 'export *' instead of the 'export * as', in cases, when 'A' is the ONLY export 'A.A'; 

	17. USE the optional '.pos' property on EVERY possibility inside the method implementations (namely - passing to member-functions/predicates, if it's not there - it's just not there, if it is - the user gets a new feature);
	
	21. Fix the tests; 
		21.1. previous Pattern/ tests; 
		21.2. import tests 
		21.3. constants tests
		21.4. utils tests
		21.5. Stream tests

Order of TODO-elimination: 
	5.21.1-4

	A.3.8. 
	A.3.9.
	A.3.10.
	A.3.11. 
	
	5.17
	5.21.5
	A.1.1.
	A.3.1.1.
	A.3.1.2

	A.3.2.
	A.3.4.

	A.3.	
	6.

	[fix test compilation errors]
	[run the tests]

	B. 
	At the same time: 
		2. 
		5.16. 
		4. 
		
	C.
	
A. testing: 

	1. Changes to tests [for modules]: 
		1. Stream-s: Create a plain '.init' test; 
					All StreamClass-es have it implemented INDIVIDUALLY [due to different signatures - v0.4. won't have such an issue]; 
					Namely, this is a 'ReInitializationTest', which (fundamentally) looks to verify correctness of behaviour upon RE-INITIALIZATION of the given Stream; 
						It is done like: 

							1. '.init' is called with given test arguments [this sequence 1.-... is done several times, depending on the '.init' arguments]; 
							2. The entire test-suite is RE-DONE, but WITHOUT needing to create an instance...; 
								2.1. A different signature is given; 

							So, basically, one needs to use a ChainClassConstructorTest together with '.init'; 
							REFACTOR THIS [once again...]; 
		
		2. Stream-s: Allow for testing different '.init' signatures; 
			Also - expand the "generated suites" to include presence/absence of different properties (altogether - 1-16 different classes to test, depending on the definition); 

	3. Implement test suites: 
		3.1. Stream-s: 
			3.1.1. "WrapperStream"-combinations; 
				Thus, for instance, one has to test ALL of: 

					LimitedStream-NestedStream-InputStream;
					StreamTokenizer-LimitedStream-TreeStream;
					NestedStream-StreamTokenizer-InputStream;
						And more...
				
				FOR THIS, one needs to be able to do CompositionClassTests; 
				Introduce a new function 'classCompose', which does '(...classes) => trivialCompose(...classes.map(classWrapper))'
					[produces a function for creation of an instance that is the result of a chain (new X)<-(new Y)<-... of classes]

					The tests for them work THE SAME WAY as the top-level cases, BUT: 
						
						1. they have a different [dynamically given] constructor; 
						2. they check FOR EACH ELEMENT to have one of the permitted types; 

			3.1.2. Test Instances: 	
			
					3.1.4.1.0. Empty Stream instance
					3.1.4.1.1. Single-element Stream instance
					3.1.4.1.2. Two-element Stream instance
					3.1.4.1.3. Long Stream instance (>= 20 elements)
				3.1.4.2. There are optimization-specific benchmarks; 	
					3.1.4.2.1. In particular, IDENTIFY THE BOTTELNECKS IN DEFINITIONS [if there are any]; 
		
		3.2. Parser/
			Test Instances:
				3.2.1. There is a set of Stream-s, for which they're tested; 
				3.2.2. With appropriate Collection-s; 
				3.2.3. They branch [by 4.1. and 4.2.]: 
					3.2.3.1. The Empty Stream; 
					3.2.3.2. The One-element Stream; 
					3.2.3.3. The Two-element Stream; 
					3.2.3.4. The many-element Stream; 

				[Grouping by test suite type, for each type of parser:]	
				1. LayeredParserTest - itself
				2. TableMapTest - itself
				3. GeneralParserTest: 
					1. PatternEliminator
					2. PatternTokenizer
					3. PatternValidator	
					4. BasicParser; 
					5. SkipParser; 
						5.1. FixedSkipParser; 
							5.1.1. StreamParser; 
					6. StreamLocator; 
					7. StreamValidator; 
					8. PositionalValidator; 

		3.4. benchmarks/	
			TODO: TO ENSURE the usefulness of the 'imports' tests, use the '.'-imports here (meaning: 'import {X} from "main.js"; const {...} = X[...]...'); 	
				This (a number of samples) + the overall thoroughness of the 'imports/' testing process will ensure that every piece of the library is reachable as intended...; 

			4.4.1. Stack benchmarks: 
				4.4.1.1 NestedStream - find out how deep a given NestedStream must be in order for the "default" Stack to break; 
					Use as metric later
			4.4.2. Parse really long strings: 
				4.4.2.1. Try to parse some incredibly long strings;
					See the speed limits for different string sizes
			4.4.3. "Cached" TokenInstance vs "Non-Cached" TokenInstance: 
				4.4.3.1. Auto-generate a GIGANTIC tree in a simple syntax made entirely of TokenInstance-s, then do a version for BOTH cached and non-cached 
				4.4.3.2. Iterate through them [TreeStream]; Measure time; 
					NOTE: it is expected that the "cached" 'TokenInstance' will be A LOT faster to build/do; 
				
		3.8. IndexMap/
			Every single class/case has 2-3 different instances [depending on the need for thoroughness...]; 

			0. [After Everything else]: 
				IndexMap/
				[note: there's the "extension" cases... Write minimal tests for this...]; 

			1. LinearIndexMap/
			2. PersistentIndexMap/
			3. InternalHash/ 
			4. HashMap/ 
			5. FastLookupTable/
		
		3.9. utils/	
			1. Walk through the project tree, enumerating the 'utils.ts' files; 
				1. IndexMap/utils.ts [design - check]
				2. HashMap/utils.ts [design - check]
				3. PersistentIndexMap/utils.ts [design - check]
				4. Parser/utils.ts [design - check]
				5. Pattern/utils.ts [design - check]
				6. Token/utils.ts [design - check]
				7. TokenizablePattern/utils.ts [design - check]
				8. ValidatablePattern/utils.ts [design - check]
				9. Position/utils.ts [design - check]
				10. InputStream/utils.ts [design - check]
				11. StreamClass/utils.ts [design - check]
				12. Tree/utils.ts		 [design - check; VERIFY TESTS FIRST...]
				13. src/utils.ts		 [design - check]

		3.10. Tree/
			To test [classes.ts]: 
				1. Tree/
				2. TreeWalker/
		3.11. Position/
			To test [classes.ts]: 
				1. MultiIndex
				2. MultiIndexModifier
		
		3.12. Buffer
		
B. documentation (change/fix it...);
	B.1. Create new docs pages:
		B.1.1. Home: 
			B1.1.1. Motivation (write that the library was created/conceived primarily as a mean of refactoring of frequent parsing tasks/data-structures); 
			B1.1.2. "One good approach" principle (write that the library was made to have a single "intended" best approach for all the things that are possible to do using it); 
			B1.1.3. Examples [projects done using it]; 
			B1.1.4. Structure - explain how the library structure works (interfaces., methods., classes., utils.); 
			B1.1.5. Known issues (this lists known problems that the library currently has);
			B1.1.6. Terminology
				This mentions that the library has a distinct terminology [albeit, not very large], that is useful for understanding its concepts; 
				
				NOTE [1]: The importance of '.value' is: 

						1. It unifies a GREAT deal of interfaces with diverse meanings (prior: '.input')
						2. It unifies them well - as Pattern-s: 
							1. things that change the '.value' [Collection, Buffer, , et cetera]
							2. or output into the '.result', which is '.flush()'-able (EliminablePattern, ValidatablePattern, TokenizablePattern)
							3. or just keep the value as-is, for further usage (Token, EnumSpace/Buffer [encapsulated])
			B1.1.7. Common naming conventions: 'is', repeating util/method names ('fromPairsList', '.getIndex', '.add', ... so forth); 
		B1.2. Properties: 
			This describes individual properties (and methods!) that can ocurr on different classes; 
			ONLY describes the properties, methods that ocurr more than once in the library [and have the same meaning]; 
		B1.3. Classes: 
			Describes individual classes; 
			THEY ARE LISTED AS EXTENSION-HIERARCHIES; 

			B1.3.1. Describe abstract classes: 
				1.3.1.1. FlushablePattern; 
				1.3.1.2. BasicPattern; 
				1.3.1.3. StreamClass({...}); 
			
		B1.4. Class-Factories: 
			Describes functions for creation of classes (ex: StreamClass, IndexMap, HashMap); 
		B1.5. Interfaces; 
			Lists the library's interfaces. 
			They are described via an, extends-implements hierarchy [where classes that implement the interfaces are the lowest level elements of the tree (the "leaves")]; 
		B1.6. Usage Notes: 
			Minor notes for the library usage. 
					
				NOTE [1]: identifying NestedStream-s that are elements as a part of a recursively-defined StreamTokenizer; 
					To define this is (typically) trivial, via having '.type' of a given 'Stream' to be 'undefined', or 
						not a part of a given NestedStream's '.typesTable';
					
				NOTE [2]: 'DelegateValidatablePattern': 
					
					1. the '.validator' is expected to ALSO remove any non-'Type' elements out of the 'result'; 
					2. when calling '.validate', the 'result[0]' should be 'true' ONLY when the source IS validatable (i.e. not already determined valid - contains only the 'Type' elements); 
			
				NOTE[3]: StreamClass, .currGetter requires reliability; 
					'Reliability' of a '.curr'-getter in context of 'StreamClass' is: 

						The results of '.currGetter()' can be safely turned into a buffer and reused
					
					Relibility is a needed property of a getter to be usable; 
		B1.7. Utils: 
			Categorize the utils ('is'-functions, 'from'/'to'-functions [ex: fromPairsList, toInputStream], 'uni'-functions, and so forth...); 

	B.2 [while working on the concrete documentation]: 
		1. Patch up the type definitions: 	
			Pay PARTICULAR attention to the methods return values; 
			Those are especially likely to be mangled up, due to the way that the methods are defined...; 

	WHEN WRITING DOCUMENTATION, make a note that the '.hash' and '.extension' functions SHOULD BE ABLE TO HANDLE null-values! 
C. rewrite the CHANGELOG [too much has been altered since the "original" v0.3]; 
	C.1. Walk through the last commit's state of v0.2.1, noting changes and writing them down [destructive]; 
	C.2. Walk through the v0.3 changes, and write them down [constructive];

[v0.4]

1. STACK! 
	The library is not too stack heavy, BUT it does have an (ultimate) stack-limitation
		when working with NestableStream-s and the 'nested' util (generally, anything that is NON-FLAT);
	This is very much impossible to solve on its own; 

	SOLUTION: use the CPS style. 
		Re-implement library (whilst keeping the non-CPS version) in CPS, 
		with an infinite 'stack'; 

	! THIS WILL REQUIRE FOR ONE TO FIRST IMPLEMENT THE INFINITE CPS-BASED STACK LIBRARY!
	That one should: 
		0. Be written in TypeScript; 
		Include Types: 
		1. InfiniteStack; 
		2. CPSArray (this is, ultimately, a CPS version of JS array);

	As, CPS is (quite) time-costly to actually run (due to amount of stack usage [primary] + the stack unwinding + additional checks...),
		this sort of thing would be useful for applications where speed of little importance
			(that is: NOT things like apps/websites/interpreters, and so forth; Stuff like AOT-compilers, perhaps);

	HOWEVER: 

		1. As said previously, it IS quite costly to do CPS, so: 
			1.1. See, whether it's feasible from time-perspective (how much slower it is, whether it's possible, or whether it's way too slow); 
			1.2. If not feasible - abandon the whole CPS idea completely; 

			To find out - benchmark; Write a sketch for parsing a very nested expression [sufficiently nested to crash the current API] 
				using the Infinite-Stack-CPS approach, then - measure and decide...; 
	
2. New Streams/Parsers + some minor stuff/code-quality/minimalism: 

	2.1. bufferize(tree, size) - util

		Given a 'Tree', returns an array of values of size `size`: 

			{
				childNumber: number,
				value: any
			}

		The `size` is a fixed number >= 0, 'childNumber' is the number of children, ahead of current node in the array.
		To be used for creating a persistent linear data structure for working with the given AST. 
		Good for cases, when the number of items is known, and is large (part of the the "big input" optimizations); 

		Would permit user to store a '.state.size: Pattern<number>' property (Pointer(number)) on the ParserState-s, 
			for counting the size of the input in terms of nodes, then re-use it with 'bufferize', 
				and use the tree for evaluation; 

	2.2. TreeStream: create two new versions; 

		The 'TreeStream' (currently) is the 'LL' pre-order tree traversal algorithm Stream-implementation; 
		There should be ANOTHER - the 'LR' post-order tree traversal; 

		1. Rename the 'TreeStream' to 'TreeStreamPre', and create an implemnetation for the 'TreeStreamPost'; 
			This is (particularly) useful for some types of interpreters; 

			The Pre visits the node, then its children one-by-one starting from the beginning, then the siblings; 
			The Post visits first the children, then the siblings, only THEN the parent [when there are NO MORE children - then the parent is visited]; 

			Basically, Pre (parent-to-children) returns the nodes 'as they go' in the left-to-right traversal in the tree,
				whereas Post returns them in 'reversed' order: for the parent to return, children must all return first; 

			[Example usage: transform the binary '+' -- a + (b + c * (d + (k + f * r))) -> a + b + c * (d + k + f * r)]; 

		2. Create a 'TreeStreamSwitch', which would be capable of "choosing" [based off underlying IndexMap] 
			whether to give items in the 'Post' order (parent first - children after), 
				or the 'Pre' order (children first - parent after);
			
			This would be a class-factory; 

	2.3. Implement a 'level-by-level TreeStream': 
		It would get an initial given 'level' (tree root), walk through all of its '.children', 
			then walk each of the '.children' themselves as levels (one after another...), THEN descending lower...; 
		Would do it via calling '.getJointLayer' function (then - caching for later '.prev'); 
		This (basically) takes the nodes from the previous 'joint layer', gets their respective children,
			CONCATENATES THEM into a new layer, then iterates it; 

	2.4. Add a module SPECIFICALLY for working with strings/identifiers/Sequence-s (currently: the 'Indexed' interface); 
		In particular, functions/abstractions: 

			1. shorten(names); 
				Given a sequence of "names" (strings/sequences),
					and a 'comparison' predicate for each of its elements,
						it would implement a name-shortening algorithm, 
							that would preserve the name uniqueness;
				The return value is an IndexMap; 

			2. renameable(names);
				Given an IndexMap of names (which could come from, for instance, 'shorten'), 
					the method: 

					1. Re-orders;
					2. Creates new name-map entries;

				In a fashion that would allow straightforward implementation of a sequential "exact replacement algorithm" based on the output: 
					1. Loop through a list of name-maps; 
						2. [In the loop] Rename current encounters of a name with its mapped value;	
					
				There are 2 operations that may be necessary: 
					1. creation of temp-names (when collisions in present names are far too high to re-order);
					2. re-order; 
				
				The algorithm for the 'renameable' function: 
					0. Keep the cached (met) names in a 'Set'; 
					1. Loop through the 'names' given: 
						1.0. If the name is in the cached Set, continue; 
						1.1. If the current 'value' of the 'name' is already present amongst 'keys' (another loop): 
							1.1.1. put the keys in question BEFORE the current value;
							1.1.2. '.swap' the two indexes;
							1.1.3. go one position back (because now one needs to check the "others" now); 
							1.1.4. cache the current amongst the checked names in a 'Set', (so that one doesn't get into an infinite loop);

				This is useful when implementing things like mass renamings/name shortenings in code mangling software; 

	2.5. RELAX overly demanding interface and types definitions/requirements;
		Example: inputStreamCopy, inputStreamNavigate; 

			These do not NEED to have all the parts of the 'InputStream' definition (even though they are originally implemented to work with it);
			RE-DEFINE them...;

	2.6. [Idea?] Create an error-handling API for the 'v0.4';
		Create means of: 

			1. Stacking different levels of exceptions (the 'try-catch' blocks); 
			2. Assigning relevant debug information to them;

		This is (primarily) for the development process of the parsers; 
		
		Define a Catcher [rough sketch]: 

			function Catcher (info: DebugInfo, logger: Function) {
				return function (thing: Function, thisArg: any, args: any) {
					try {
						thing.call(thisArg, ...args)
					} catch (e) {
						logger(info)
						throw e // NOTE: THIS here is to represent DEPTH [as parsers can be VERY recursive indeed, it may be needed to eliminate the recursive errors on a case-by-case basis]; 
					}
				}
			}

		Although... This is rather general. Perhaps, better implement as a separate package; 
		Likewise, there'd be ways to count levels of recursion, and assign types to them [specialized signature]; 

	2.7. [maybe?] BreakableStream - a major generalization of the library's patterns: 
		Returns a class for producing Stream-s: 

		1. Have an '.input' (another Stream); 
		2. Has an 'Indexable' of 'BreakPattern's (they are objects) pre-given, which define how the Stream instances behave: 
			2.1. type: string - one of: 
				2.1.1. limit - works like LimitedStream; 
				2.1.2. nest - works like NestedStream; 
				2.1.3. halt - stops the stream [no more items after that]; 
				2.1.4. navigate - finds: 
					2.1.4.1. Either the next item [or the first item AFTER THE BEGINNING] that obeys a given property (PredicatePosition)
					2.1.4.2. Goes to a static position [relative or absolute]; 
					
					ALSO, has a boolean arg 'isRelative' (default: true, basically - whether the values for 'navigate' are to be handled absolutely or not...); 
				2.1.5. transform - applies a given transformation on the '.input' (calls a function); Returns the result;
			2.2. args: object - the way that the 'type' is resolved (equivalent of constructor-arguments/.init-arguments); 
			2.3. buffer: boolean | () => boolean - this one corresponds to whether the current value should be bufferized; 
				When a function is passed, it is called on the 'BufferizedStream' in question to get the value; 
		
		The result of matching 2. to the '.input.curr' is (effectively) evaluated to be the Stream's '.curr'; 

		This todo is a 'maybe?' because all the said things can (pretty much) already be done by the user using StreamTokenizer and the rest
			- this really only just provides a minor abstraction over the whole thing (frees the user from needing to use the exports of the library); 
		This kind of thing is (somewhat) against the library's "all-minimialism" approach; 

		On the other hand, it would permit one to use a very domain-specific language to describe operations on streams conventionally, 
			thus - simplifying semantics + unifying a very large number of different abstractions that are creatable via the library using just this one; 	
		Conclusion: think about it, more of a YES, tha a NO currently;
	
	2.8. [reminder] Optimize minitua (that is, just go through the code, removing unnecessary repetitions of things, useless work and so forth...); 
		When the same (even tiny) operations get repeated a large number of times, the speed decay accumulates; 
		Try to make the library code that avoids this stuff fundamentally; 

		2.11.1. DO ACTIVE MICRO-BENCHMARKING!
			Having general/particular implementations in mind, MICRO-BENCHMARK, 
				and on the basis of those, optimize particular functions/approaches; 
	
	2.9. utility - 'nestedInfo'; 

		1. Finds a depth of a given Stream, based off 'inflate' and 'deflate'; 
			NOTE: the MAX depth achievable; 
		2. Finds its length - how many positions (in 'number') must one walk before the current nestedness ends, from the initiated position; 

		Returns it as a pair; 

	2.10. Idea for a method: ProlongedStream.prolong; 	

		This (utilmately) modifies the 'this.streams' [modifying the '.isEnd' accordingly]; 
		Also, there'd be '.shorten(n: number)' - it'd delete a certain number of elements from '.stream'-s, 
			optionally changing '.curr', and set '.isEnd = true' (if the .curr is in one of the deleted 'Stream'-s); 
	
	2.11. [Maaayybe?] Create a 'samples' directory; 
		This is for 'common-case' parsing - generally, walk through the various syntaxes that were 
			PRESENT within the previous parsers, use them?
		
		Or, better, make this into a separate mini-project called 'parsing-samples'; 
		[EXAMPLE: handling the 'escaped' strings/sequences all the time];

		Think about it [this particular refactoring is VERY fine and case-specific, though, it can come in useful...]; 

		No, if one is to do it, the thing should: 

			1. Work with general parsing patterns, that would be CONFIGURABLE for every case; 
			2. Provide low-level Token-s with specialized given names; 

	2.12. IDEA: generalize the type of 'T<K> = CollectionLike<K | T<K>>' properly; 
		It appears at least in 2 places inside the library: 

			1. Tree (InTreeType)
			2. NestedStream;
		
		This might (also) resolve some 'interface'-related issues for the 'NestedStream'; 
	
	2.13. v8-specific optimizations: 
		Having done some benchmarking on some typical Array methods, one arrived at a...
	
		CONCLUSION: 
			1. Handwritten methods are *often* __MUCH__ faster than the builtins ON LARGE PIECES OF DATA; 
				On the more common-place cases, however, they are inferior in time performance; 
			2. THEREFORE, one should do: 
				Re-organize the library (how?) to permit usage of the current methods on LARGE pieces of data; 
				Prior - do more benchmarking; 

				About re-organization: 
					1. Think - whether to introduce explicit low-level 'size-controls' [possibly costly, when run a lot]; 
						Although, it's likely to be optimized/negligible; 
					2. Or, whether to split the thing onto 2 chunks [where the 'size' is precomputed...]; 
						This is less flexible (and thus, error-prone), plus will be more difficult to re-structure; 
					
				Current vote is for 1.; 

				ALSO, about bounds - those should be CAREFULLY BENCHMARKED; 
		
		MORE GENERALLY - try to adapt the library for work on LARGE datasets; 
		It'll make it useful for working with big projects/pieces-of-text; 

		THINGS OF ISSUE TO THINK ABOUT IN PARTICULAR: 	

			1. What "LARGE" sizes are practical? [meaning - sources of what sizes can occur in the wild?]
				Take them as large as one can. At least 10000000 symbols (~30MB, ~100000 lines of 90-100 chars), maybe try more; 
			2. How much of a performance difference can one get by doing these optimizations?
				In particular - try to sketch out a parser for something (the smtf format of one's own? it's simple to parse), perform 
					profiling on a REALLY big auto-generated file, then: 
						
						1. take overall time measurements (performance.now()); 
						2. profile, break the execution down on several tiny pieces; Then - benchmark and optimize them accordingly; 
							For this, use results from samples from one's benchmarks library;  
			3. Optimize for smaller sizes ONLY when it doesn't impair the memory severely; 
				When it impairs it at all, choose whether or not to do it based off the chosen memory/speed difference measurements...; 
				
				3.1. FOR THIS [maybe] - have specialized "Heuristics" constant-space in 'constants.ts'; 
					3.1.1. IF doing them - create the heuristics for DIFFERENT ENGINES; 
					3.1.2. IF oding them - make the 'Heuristics' only the "default value" for transition of sizes; 
						THE USER must be able to set their own boundries; 
					3.1.3. IF DOING THEM - check how to "bounrdies" for the increase in efficiency when using 
						the custom implementation "shifted" as the versions of Node progressed - TRY IT with different node, v8 
							versions; 
					3.1.4. IF DOING THEM - other engines to test: 
						3.1.4.0. V8 (Chrome)
						3.1.4.1. SpiderMonkey (Firefox)

					[maybe?] Try building the engines from nil, running the benchmarks for heuristics standalone...; 
	
	2.14. [maaayyybe??] Bring the old planned stuff for v0.3 back; 
		That is the 'Parsers/utils.ts' - 'transform', 'delimited', 'consume' [equivalent of 'LimitedStream', uses 'Collection'-s]; 
	
		Do only if there are code aspects suffering from this significantly: 

			1. readability/accesibility; 
			2. performance; 
		
		Primary cause for this todo is that, despite being "slow" (memory allocation), 
			they did still have the benefit of semantic simplicity; 

		The only cause for performance issues with OOP Stream-s API is overhead from re-structuring the original loop so much
			[need for calling the '.isCurrEnd()' two times more]; 
		Again, do this ONLY if the performance difference is SIGNIFICANT!
	
	2.15. util - enumerateTree; 
		Given an array of "types" of nodes, it recursively converts their properties (given in the 'shapes' array), 
			to single-array-siblings form (the current ChildrenTree); 	
		This would permit a more "variable" set of trees (those that have properties with fixed names), 
			to be enumerated accordingly...; 

		Example: 

			{
				a: any
				b: any
				c: any
			}

		Becomes: 

			{
				value: [
					// ... a, b, c
					// ... or b, a, c
					// ... or any other order
				]
			}
		
	2.16. About generics - relax them; 
		Use ONLY WHEN NEEDED; 
		For example: a method makes use of X generics, out of which Y < X is needed [id est, used somewhere beyond the 'this: ...' in the parameters list]; 
			REMOVE the unneeded ones; 
			RE-ORDER THEM, if need be; 
			Try to make them more minimal...; 

	2.17. [maybe?] Create a module for randomized Stream-generation (`random`);
		(given, say, an InputStream with an Array of expected values inside of it [that can be picked for Nested], 
			it will choose a pair of them for different [distinct] boundries); 
		Similarly - one could give an Array of limiting characters for a LimitedStream to be defined by...; 
	
	2.18. New type of Tree - a BacktrackableTree, takes up more memory, but is (ultimately) faster to use in TreeWalker [when going backwards inside the Tree]; 
		In the 'TreeWalker.renewLevel', it does the 'this.stream.pos.slice', which is O(n) complexity relative to 'n' - number of levels in a tree; 	
		A 'BacktrackableTree' [interface] is one that has a method: 

			1. backtrack(pos: number)

		Which returns the 'pos'-parent [that being, goes "up" the level in the tree]; 
		Make the old 'Tree' into the 'BacktrackableTree', create a 'ParentTree' [implementation], where 'backtrack' is defined as: 

			backtrack = (pos: number) => {
				let result = this
				while (pos--) result = result.parent
				return retult
			}
		
		Which is O(1) to get the immidiate parent, and O(p) relative to the number of levels to go back (`p`); 
		For this to work (the '.parent' property), the Tree must be CONSTRUCTED in a certain fashion; 
		Add more items to the interface: 

			1. pushChildren(children: any[])
			2. clearChildren() 
		
		The 'ParentTree' implementation would (therefore) assign '.parent = this' on all the pushed children in '.pushChildren', and do 'this[propName] = []' in 'clearChlidren'; 
		[IMPORTANT!: Make a 'ParentTree' implementation work with the 'ChildrenTree'-s, that is it'd have its own 'propName']; 

		THEN, SPLIT the 'renewLevel' onto 2 functions [one of which uses the 'this.stream.curr' (in '.goPrevLast'), and the other - that uses '.backtrack(1)']; 
			Better still - for optimization purposes, let it CHOOSE which implementation to use DEPENDING on the number of parent levels and pre-parent levels;

	2.19. Add a fast implementation of '.finish' to the 'TreeStream'; 
		When called for the first time - it SAVES the "last index", THEN - re-uses the computed one on any subsequent '.finish()' calls; 
	2.20. Re-structure the tests to be made on a per-method basis? 
		Current per-class suites are rather verbose (don't do this until v0.4, already spent too much time on the tests...); 
	
	2.21. RE-INTRODUCE the '.copy()' method and 'Copiable' interface; 
		Comes inside the 'StreamClass' (optionally) via the 'hasState' property;
		Will additionally be supported by: 

			1. 'StreamClass(...).prototype.copy()' method; 
				Solution for '.copy': 
					1.1. By default, copy ONLY: 
						1. the user-defined variables under '.state'; 
						2. the '.pos', '.buffer', and so forth - predefined "inherited" properties;
							Any CUSTOM copying/initialization logic can be defined by the user in DERIVED CLASSES; 
				
				Definition [loose, has to have overloads, depending on presence/absence of '.pos']: 

					function uniCopy (stream: ...) {	
						const copy = new (Object.getPrototypeOf(stream).constructor)()
						copy.uniInit(initSignature(this, this.bits))	
						return copy
					}

				NOTE: here, 'this.bits' is the '.prototype'-level BitArray with different properties that ARE PRESENT; 
					Look below for pseudocode explanation

		Then, the various 'Indexable'-s can use the '.state' inside the '.index' call; 

		ALSO [regarding '.init']: 
			[Predefined] things like '.value', '.buffer', '.pos' and so-forth, DO NOT become '.vars'; 
				To simplify the process, store (on each StreamClass), a 'BitArray' - to indicate, which are present; 
					Depending on this - the user will be able to call the '.init' with "universal" signature, like so: 

						// note: stored on the '.prototype', no memory waste
						stream.uniInit(initSignature(this, this.bits))
					
					The 'initSignature' is a util, returning the "maximum" '.init'-signature, that is possible; 
					The '.uniInit' is a method that initializes the given Stream BASED OFF THE "maximum" signature; 
					This version of 'initSignature' is "from-bits", one that isn't is done via an object like: 

						stream.uniInit({
							state: this.state, 
							vars: this.vars, 
							...
						})

	2.22. For future: improve the 'constructor' tests. 
		Refactor them, permit the ability to properly test the '.copy()'-ed instances using the constructor
			tests (namely - the "prototypeProps" and "ownProps" thing - missing currently); 
			
	2.23. 'regex' - allow for methods for creation of regular expressions that use the extended regex-syntax (enabled by the 'v' flag); 
		Namely: 
			1. intersection: 	/[[...]&&[...]]/v
			2. subtraction: 	/[[...]--[...]]/v
			3. string literals: /[q{...}]/v
		
	2.24. [prototypes, generality] make all the prototype 'value's (in particular - methods) OVERRIDABLE! 
			via doing: 

				Object.defineProperty(_class.prototype, {	
					methodName: { value: ..., writable: true }
				})
			
			Currently, they are ALL non-overridable; 
			This will enable the user to more liberally use the library classes [they are, presently, highly single-purposed (intentionally)]; 
	
	2.25. Create a new 'IndexMap' implementation: 'SetMap'; 
		This would be a: 
			1. IndexMap interface implementation; 
			2. generalized wrapper around a Set (in a way that HashMap is a wrapper around Object and Map); 
				2.1. Would have its own 'extension'; 
				2.2. For 'values' would contain true/false; 

		This is useful for working with cases that require the 'or'-predicate; 
		Example: 'nested' utility; 
		How it is done [pseudo-code]: 

			const SpecialCaseSetMap = SetMap(...)
			const processNested = (x) => nested(TableMap(new SpecialCaseSetMap([...])), TableMap(new SpecialCaseSetMap([...])))(new WhateverStream(x))
		
	2.26. Add the 'TravelableStream' interface, with a '.travel()' method. 
		It would exist to remedy the: 

			while (pos--) stream.next()

		lines of the 'uniNavigate'; 
		For some streams (example: InputStream), this can be easily replaced with: 
			stream.pos += pos
		Which is a single instruction; 

		CURRENTLY, this is precisely what the 'InputStream' is doing, HOWEVER, because of this, 
			the '.navigate' as an interface part has become rather useless
				(in the sense that it is not centralized); 
		Then: 

			1. InputStream.navigate() [and similar 'deviating' cases] would be re-implemented to use ABSOLUTE position instead; 
			2. 'uniNavigate' will become 'uniTravel' (and 'navigate' default-method of StreamClass will be renamed into 'travel'); 
				2.1. '.navigate' will be re-implemented on 'StreamClass' by default (via '.rewind() + .travel()'), for 'ReversedStreamClass'-cases; 
			3. 'skip' will call the '.travel' on the given stream (instead of using the 'uniTravel', as it does now); 
		CONCLUSION: 'skip' and 'has' utilities optimized, interfaces unified; 

	2.27. [memory] Cache stuff related to StreamClass -- dynamic creation of classes [EVERYTHING...]: 
			0. [add to 'one.js'] the 'multiCached(props: string[])(object: object)' function: 
				0.1. Based off an object: 
					Takes in an object, with predefined list of properties; 
					This'll be using NESTED 'Map'-s; 
						Meaning, that 'i'-th property in the 'keyList', would have the 'i'-th-depth Map; 
			1. 'multiCache' everything:
				1.1. StreamClass [in terms of given signature objects]; 
				1.2. Functions that call the StreamClass for BaseClasses [the 'InputStreamBaseClass', ... others]
				1.3. The GeneratedStream-functions for creation of classes (based off THE SAME sub-signature...); 
				1.4. the 'baseStreamInitialize' function (that chooses the appropriate definition for the '.init' method...); 

				This may be important, because - when writing libraries, the user may want to override some 
					of the functionality (for instance - add/remove '.bufferized', '.pos'), and end up creating A NEW 
						class; 
						This will cause a slowdown [in a long chain, or when done a sufficient number of times - a noticeable one...]; 

	2.28. refactor the 'imports' tests; 
		1. more use of 'specificChildImports'
		2. the 'prefixedNames' + 'namesCapitalized' - appears lots more than once, take out; 
		3. Make better use of the 'namesCapitalized'; 

	2.29. [important] Replace the 'Parser/' functionality completely with special cases of 'StreamTokenizer': 
		Because: 
			34.1. '.result' can be replaced with '.curr'; 
			34.2. '.finished' can be replaced with '.isEnd'; 
			[as '.next']
			34.3. '.parser' can be defined in terms of '.input/.sub'; 
			34.4. '.change' can be defined in terms of '.input/.sub'; 

		The only interesting thing about 'GeneralParser' (that can't be directly replaced), is its level of dynamicity: 
			the '.next', '.isEnd' of the 'StreamTokenizer' can't be quite
				so easily (explicitly) modified from within '.baseNextIter', '.isCurrEnd', so forth, 
					precisely BECAUSE of the underlying behaviour; 
				
		SOLUTION: 
			1. get rid of the 'GeneralParser'; 
			2. replace its implementations with cases of 'StreamTokenizer'; 
			3. allow '.writab'-ility for the StreamClass properties; 
				3.1. add an optional '.configure' method, for changing them (interface: ConfigurableStream); 
					Works in a similar fashion to '.init'; 
			4. RELOCATE those of 'Parser/'-classes that are (currently) implemented via GeneralParser-functions and ARE Stream-s, 
				into 'Stream/'. Let ONLY the Parser-s that aren't that remain: 

					[currently]
					1. TableMap
					2. LayeredParser
					[ALSO - move these three under TokenizablePattern/, ValidatablePattern/ and EliminablePattern/!]
					3. PatternTokenizer 
					4. PatternValidator
					5. PatternEliminator	
			5. MERGE the 'TableMap' and 'LayeredParser' into the 'src/classes.ts'
			6. DELETE the 'Parser/' directory altogether; 
				The final project top-level tree would look like: 

					1. IndexMap
					2. Pattern [this includes the new stuff for working with Indexed-based Pattern-s - Sequence-s]; 
						AND this includes 'GeneralBuffer' (as they are also 'Pattern'-s); 
					3. Position
					4. regex
					5. Stream
					6. Tree
				
		34.5. The ability to work with several '.stream'-s can be replaced with a utility [pseudocode]: 

			[NOTE 1: ADD this to the library]; 
			function streamSwap(stream: (whatever) BasicStream, i: number) {
				const swapped = stream.state.streams[i]
				stream.state.streams[i] = stream.input
				stream.init({
					input: swapped
				})
			}	
	
	2.30. Add the 'export default' from 'classes.ts' and 'interfaces.ts' files for their "main" stuff; 
		Example [1]: a 'classes.ts/interfaces.ts' with THE ONLY export will have it as default, besides referencing by name; 
		Example [2]: when a 'classes.ts/interafaces.ts' has an export with THE SAME name as the module, the export is default (besides being present as of itself); 

	2.31. turn the 'AssignmentClass'-es into abstract classes
				[ones that aren't redundant - remember the 'replace .input with .sub' 
					and '.state is an optional StreamClass property' todos...]; 
				
	2.32. [large input optimizations] Investigate whether retaining the indexes information from 'matchAll' in 'tokenizeString' may not be faster on large inputs; 
		This is largely a question of how fast '.split' is compared to building the array manually using the indicies information from 'matchAll' (that is, a large sequence of '.slice()'-es); 
		Potentially, this could be a large time saving (if '.slice()'-es are relatively short), but otherwise - could be a waste of memory (additional strings allocated without need); 
			Then, of course comes the question of whether the strings are allocated at all; 
			BENCHMARK IT; 
	
	2.33. Introduce a new Position class - MultilinePosition
		Basically, a pair of numbers: 
			(linesNum, symsNum)	
		Create a generalization of it (where 'newline' is a context-defined term, like a particular "separation token", or something...): 
		Which correspond to prior number of newlines and prior number of symbols; 
		Used frequently throughout for things like debug information; 
		The '.convert()' would play very nicely with it...; 

	2.34. [clarity, refactoring] Create a type 'TypePredicate<T> = (x: any) => x is T';  Use all over the place...; 
		Appears in: 
			1. Tokenizable; 
			2. structCheck; 
			[and some other places...]

	2.35. refactor the '(x: any, y: any) => boolean' comparison into a 'Comparison' type; 
		This is (most often) is used inside the library's testing mini-framework; 

	2.36. About validation and parsing: 
		This is a somewhat worrysome part of the library, currently, as:

			1. Validation happens separate from parsing, despite using the same abstraction (ex: ValidatablePattern and TokenizablePattern); 
			2. '1.' causes the entire process of 'validate + parse' to take 2x the time (need another loop); 
		
		[SOLUTION 1] In simpler cases (such as when a grammar is FULLY covered by a PatternTokenizer), the user can do this themselves: 
			
			1. Create a HashMap based off '.type'; 
			2. Create a PatternValidator; 
			3. Walk the PatternValidate-d string in a StreamTokenizer defined via the TableParser based off HashMap given; 
			4. If HashMap DOES NOT manage to match a thing - it's a string, therefore - not covered, therefore - invalid; 
				4.1. Then - throw an exception/otherwise handle the error; 

			This is highly natural, and requires no additional abstractions; 
			Update the documentation to include it;

		[SOLUTION 2] Still, for more complex cases (in particular - those that require a per-Stream-element identification - this will not suffice); 
			For this reason, a TODO: 

				Create a new abstraction that would encompass BOTH parsing and validation; 
				Granted, it would take more time than a simple parsing procedure, 
					but (and this is CRUCIAL) it must take LESS time than split parsing + validation (in exchange for increased memory usage); 
				
			This is already doable (manually) using 'StreamTokenizer', but one would love to make it a part of the library (general abstractions + interface + algorithm); 
			From the noted above, 'PatternTokenizer' and 'PatternValidator' are a no-go here 
				(potentially, an infinite set of possible valid/invalid expressions with convoluted structure); 
			It should be a case of 'StreamTokenizer'; 

			Ideas: 
				0. It is a StreamClass (a configurable class of StreamTokenizer-s, more precisely); 
				1. Let stream elements be '<OutType>[valid, token]: [boolean, OutType]'; 
				2. Contains a field of '.validator', which returns '[boolean, OutType]', where 'OutType' is the UPDATED token (based off current one and 'input'); 
					return value is, then: 
						
						return this.validator(this.handler(this.input), input)

		[SOLUTION 3] It integrates best with current API (although, still carries the downsides of SOLUTION 1); 

			All it requires is to "save" the Fully Tokenized outputs somewhere, separately from the 'ValidatablePattern'; 
			It requires SPLITTING the Tokenization and Validation procedures; 

			Downside is - takes longer to run for invalid inputs + all of the ones from Solution 1
				[only really deals with coverage + closed/well-defined expressions of the 'beginning/end' kind]; 

		CONCLUSION: 

			1. Do SOLUTION 3 [split 'tokenizeString' from 'validateString']; 
				Let THE USER do it (in particular - use the 'validateTokenized' themselves); 
				Then - there is NO RE-DOING THE ALREADY DONE WORK; 

				The reasons one IS NOT keeping the old solution: 

					1. Code clarity [it's not very general]; 
					2. Code uniqueness [avoiding duplication]; 
				
				NOTE: the 'validateString' STILL remains, only it's standalone from now on; 
			
			2. Do SOLUTION 2 [a simple new StreamTokenizer]
		
	2.37. Later - improve the tests: 
		2.38.1. Add generics to the class-tests signatures [just for the sake of it - very pretty and give a lot more sense to using TypeScript for tests]; 
		2.38.2. Refactor the 'while(times--) doTestSmth(...)' into a special 'times' function; 
		2.38.3. Create a way to print out the class tests' instances (via a special 'instance' library function, based off 'it'); 
	
	2.38. Implement a new interface for the library - FiniteEnum: 
		This, basically, creates an 'EnumSpace', that consists of all the given items, 
			BUT requires finding out their uniqueness beforehand (using the 'new Set', or the 'one.js' 'norepetitions'). 
			Thus, for instance, these are equivalent: 

				const a = new FiniteEnum(["800", "elorian", "800", 998])
				const b = new FiniteEnum(["800", "elorian", 998])
			
			Thus, the '.size' of the given EnumSpace is actually fluid; 
			Likewise, its' '.add()' would NOT be a number (hence - one NEEDS A NEW INTERFACE); 

	2.39. IDEA: how to speed up the tokenization algorithm [attempt]:
		Way: 
		1. Split the "tokenized" and "untokenized" parts with 2 arrays
		2. Keep the 'indexes' of the tokenized (on a per-item basis) and untokenized (on a "linear" - read below - basis) parts
		3. Split the final result (tokenized) into BATCHES	
			3.1. The underlying is a NEW structure called 'BatchArray'; 
				It keeps its values as a list of lists, which HAVE THEIR INDEXES; 
				The second-level lists are all ordered as the original array is; 

				Operations: 
					
					0. bind(array) - binds the given array
					1. newBatch() - creates a new last batch, to have values written into
					2. merge() - combines the batches into a single array, removing the indexes; 
						For this - use Merge-sort (it's a natural choice here - the structure is precisely what the algorithm requires); 
						Due to quality of data, the complexity (in this case) is O(nlog k) [considerably better than 'O(nlog n)'];
					3. replace(i, subarr, subinds) - replaces a given index with a value of 'subarr'
						This has to be fast, therefore: 

							1. Store the '.bound' NOT as 'any[]', but as a special 'ContinuedElement[]'; 
								It keeps elements as "referenced" (ContinuedElement), and 
									items that are referenced can have: 
										1. a '.value' (when no array is used); 
										2. an '.array' (when one is used, otherwise - null); 
									
								Thus, the replacement-with-array operation becomes O(1);

							2. When this happens, after changing the 'ContinuedElement' one: 

								2.1. Finds the batches that have the affected elements; 
									To make it fast (for general case), store batches NOT just as 'any[]', 
										but also have '.minIndex' and '.maxIndex' properties; 
									Thus, it's possible to determine (in a single check) whether 
										they are affected ('.maxIndex < .changedIndex')
										in O(1) time; 
								2.2. Finds the elements inside the batches; 
									Done via binary search over the indexes; 
									Then - as they are linearly ordered, walks the remainder of the batch-list, 
										doing 2.3.; 
								2.3. Changes the elements; 
									Add the 'length-of-the-replaced-subarray - 1'; 
									O(1) already, no need to change anything; 

								IMPORTANT: for this to be FAST (algorithm becomes O(k^2 logn) and NOT O(nlogn)), 
									one needs to do this step is done in a SEPARATE 
							
							3. When 'replacement' occurrs recursively (inside an existing ContinuedElement): 
								3.1. It's the same, only difference is - one has to calculate the index manually (which can be mildly cumbersome, but is still O(n)); 
							
							4. The '.indexes' list, then, is ALSO altered; 
								The old 'untokenized' index is "deleted", new UNTOKENIZED items' indexes (subinds) are put in its stead; 
								It is ALSO kept as a 'ContinuedElement[]'; 
								When an index is JUST deleted (that is, it's tokenized wholy and singl-y), 
									one replaces the value with 'null' (no '.splice' necessary); 

								To find it, one has to: 

									1. Take the first 'ContinuedElement' x: 
										1.1. Increase the value by (if array has no) 'x.array.length', or (if no '.array' is used, just '.value') 1
											To make checking for presence of a 'ContinuedElement' inside 'ContinuedElement' faster - add a flag-property ('isRecursive: boolean'); 
											BETTER IDEA: 
												Use a 'BitArray' (a new data structure - a UInt8Array/UInt32Array, whatever, that keeps flags as bits); 
												ANOTHER IDEA: 
													Use a 'BitArray' to store a number [together with a boolean]; 
													This way, it can have less bits, and more appropriate for a given usage ("cram it" with the flags - these go first); 
													Then, one can also KEEP TRACK of the first 'ContinuedElement' [helps, when it's > 0]; 

													ANOTHER number stored - count of recursive 'ContinuedElement'-s [allows to sometimes finsh early]; 
													IDEA: 
														Keep not the FIRST 'ContinuedElement', but the one that is THE FARTHEST! 
														Then, what we have is - the largest possible speed increase; 
														This also requires an ADDITIONAL number getting stored: separate BitArray (for the index that is skipped); 
														Remainder are linearly walked-through; 

														BETTER IDEA STILL: 	
															Store it upon the 'CountedElement'-s that are using '.value'; 
															THEN - there is no need for the flag, as one ONLY works with 'the next' value; 

										1.2. Proceed until the happenstance that it becomes equal to the sought-after index, OR 
											the difference between the current one, and the next becomes "too far" to be used as an intermediate-point; 
											From there - one does a literal jump (as there are no more "nested" indexes left); 
										
									Therefore, our '.indexes' is capable of getting the current index at about O(k), where 'k' is the table size (number of tokenizations); 

									IMPORTANT: the '.indexes' is actually A 'ContinuedElement' ITSELF; 
										Thus, all techniques can be used on it also...; 
						4. indexSync() - see '3.->2.->IMPORTANT'; 
						5. getIndex(prevIndex: number): number - for iteration, see 'Fields->2.'; 

				Fields: 

					1. .bound: ContinuedElement[] - the array of items; 
					2. .indexes: number[] - the list of indexes that are NON-REMOVED [used for iteration]
						IDEA: instead of keeping it as a list of numbers, use 'BitArray'-s, with a changing memory outfit; 
						This permits to (greatly) economy the memory, because to see if an index is "present" one just 
							marks it as 1/0; 
						The "nested" ones will require more memory; 

						(
							Possibly... One could save *some* memory with a complex bit-layout?
							Thus, for instance, having, 01 mean 'non-nested, present', 00 - 'non-nested/nested, missing', 
							'1' - 'nested' [only a single (first) bit used]; 

							Also - there'd be another - 'GrowingNumber' (new library data structure; this time - a single-element UInt8Array/UInt16Array/UInt32Array as a base, not UInt8Array); 
							This one'll keep a single number; NOTE: the thing is needed at all even because of the '2^53 - 1' number-bound (namely, that one doesn't like it...)
							This is (basically) a wrapper around a number to allow for quick manipulations that are ALSO memory-efficient

							'Nested' ones will USE THE SEPARATE INDEXES inside the '.indexes: (...)[]' array; 

							Then - one'll be able to have ANOTHER method on this, called 'getIndex'; 
							It'll return the next non-missing iteration index, based off the current one; 
						)

						Also - regarding the 'automatic' empty return from 'tokenization' function - this won't work here (too much per-element overhead); 
						Conclusion - one has to check the return-array length FROM THE START; 

						PROBLEM: consider this - '.indexes' becomes PARTITIONED! 
							More importantly - a given portion now becomes more about "missing" than "present" 
								non-nested bits; 
							SOLUTION: reverse the bits inside the given 'nested' bit; 
							To determine WHEN to do it - count the "missing" and "present" parts; 

					3. .batches: [any, number][] - the list of indexed elements inside the array
					4. .syncInfo: [number, number, number] - the list of REPLACEMENTS that has been done; 
						Used for increasing speed (namely - DELAYING the slow index-sync operation); 
						Has form of '[batchHappen, replacementLength, replacementIndex]'
						
						1. Because one KNOWS that for all things that are AFTER the 'batchHappen', the index is "fixed", we also know
							that only 'batchHappen' batches need be checked at all - O(k^2), where k^2 is the size of the table; 
						2. Because one KNOWS that there are '.minIndex' and '.maxIndex' for a given batch (and it's ordered), 
							one can find out whether one has the desired changed indexes - O(1)
						3. Because one has the indexes as NUMBERS, one can use binary search - O(log n)

						IDEA: for an even BETTER optimization - to have better memory consumption, a more complicated structure: 

							1. STORE THE INDEX-ADDITION INFO on a per-batch-basis' (buckets)
							2. Store them in a linearly ordered fashion (id est, so as to PRESERVE the values...); 
						
							QUESTION: how much will this book-keeping piece cost? (the 'number' triple only requires O(1))
								To do this, however, one is in need of: 

									1. get the triple; 
									2. put it into one of the 'batch-baskets' of the '.syncInfo' (based off 'batchHappen'); 
									3. in the chosen basket - '.push([index, length])'; 

									[post-creation, when calling '.sync']
									4. later - get the value from the appropriate bucket (they are ITERATED from 0 to 'curr-batch-index' for '.batches'); 
									5. apply all the transformations to the given batch; 
								
							ANSWER: not much in terms of performance; 
								ALSO - it's actually better in terms of cache-locality (because there's less jumping around); 

							ALSO: the two numbers are kept as GrowingNumber-s! 
		Result [theory]: 
		1. O(n) additional memory usage (where 'n' - number of final tokens)
		2. O(1)-O(n) performance gain (depends on number of tokenized items, least waste in cases when there's a lot)
			The current "speedy" O(n^2) (O(n) - walk through the items; O(n) - the '.splice' in replace due to index-keeping) 
				can be replaced with a "slowish" (meaning - one with a possibly significant factor) O(n).
			This ability is important, and in certain applications can be considerable; 
			Only questions are: 

				1. When can this be used? (use-cases)
				2. [Most important] Can use-cases be identified prior? 
			
			Generally, for: 

				1. Large inputs; 
				2. Large (> 15 items) tokenization-tables; 
			
			This could make some sense. 

				1. The '.splice' requires one to go SEVERAL TIMES through the 
					ALREADY TOKENIZED elements (the reason for inner O(n))
				2. The 'isType' requires performing redundant checks; 

			In expense of memory and: 

				1. Book-keeping cost (a noticeably more complex algorithm that runs in O(n) linear time + lots of index-keeping); 
			
			this implementation would permit to remove these; 	

			Final performance is: 

				O(nk^2log n), where 'n' - number of tokenized elements, k - table size

		CONCLUSION [post-benchmarking]: 
			1. After some little fiddling with benchmarks, one found out that (actually), 
				'.splice' is optimized to be extremely fast; 
			2. One'll need to check the optimization difference betwee this and '.splice'
				(very well may be that this turns out to be slower regardless)
			3. (Theoretically), the '.splice' STILL copies the array, even though it happens on a 
				MUCH lower-level (C++, Asm); IF it's faster (which is quite likely, indeed), 
					that is only due to the purely numeric part; 
			
			4. Important: 
				(From what is known insofar) The only "really" slow part of the algorithm, 
					that genuinely would ruin the relative performance to repeated '.splice' (which, for a large enough input would take literally forever...), 
						is the "setting up". 

				Namely, the process of "guessing" the number of 'nested' indexes [their tree]; 
				One would be required to "grow" it accordingly, with a relatively few size-increases for it...; 

			5. Before adding ANY memory-saving data structures 
				1. Benchmark for memory; 
				2. Benchmark for speed; 

				Because if either is absent, the whole thing could simply collapse due to speed/memory assumptions going horribly wrong; 

			6. FINAL: if it's slower, REMOVE; 
				First - implement and benchmark performance for some large inputs (see if faster at all, likely not); 
				Second - see the 'time-difference/memory-difference' ratio, see if it's worth it (very likely not); 
				Third - either delete, or keep; 

	2.40. Performance concern: '.push(...elems)' FAILS for large arrays; 
		Problem - with the '...elems' spread operator: it creates a new array; 

		1. Do '.push()'-es in batches, or avoid them completely....; 
			[Idea: use ContinuedList-s instead? - new data structure
				Same thing as 'ContinuedElement', but without the '.value' to store, 
					and that references ANOTHER ContinuedList, instead of just an array]; 
		2. Better idea - where possible, use "mixed signatures"
			(example: 'string[] | ContinuedList | string[][]' for 'StringCollection.push'); 
			Then - the methods in question have a higher versatility; 

	2.41. BitArrayHash; 
		Accepts a BitArray, returns a value from it; 
			A hash based off bit-shifts (<<, >>); 
		Use for defining the '.init', '.curr' and other choice functions in the StreamClass; 

	2.42. [TypeScript; clarity] Add the 'type ForcePresence<T, K extends keyof T> = {[x in K]-?: T[]}'
		This (and similar ones), for adding/removing various type features, are handy for refactoring (appears at least in StreamClass); 
		ALSO: 

			1. Doing this COULD allow one to be switching from the "composite interfaces" to polymorphic ones with optional properties; 
				(Even create a SINGLE RIGID 'Stream' interface! [NOTE: the OLD ones AREN'T GOING ANYWHERE - they're just getting re-formulated...]; 

					interface Stream<Type = any> extends Summat {
						pos?: number
						buffer?: FreezableBuffer<Type>
						state?: Summat
						prev?: () => Type
						isStart?: boolean

						init?: (...x: any[]) => Stream<Type>
						navigate?: (pos: Position) => Type
						finish?: () => Type
						rewind?: () => Type

						curr: Type
						next: () => Type
						isEnd: boolean
					}

					Plus, one COULD start using the OPTIONAL per-element interfaces [then, using the 'ForcePresence' with a "stricter" interfaces]; 
					This, in particular givest the benefit of being able to recognize properties on a "lower" type WITHOUT requiring an 'as' (no significant type downgrading); 

					2.49.1. ALSO  - create a single RIGID 'StreamClass' interface! [for the implementation - write down the optional '.pos, .buffer', and so forth... Missing currently...]; 
				)
	
	2.43. Use C preprocessor with this code; 

		Getting tired of typing "optionalValue(this, value)", and other such things in repetative situations (like 'init.ts'); 
		Abstract those away with a good old '#define' + '#include'; 
			Then - introduce C preprocessor tools to the project's build; 
				The build process, then, would then be : 

					1. steps: 
						prep/ (with-preprocessor dir, via 'make') -> ts/ (pure TypeScript) -> dist/ (pure JavaScript)
					2. extension: 
						change from '.ts', to stop the VSCode from doing its thing; 
					3. create a basic theme for this "mixed" TypeScript + C-preprocessor thingie [VSCode extension idea]; 
						For syntax highlighting

		Due to library's (intended) reliability in therms of name-usage, it ouhgt to be ok; 
		ALSO - do this only AFTER everything else in the library was ALREADY DONE! 
		Due to the fact one doesn't want to touch its code again after the v0.4, this particular "hybrid" 
			language shouldn't become a thing to cause problems [that would need separate solving] during development; 

[DEFINITELY, for v0.4!]
3. Dependency management: 
	3.1. [maybe???] Create a 'refactor' library, specifically designed to refactor common design patterns; 
		In particular, it would have: 	

			1. 'delegate' functions [from here]; 
			2. the 'classWrapper' function; 
			3. the 'AssignmentClass' function; 
			4. A 'PropertyDescriptor' class; 
				4.1. This one is for creation of 'Object.defineProperty' signature-objects (the "property descriptors"); 
			5. The 'Constructor' type; 
			6. SelfAssignmentClass; 

	3.2. All the routine writing of tests made one think whether it may not be more desireable to 
		automate even the process of their WRITING; 

		Create a library like this containing: 

			1. a path-resolver (for relative/absolute imports); 
			2. a JS code-generator (based off the 'parsers.js' + the JS parser of one's own...); 
			3. various functions for configurable code-template creation; 
			4. functions for recursive directory-traveral, automated file-creation, types for specifying a directory structure; 
		
		The combination of the three will permit A LOT of different code to just be "magicked away"; 
		USE THOSE to "re-write" ["pack", the .ts files will be generated (.gitignore-d) - then run], 
			the tests/their designs for 'parsers.js' v0.4 [the repetative parts]; 