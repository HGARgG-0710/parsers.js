[v0.3]

REMINDERS [2.]: 
	1. You can document inside-functions and inside-classes!
	2. When the good-ol' inner JSDoc fails one after all [or when "shadowing" the old inner JSDoc], one can do: 

		const UnrecognizedFunction = whatever()

		/**
		 * I DOCUMENT, THEREFORE I AM! 
		 * @param {T} x - bababababa, i am a parameter
		 * ...
		 */
		export const ToBeRecognizedFunction = UnrecognizedFunction as (x: T) => TOUT

	3. Fix the 'tests/constants.ts' [defaults]; 
	
4. [Part of the docuementation effort] Add the JSDoc; 
	Let it be brief (very), and work like: 

		1. Contains a short (SHORT) description of the given class/method; 
			WITHOUT the arguments' types! 
		2. Contains a reference to the proper documentation; 
	
	The only benefit of actually having the thing is the ability to reference the "big docs".

5. Source code TODO-s: 
	16. Individually add return types ['function IGenerateAStream(): __T__'] for EACH function for generation of classes for Stream-creation; 
		Good Example: LimitedStream

	17. USE the optional '.pos' property [Stream] on EVERY possibility inside the method implementations 
		(namely - passing to member-functions/predicates, if it's not there - it's just not there, if it is - the user gets a new feature);

	2.52. work on Stream: 
			Quite obviously, one is to work FURTHER on the 'StreamClass'; 
			In particular, 

				1. StreamClass/interfaces.ts - refactor types via 'Pick', 'Partial' and 'Omit': 
				2. Add the 'LocatorStream' (and other SPECIFIC interfaces, to indicate where and which optional flags are present): 
					These are just 'A & B'-type expressions [marker interfaces, mostly]... ;
					IDENTIFY them with the respective classes/functions
				3. OptBufferized, OptPos - I'M LOOKING AT YOU...
					These can be re-done via 'Partial' - been looking for something of this nature, hadn't dfound previuosly; 
	
	2.62. PROBLEM - with LayeredFunction: 

		TAKEAWAY: add another abstraction OVER IT, that would permit one to pass 
			a 'new (e: EndableStream, state: Summat) => StreamParser', which would [then], 
				be used to create the StreamParser instance USING: 

					1. the 'input' [dynamically given, as first argument]
					2. the initial 'state' [static - later can be changed from inside 'table']

				PROBLEM: consider doing 'StreamParser()' WITH A '.buffer'?! 
					Then - INSTEAD, create a function [separate - put in one.js/functional]: 	

						// the gist is: 
						1. precompute = (f, indexes: Set<number>) => (...values: any[]) => (...x: any[]) => {
							const concatted = []
							for (let i = 0, vi = 0, xi = 0; i < f.length; ++i) 
								concatted.push(indexes.has(i) ? values[vi++] : x[xi++])
							return f(...concatted)
						}

						[ALSO - include into 'one.js' the results of refactoring]: 
						2. propByName (Stream/refactor.ts) - an IMMENSELY useful function, would go well into 'functional'; 
						3. classWrapper (src/refactor.ts)  - very key for the library in particular - SADLY, it falls outside its ontology..., needs to be put someplace else; 
						[GENERALLY - walk through the 'refactor.ts' files and CHOOSE which should go into the new version of 'one.js']

					[ADD THIS TO DOCS!!!]
					Then, one can do: 
						
						const layers = new LayeredFunction(...)
						const singlePrecompute = precompute(new StreamParser(...), new Set([1, ... (other inds)]))
						layers.layers = [singlePrecompute({ parser: layers })]

					Presence of this in one.js would be a PERFECTLY good excuse for keeping 'LayeredFunction' HERE (for the moment, at the very least... - one did plan to refactor it out somewhere, is needed for other porjects as well...); 

		OLD NOTES:

		1. It is (really) just a modifiable/recomputable composition of functions; 
			Too general for the library as it currently stands; 
		2. One needs to specialize it - how? 
			Largest problem with it is that it's INCAPABLE of working with StreamParser,
				because: 

					1. self-modification requires storing a reference on NECESSARY LEVELS of 'StreamParser'; 
					2. 'LayeredFunction' ONLY takes functions that accept 1 argument, so 'classWrapper(StreamParser(table))' WON'T DO (accepts 2 - 'input', 'state'); 
				
				One could, of course, do this: 

					const layered = LayeredFunction(A, ..., (input) => StreamParser(table)(input, { parser: layered })); 

				CONCLUSION: add a new function for [generally] doing '(f, n, v) => (...values: any[]) => f(...insert(values, n, v))'; 
			
				Problem then becomes - 'layered' has to be given BEFORE all this is done...; 
				Also - one ought to be able 
				So, what one actually has to do is: 

					const layered = new LayeredFunction()
					layered.layers = [A, ..., ((v) => (input) => StreamParser(table)(input, v))({ parser: layered }), ...]
				
				Then, the 'table' would have a 'function', that would in certain circumstances access the 'this.state.parser', and CHANGE IT; 

	[LATER - during testing...]
	2.66. Cases of "Bad Types" [to fix]: 
		1. DefaultType - for IndexMap-types: 
			It's pretty important, as it determines about half the return types of the type's methods. 
				Put those up! [in templates, and as a dependency of *EVERYWHERE*]
		2. 'this'-returning: 
			Stop with the ': any'-nonsense, or overly-specific-this-return-type-nonsense. 
			Just do ': this', AS INTENDED...
	
	2.68. refactor: 
		1. 'super: {value: x.prototype}' - create a special 'defineSuper' function inside a 'refactor.ts' file...; 

	2.69. PROBLEM [performance] - StreamClass\methods\curr.ts: 

		1. posBufferGet

			For this: 
				
				0. Note in the 'UnfreezableArray' docs that AFTER '.unfreeze()' is called, the unerlying Stream behaviour *DOES NOT CHANGE* (meaning - one has to create a NEW Stream...); 
				1. UNITE the "fast path" of 'posBufferGet' with 'next-prev.ts'

	2.71. file-pile: 
		Clean up the project root directory: 
			1. bunch together the 'ts-types.json' and 'ts-config.json' (change respective paths in "scripts" in package.json); 

	2.72. A new abstraction OptimizedCharMap ('HashMap' and 'OptimizedLinearIndexMap' extension): 
		Uses the '(string, i) => string.charCodeAt(i)', 
			FOR THIS TO WORK: 
				1. add multiple arguments: 

					1. HashMap.index(x, ...)
					2. LinearIndexMap.index(x, ...)
					3. TableMap.index(x, ...)
					4. MapWrap.index(x, ...)

				2. Create a function 'streamInfo = (f) => (x: Stream) => f(x.buffer., x.pos, x.state, x)', and so forth. 
					REASON: the '.extend' would be REALLY hard/unperformant to make work witih multiple arguments the way that '.index' does. 
					Applying 'streamInfo' on a 'TableMap' would yield desired results. 

	2.73. Do research (V8 performance): https://floitsch.blogspot.com/2012/03/optimizing-for-v8-introduction.html
		SPECIFICALLY - *inlining*. The library assumes its usage HEAVILY to remain efficient. 

	2.74. Do research (Chevrotain - source, capabilities, optimizations): 
		It is *the* fastest JS parsing library out there. See what it can do...:

			1. [basics] http://chevrotain.io/docs/tutorial/step0_introduction.html
			2. [important!] http://chevrotain.io/docs/guide/introduction.html

			[Post parsers.js v0.3; ADD this as parsers.js v0.4 todo...]; 
			3. [yes, that thing also...] https://github.com/Chevrotain/chevrotain/blob/master/packages/utils/src/to-fast-properties.ts#L2
			4. [building v8] d8 - USE IT to profile the optimizations of the used library code [reflect upon, and change the library respectively]
			5. [Stream-specific] see if the '.curr/.next/.prev'  method re-binding optimizations were an overkill [elimination of 'this.isEnd = true/false/whatever-for-each-call']; 
	
Order of TODO-elimination: 
	5.2.69. 
	5.2.72.
	5.2.68. 
	5.2.71.
	5.2.52. 
	5.2.62.
	5.16.
	5.17.
	5.2.73.
	5.2.74.	
	
	TESTS [work all anew... too much has changed]: 
		1. enumerate the things to test; 
		2. walk through tests, verify them for being satisfactory; 
		3. re-do where necessary, add new cases; IMPLEMENT MISSING;
		4. re-do the 'import' tests as well; 
		5. log the 'run.ts' thingie into a special temp text file; 
			1. [make into an npm command - 'test-run' and 'test: test-compile + test-run']
			2. add to '.vscode/launch.json' [to simplify the debugging process... we'll need it]; 
			3. WORK on the failed tests from there...; 
	
	2.66. 

	[fix test compilation errors]
	[run the tests]

	B. 
	At the same time: 
		2. 
		5.16. 
		4. 
		
	C.
	
B. documentation (change/fix it...);
	Hosted on the project's GitHub Wiki. 
	WHEN WRITING DOCUMENTATION, make a note that the '.hash' and '.extension' functions SHOULD BE ABLE TO HANDLE null-values! 
	
C. rewrite the CHANGELOG [too much has been altered since the "original" v0.3]; 
	C.1. Walk through the last commit's state of v0.2.1, noting changes and writing them down [destructive]; 
	C.2. Walk through the v0.3 changes, and write them down [constructive];

[v0.4]

0. Check if this works with Deno; 	

1. STACK! 
	The library is not too stack heavy, BUT it does have an (ultimate) stack-limitation
		when working with NestableStream-s and the 'nested' util (generally, anything that is NON-FLAT);
	This is very much impossible to solve on its own; 

	SOLUTION: use the CPS style. 
		Re-implement library (whilst keeping the non-CPS version) in CPS, 
		with an infinite 'stack'; 

	! THIS WILL REQUIRE FOR ONE TO FIRST IMPLEMENT THE INFINITE CPS-BASED STACK LIBRARY!
	That one should: 
		0. Be written in TypeScript; 
		Include Types: 
		1. InfiniteStack; 
		2. CPSArray (this is, ultimately, a CPS version of JS array);

	As, CPS is (quite) time-costly to actually run (due to amount of stack usage [primary] + the stack unwinding + additional checks...),
		this sort of thing would be useful for applications where speed of little importance
			(that is: NOT things like apps/websites/interpreters, and so forth; Stuff like AOT-compilers, perhaps);

	HOWEVER: 

		1. As said previously, it IS quite costly to do CPS, so: 
			1.1. See, whether it's feasible from time-perspective (how much slower it is, whether it's possible, or whether it's way too slow); 
			1.2. If not feasible - abandon the whole CPS idea completely; 

			To find out - benchmark; Write a sketch for parsing a very nested expression [sufficiently nested to crash the current API] 
				using the Infinite-Stack-CPS approach, then - measure and decide...; 
	
2. New Streams/Parsers + some minor stuff/code-quality/minimalism: 

	2.1. bufferize(tree, size) - util

		Given a 'Tree', returns an array of values of size `size`: 

			{
				childNumber: number,
				value: any
			}

		The `size` is a fixed number >= 0, 'childNumber' is the number of children, ahead of current node in the array.
		To be used for creating a persistent linear data structure for working with the given AST. 
		Good for cases, when the number of items is known, and is large (part of the the "big input" optimizations); 

		Would permit user to store a '.state.size: Pattern<number>' property (Pointer(number)) on the ParserState-s, 
			for counting the size of the input in terms of nodes, then re-use it with 'bufferize', 
				and use the tree for evaluation; 

		If the 'size' is unknown, implements a simple DFS (namely - conversion of a given 'Tree' into an 'Array')

	2.2. TreeStream: create two new versions; 

		The 'TreeStream' (currently) is the 'LL' pre-order tree traversal algorithm Stream-implementation; 
		There should be ANOTHER - the 'LR' post-order tree traversal; 

		1. Rename the 'TreeStream' to 'TreeStreamPre', and create an implemnetation for the 'TreeStreamPost'; 
			This is (particularly) useful for some types of interpreters; 

			The Pre visits the node, then its children one-by-one starting from the beginning, then the siblings; 
			The Post visits first the children, then the siblings, only THEN the parent [when there are NO MORE children - then the parent is visited]; 

			Basically, Pre (parent-to-children) returns the nodes 'as they go' in the left-to-right traversal in the tree,
				whereas Post returns them in 'reversed' order: for the parent to return, children must all return first; 

			[Example usage: transform the binary '+' -- a + (b + c * (d + (k + f * r))) -> a + b + c * (d + k + f * r)]; 

		2. Create a 'TreeStreamSwitch', which would be capable of "choosing" [based off underlying IndexMap] 
			whether to give items in the 'Post' order (parent first - children after), 
				or the 'Pre' order (children first - parent after);
			
			This would be a class-factory; 

	2.3. Implement a 'level-by-level TreeStream': 
		It would get an initial given 'level' (tree root), walk through all of its '.children', 
			then walk each of the '.children' themselves as levels (one after another...), THEN descending lower...; 
		Would do it via calling '.getJointLayer' function (then - caching for later '.prev'); 
		This (basically) takes the nodes from the previous 'joint layer', gets their respective children,
			CONCATENATES THEM into a new layer, then iterates it; 

	2.4. Add a module SPECIFICALLY for working with strings/identifiers/Sequence-s (currently: the 'Indexed' interface); 
		In particular, functions/abstractions: 

			1. shorten(names); 
				Given a sequence of "names" (strings/sequences),
					and a 'comparison' predicate for each of its elements,
						it would implement a name-shortening algorithm, 
							that would preserve the name uniqueness;
				The return value is an IndexMap; 

			2. renameable(names);
				Given an IndexMap of names (which could come from, for instance, 'shorten'), 
					the method: 

					1. Re-orders;
					2. Creates new name-map entries;

				In a fashion that would allow straightforward implementation of a sequential "exact replacement algorithm" based on the output: 
					1. Loop through a list of name-maps; 
						2. [In the loop] Rename current encounters of a name with its mapped value;	
					
				There are 2 operations that may be necessary: 
					1. creation of temp-names (when collisions in present names are far too high to re-order);
					2. re-order; 
				
				The algorithm for the 'renameable' function: 
					0. Keep the cached (met) names in a 'Set'; 
					1. Loop through the 'names' given: 
						1.0. If the name is in the cached Set, continue; 
						1.1. If the current 'value' of the 'name' is already present amongst 'keys' (another loop): 
							1.1.1. put the keys in question BEFORE the current value;
							1.1.2. '.swap' the two indexes;
							1.1.3. go one position back (because now one needs to check the "others" now); 
							1.1.4. cache the current amongst the checked names in a 'Set', (so that one doesn't get into an infinite loop);

				This is useful when implementing things like mass renamings/name shortenings in code mangling software; 

		Put identifier, and Indexed-related stuff there (organization, overall tidyness); 

	2.5. [Idea?] Create an error-handling API for the 'v0.4';
		Create means of: 

			1. Stacking different levels of exceptions (the 'try-catch' blocks); 
			2. Assigning relevant debug information to them;

		This is (primarily) for the development process of the parsers; 
		
		Define a Catcher [rough sketch]: 

			function Catcher (info: DebugInfo, logger: Function) {
				return function (thing: Function, thisArg: any, args: any) {
					try {
						thing.call(thisArg, ...args)
					} catch (e) {
						logger(info)
						throw e // NOTE: THIS here is to represent DEPTH [as parsers can be VERY recursive indeed, it may be needed to eliminate the recursive errors on a case-by-case basis]; 
					}
				}
			}

		And a ToplevelCatcher [rough sketch, can use to generalize the above, with 'quit = (e) => throw e']: 

			function ToplevelCatcher (info: DebugInfo, logger: Function, quit: Function) {
				return function (thing: Function, thisArg: any, args: any) {
					try {
						thing.call(thisArg, ...args)
					} catch (e) {
						logger(info)
						quit(e, info) // this is here to represent continuation of a program AFTER the exception
					} 
				}
			}

		Although... This is rather general. Perhaps, better implement as a separate package; 
		Likewise, there'd be ways to count levels of recursion, and assign types to them [specialized signature]; 

	2.6. [maybe?] BreakableStream - a major generalization of the library's patterns: 
		Returns a class for producing Stream-s: 

		1. Have an '.input' (another Stream); 
		2. Has an 'Indexable' of 'BreakPattern's (they are objects) pre-given, which define how the Stream instances behave: 
			2.1. type: string - one of: 
				2.1.1. limit - works like LimitedStream; 
				2.1.2. nest - works like NestedStream; 
				2.1.3. halt - stops the stream [no more items after that]; 
				2.1.4. navigate - finds: 
					2.1.4.1. Either the next item [or the first item AFTER THE BEGINNING] that obeys a given property (PredicatePosition)
					2.1.4.2. Goes to a static position [relative or absolute]; 
					
					ALSO, has a boolean arg 'isRelative' (default: true, basically - whether the values for 'navigate' are to be handled absolutely or not...); 
				2.1.5. transform - applies a given transformation on the '.input' (calls a function); Returns the result;
			2.2. args: object - the way that the 'type' is resolved (equivalent of constructor-arguments/.init-arguments); 
			2.3. buffer: boolean | () => boolean - this one corresponds to whether the current value should be bufferized; 
				When a function is passed, it is called on the 'BufferizedStream' in question to get the value; 
		
		The result of matching 2. to the '.input.curr' is (effectively) evaluated to be the Stream's '.curr'; 

		This todo is a 'maybe?' because all the said things can (pretty much) already be done by the user using StreamTokenizer and the rest
			- this really only just provides a minor abstraction over the whole thing (frees the user from needing to use the exports of the library); 
		This kind of thing is (somewhat) against the library's "all-minimialism" approach; 

		On the other hand, it would permit one to use a very domain-specific language to describe operations on streams conventionally, 
			thus - simplifying semantics + unifying a very large number of different abstractions that are creatable via the library using just this one; 	
		Conclusion: think about it, more of a YES, tha a NO currently;
	
	2.7. utility - 'nestedInfo'; 

		1. Finds a depth of a given Stream, based off 'inflate' and 'deflate'; 
			NOTE: the MAX depth achievable; 
		2. Finds its length - how many positions (in 'number') must one walk before the current nestedness ends, from the initiated position; 

		Returns it as a pair; 

	2.8. Idea for a method: ProlongedStream.prolong; 	

		This (utilmately) modifies the 'this.streams' [modifying the '.isEnd' accordingly]; 
		Also, there'd be '.shorten(n: number)' - it'd delete a certain number of elements from '.stream'-s, 
			optionally changing '.curr', and set '.isEnd = true' (if the .curr is in one of the deleted 'Stream'-s); 
	
	2.9. [Maaayybe?] Create a 'samples' directory; 
		This is for 'common-case' parsing - generally, walk through the various syntaxes that were 
			PRESENT within the previous parsers, use them?
		
		Or, better, make this into a separate mini-project called 'parsing-samples'; 
		[EXAMPLE: handling the 'escaped' strings/sequences all the time];

		Think about it [this particular refactoring is VERY fine and case-specific, though, it can come in useful...]; 

		No, if one is to do it, the thing should: 

			1. Work with general parsing patterns, that would be CONFIGURABLE for every case; 
			2. Provide low-level Token-s with specialized given names; 

		1. Tokens: 

			1. Take them from various parsing projects of yours already existing: 

				1. xml
				2. selector
				3. regex

				Unite, choose best string-names; 
				Create appropriate names for sample-token classes (via TokenInstance) [
					example: 
					
					Opbrack - (, 
					Clbrack - ), 
					OpSqBrack - [, 
					ClSqBrack - ], 
					OpBrace - {, 
					ClBrace - }, 

				...]; 

		2. 'inSet' functions: 

			'isBinary = inSet(0, 1)'
			'isDecimal = inSet(0, 1, ..., 9)'
			'isHex = inSet(0, 1, ..., F)' - TAKE OUT of the global '/utils.ts'
			'isAscii = inSet(0, 1, ...)'
			'isAlphanumeric = inSet(0, 1, ..., 9, a, ..., Z)'

			Also - the sets in question should be provided AS-ARE (without the 'inSet'...); 
		
		3. ALSO - join the 'samples' with the 'constants.ts' file 
			[they are pretty similar in that they BOTH provide frequently recurring ambigious constant entities with meaningful "static" names and uses]; 
	
	2.10. v8-specific optimizations: 
		Having done some benchmarking on some typical Array methods, one arrived at a...
	
		CONCLUSION: 
			1. Handwritten methods are *often* __MUCH__ faster than the builtins ON LARGE PIECES OF DATA; 
				On the more common-place cases, however, they are inferior in time performance; 
			2. THEREFORE, one should do: 
				Re-organize the library (how?) to permit usage of the current methods on LARGE pieces of data; 
				Prior - do more benchmarking; 

				About re-organization: 
					1. Think - whether to introduce explicit low-level 'size-controls' [possibly costly, when run a lot]; 
						Although, it's likely to be optimized/negligible; 
					2. Or, whether to split the thing onto 2 chunks [where the 'size' is precomputed...]; 
						This is less flexible (and thus, error-prone), plus will be more difficult to re-structure; 
					
				Current vote is for 1.; 

				ALSO, about bounds - those should be CAREFULLY BENCHMARKED; 
		
		MORE GENERALLY - try to adapt the library for work on LARGE datasets; 
		It'll make it useful for working with big projects/pieces-of-text; 

		THINGS OF ISSUE TO THINK ABOUT IN PARTICULAR: 	

			1. What "LARGE" sizes are practical? [meaning - sources of what sizes can occur in the wild?]
				Take them as large as one can. At least 10000000 symbols (~30MB, ~100000 lines of 90-100 chars), maybe try more; 
			2. How much of a performance difference can one get by doing these optimizations?
				In particular - try to sketch out a parser for something (the smtf format of one's own? it's simple to parse), perform 
					profiling on a REALLY big auto-generated file, then: 
						
						1. take overall time measurements (performance.now()); 
						2. profile, break the execution down on several tiny pieces; Then - benchmark and optimize them accordingly; 
							For this, use results from samples from one's benchmarks library;  
			3. Optimize for smaller sizes ONLY when it doesn't impair the memory severely; 
				When it impairs it at all, choose whether or not to do it based off the chosen memory/speed difference measurements...; 
				
				3.1. FOR THIS [maybe] - have specialized "Heuristics" constant-space in 'constants.ts'; 
					3.1.1. IF doing them - create the heuristics for DIFFERENT ENGINES; 
					3.1.2. IF oding them - make the 'Heuristics' only the "default value" for transition of sizes; 
						THE USER must be able to set their own boundries; 
					3.1.3. IF DOING THEM - check how to "bounrdies" for the increase in efficiency when using 
						the custom implementation "shifted" as the versions of Node progressed - TRY IT with different node, v8 
							versions; 
					3.1.4. IF DOING THEM - other engines to test: 
						3.1.4.0. V8 (Chrome)
						3.1.4.1. SpiderMonkey (Firefox)

					[maybe?] Try building the engines from nil, running the benchmarks for heuristics standalone...; 
	
	2.11. [maaayyybe??] Bring the old planned stuff for v0.3 back; 
		That is the 'Parsers/utils.ts' - 'transform', 'delimited', 'consume' [equivalent of 'LimitedStream', uses 'Collection'-s]; 
	
		Do only if there are code aspects suffering from this significantly: 

			1. readability/accesibility; 
			2. performance; 
		
		Primary cause for this todo is that, despite being "slow" (memory allocation), 
			they did still have the benefit of semantic simplicity; 

		The only cause for performance issues with OOP Stream-s API is overhead from re-structuring the original loop so much
			[need for calling the '.isCurrEnd()' two times more]; 
		Again, do this ONLY if the performance difference is SIGNIFICANT!
	
	2.12. util - enumerateTree; 
		Given an array of "types" of nodes, it recursively converts their properties (given in the 'shapes' array), 
			to single-array-siblings form (the current ChildrenTree); 	
		This would permit a more "variable" set of trees (those that have properties with fixed names), 
			to be enumerated accordingly...; 

		Example: 

			{
				a: any
				b: any
				c: any
			}

		Becomes: 

			{
				value: [
					// ... a, b, c
					// ... or b, a, c
					// ... or any other order
				]
			}

		For this, define a 'TreeKind'/'TreeShape' - with a list of properties to be used; 
		2.12.1. Also: 

			Create a "fast" 'TreeIterator' class; 
				This is to replace the WalkableTree+TreeWalker+TreeStream combination due to: 

					1. They are slow (require FIRST converting the source to a 'Tree'); 
					2. They are memory-costly (creating a new Tree-structure is somewhat demanding); 
				
			This would (instead) approach iteration directly - it would use PARTICULAR properties for a given Tree;
			Also, (as it's not a PERSISTENT type), it'd accept a 'converter' (so as to be able to "iterate" AND "convert" a given Tree); 

			The interface works like: 

				TreeIterator(TreeKind(...))(childIterator, converter)
			
			Requires NO copying for long-term storage (it's "inlined"), hence - no time needed for unnecessary allocations; 

			The 'childIterator' is (basically) an IndexMap/function that returns for any given item its' sub-iterated items
				(an array of children); 
			The 'childIterator' is used for walking through the items INSIDE the 'TreeIterator'; 
		
			BENCHMARK THIS [compared to the 'WalkableTree'+'TreeWalker'+'TreeStream' approach to code-generation]!
		
	2.13. [generally - see if/where applicable] About generics - relax them; 
		Use ONLY WHEN NEEDED; 
		For example: a method makes use of X generics, out of which Y < X is needed [id est, used somewhere beyond the 'this: ...' in the parameters list]; 
			REMOVE the unneeded ones; 
			RE-ORDER THEM, if need be; 
			Try to make them more minimal...; 

	2.14. [maybe?] Create a module for randomized Stream-generation (`random`);
		(given, say, an InputStream with an Array of expected values inside of it [that can be picked for Nested], 
			it will choose a pair of them for different [distinct] boundries); 
		Similarly - one could give an Array of limiting characters for a LimitedStream to be defined by...; 
	
	2.16. Add a fast implementation of '.finish' to the 'TreeStream'; 
		When called for the first time - it SAVES the "last index", THEN - re-uses the computed one on any subsequent '.finish()' calls; 
		Calls the '.index' on the saved '.lastIndex'; 

	2.17. Re-structure the tests to be made on a per-method basis? 
		Current per-class suites are rather verbose (don't do this until v0.4, already spent too much time on the tests...); 
	
	2.18. RE-INTRODUCE the '.copy()' method and 'Copiable' interface; 
		Comes inside the 'StreamClass' (optionally) via the 'hasState' property;
		Will additionally be supported by: 

			1. 'StreamClass(...).prototype.copy()' method; 
				Solution for '.copy': 
					1.1. By default, copy ONLY: 
						1. the user-defined variables under '.state'; 
						2. the '.pos', '.buffer', and so forth - predefined "inherited" properties;
							Any CUSTOM copying/initialization logic can be defined by the user in DERIVED CLASSES; 
				
				Definition [loose, has to have overloads, depending on presence/absence of '.pos']: 

					function uniCopy (stream: ...) {	
						const copy = new (Object.getPrototypeOf(stream).constructor)()
						copy.uniInit(initSignature(this, this.bits))	
						return copy
					}

				NOTE: here, 'this.bits' is the '.prototype'-level BitArray with different properties that ARE PRESENT; 
					Look below for pseudocode explanation

		Then, the various 'Indexable'-s can use the '.state' inside the '.index' call; 

		ALSO [regarding '.init']: 
			[Predefined] things like '.value', '.buffer', '.pos' and so-forth, DO NOT become '.vars'; 
				To simplify the process, store (on each StreamClass), a 'BitArray' - to indicate, which are present; 
					Depending on this - the user will be able to call the '.init' with "universal" signature, like so: 

						// note: stored on the '.prototype', no memory waste
						stream.uniInit(initSignature(this, this.bits))
					
					The 'initSignature' is a util, returning the "maximum" '.init'-signature, that is possible; 
					The '.uniInit' is a method that initializes the given Stream BASED OFF THE "maximum" signature; 
					This version of 'initSignature' is "from-bits", one that isn't is done via an object like: 

						stream.uniInit({
							state: this.state, 
							vars: this.vars, 
							...
						})

	2.19. For future: improve the 'constructor' tests. 
		Refactor them, permit the ability to properly test the '.copy()'-ed instances using the constructor
			tests (namely - the "prototypeProps" and "ownProps" thing - missing currently); 
			
	2.20. 'regex' - allow for methods for creation of regular expressions that use the extended regex-syntax (enabled by the 'v' flag); 
		Namely: 
			1. intersection: 	/[[...]&&[...]]/v
			2. subtraction: 	/[[...]--[...]]/v
			3. string literals: /[q{...}]/v
		
	2.21. [prototypes, generality] make all the prototype 'value's (in particular - methods) OVERRIDABLE! 
			via doing: 

				Object.defineProperty(_class.prototype, {	
					methodName: { value: ..., writable: true }
				})
			
			Currently, they are ALL non-overridable; 
			This will enable the user to more liberally use the library classes [they are, presently, highly single-purposed (intentionally)]; 
	
	2.22. Create a new 'IndexMap' implementation: 'SetMap'; 
		This would be a: 
			1. IndexMap interface implementation; 
			2. generalized wrapper around a Set (in a way that HashMap is a wrapper around Object and Map); 
				2.1. Would have its own 'extension'; 
				2.2. For 'values' would contain true/false; 

		This is useful for working with cases that require the 'or'-predicate; 
		Example: 'nested' utility; 
		How it is done [pseudo-code]: 

			const SpecialCaseSetMap = SetMap(...)
			const processNested = (x) => nested(TableMap(new SpecialCaseSetMap([...])), TableMap(new SpecialCaseSetMap([...])))(new WhateverStream(x))

	2.23. refactor the 'imports' tests; 
		1. more use of 'specificChildImports'
		2. the 'prefixedNames' + 'namesCapitalized' - appears lots more than once, take out; 
		3. Make better use of the 'namesCapitalized'; 

	2.24. Add the 'export default' from 'classes.ts' and 'interfaces.ts' files for their "main" stuff; 
		Example [1]: a 'classes.ts/interfaces.ts' with THE ONLY export will have it as default, besides referencing by name; 
		Example [2]: when a 'classes.ts/interafaces.ts' has an export with THE SAME name as the module, the export is default (besides being present as of itself); 
				
	2.25. [large input optimizations] Investigate whether retaining the indexes information from 'matchAll' in 'tokenizeString' may not be faster on large inputs; 
		This is largely a question of how fast '.split' is compared to building the array manually using the indicies information from 'matchAll' (that is, a large sequence of '.slice()'-es); 
		Potentially, this could be a large time saving (if '.slice()'-es are relatively short), but otherwise - could be a waste of memory (additional strings allocated without need); 
			Then, of course comes the question of whether the strings are allocated at all; 
			BENCHMARK IT; 
	
	2.26. Introduce a new Position class - MultilinePosition
		Basically, a pair of numbers: 
			(linesNum, symsNum)	
		Create a generalization of it (where 'newline' is a context-defined term, like a particular "separation token", or something...): 
		Which correspond to prior number of newlines and prior number of symbols; 
		Used frequently throughout for things like debug information; 
		The '.convert()' would play very nicely with it...; 

	2.27. refactor the '(x: any, y: any) => boolean' comparison into a 'Comparison' type; 
		This is (most often) is used inside the library's testing mini-framework; 

	2.28. About validation and parsing: 
		This is a somewhat worrysome part of the library, currently, as:

			1. Validation happens separate from parsing, despite using the same abstraction (ex: ValidatablePattern and TokenizablePattern); 
			2. '1.' causes the entire process of 'validate + parse' to take 2x the time (need another loop); 
		
		[SOLUTION 1] Still, for more complex cases (in particular - those that require a per-Stream-element identification - this will not suffice); 
			For this reason, a TODO: 

				Create a new abstraction that would encompass BOTH parsing and validation; 
				Granted, it would take more time than a simple parsing procedure, 
					but (and this is CRUCIAL) it must take LESS time than split parsing + validation (in exchange for increased memory usage); 
				
			This is already doable (manually) using 'StreamTokenizer', but one would love to make it a part of the library (general abstractions + interface + algorithm); 
			From the noted above, 'PatternTokenizer' and 'PatternValidator' are a no-go here 
				(potentially, an infinite set of possible valid/invalid expressions with convoluted structure); 
			It should be a case of 'StreamTokenizer'; 

			Ideas: 
				0. It is a StreamClass (a configurable class of StreamTokenizer-s, more precisely); 
				1. Let stream elements be '<OutType>[valid, token]: [boolean, OutType]'; 
				2. Contains a field of '.validator', which returns '[boolean, OutType]', where 'OutType' is the UPDATED token (based off current one and 'input'); 
					return value is, then: 
						
						return this.validator(this.handler(this.input), input)
					
		[SOLUTION 2] Alternative solution [most natural approach in practice, although forces using more general grammars]: 
			1. StreamParser; 
			2. Use a table (HashMap) to see whether a given token can be (in-order) parsed to anything; 
			3. If it cannot be matched - an error/exception, one needs a specialized ErrorHandler; 
			4. Otherwise - part of the Stream; 

			Benefits: 
				1. Least possible overhead
				2. Trivial to implement
				3. Natural 
			
			Cons [problems]: 
				1. Little-to-no custom validation logic: 
					SOLUTION: add it; 
						the 'ErrorHandler' be passed the given Stream; 
				2. Forces stopping of the parsing:	
					SOLUTION: do not; 
						Add the ability to return a value from the 'ErrorHandler': 
							either 'true' - ContinueParsing, or 'false' - HaltParsing
		
	2.29. Later - improve the tests: 
		2.34.1. Add generics to the class-tests signatures [just for the sake of it - very pretty and give a lot more sense to using TypeScript for tests]; 
		2.34.2. Create a way to print out the class tests' instances (via a special 'instance' library function, based off 'it'); 
	
	2.30. Implement a new interface for the library - FiniteEnum: 
		This, basically, creates an 'EnumSpace', that consists of all the given items, 
			BUT requires finding out their uniqueness beforehand (using the 'new Set', or the 'one.js' 'norepetitions'). 
			Thus, for instance, these are equivalent: 

				const a = new FiniteEnum(["800", "elorian", "800", 998])
				const b = new FiniteEnum(["800", "elorian", 998])
			
			Thus, the '.size' of the given EnumSpace is actually fluid; 
			Likewise, its' '.add()' would NOT be a number (hence - one NEEDS A NEW INTERFACE); 

	2.31. IDEA: how to speed up the tokenization algorithm [attempt]:
		Way: 
		1. Split the "tokenized" and "untokenized" parts with 2 arrays
		2. Keep the 'indexes' of the tokenized (on a per-item basis) and untokenized (on a "linear" - read below - basis) parts
		3. Split the final result (tokenized) into BATCHES	
			3.1. The underlying is a NEW structure called 'BatchArray'; 
				It keeps its values as a list of lists, which HAVE THEIR INDEXES; 
				The second-level lists are all ordered as the original array is; 

				Operations: 
					
					0. bind(array) - binds the given array
					1. newBatch() - creates a new last batch, to have values written into
					2. merge() - combines the batches into a single array, removing the indexes; 
						For this - use Merge-sort (it's a natural choice here - the structure is precisely what the algorithm requires); 
						Due to quality of data, the complexity (in this case) is O(nlog k) [considerably better than 'O(nlog n)'];
					3. replace(i, subarr, subinds) - replaces a given index with a value of 'subarr'
						This has to be fast, therefore: 

							1. Store the '.bound' NOT as 'any[]', but as a special 'ContinuedElement[]'; 
								It keeps elements as "referenced" (ContinuedElement), and 
									items that are referenced can have: 
										1. a '.value' (when no array is used); 
										2. an '.array' (when one is used, otherwise - null); 
									
								Thus, the replacement-with-array operation becomes O(1);

							2. When this happens, after changing the 'ContinuedElement' one: 

								2.1. Finds the batches that have the affected elements; 
									To make it fast (for general case), store batches NOT just as 'any[]', 
										but also have '.minIndex' and '.maxIndex' properties; 
									Thus, it's possible to determine (in a single check) whether 
										they are affected ('.maxIndex < .changedIndex')
										in O(1) time; 
								2.2. Finds the elements inside the batches; 
									Done via binary search over the indexes; 
									Then - as they are linearly ordered, walks the remainder of the batch-list, 
										doing 2.3.; 
								2.3. Changes the elements; 
									Add the 'length-of-the-replaced-subarray - 1'; 
									O(1) already, no need to change anything; 

								IMPORTANT: for this to be FAST (algorithm becomes O(k^2 logn) and NOT O(nlogn)), 
									one needs to do this step is done in a SEPARATE 
							
							3. When 'replacement' occurrs recursively (inside an existing ContinuedElement): 
								3.1. It's the same, only difference is - one has to calculate the index manually (which can be mildly cumbersome, but is still O(n)); 
							
							4. The '.indexes' list, then, is ALSO altered; 
								The old 'untokenized' index is "deleted", new UNTOKENIZED items' indexes (subinds) are put in its stead; 
								It is ALSO kept as a 'ContinuedElement[]'; 
								When an index is JUST deleted (that is, it's tokenized wholy and singl-y), 
									one replaces the value with 'null' (no '.splice' necessary); 

								To find it, one has to: 

									1. Take the first 'ContinuedElement' x: 
										1.1. Increase the value by (if array has no) 'x.array.length', or (if no '.array' is used, just '.value') 1
											To make checking for presence of a 'ContinuedElement' inside 'ContinuedElement' faster - add a flag-property ('isRecursive: boolean'); 
											BETTER IDEA: 
												Use a 'BitArray' (a new data structure - a UInt8Array/UInt32Array, whatever, that keeps flags as bits); 
												ANOTHER IDEA: 
													Use a 'BitArray' to store a number [together with a boolean]; 
													This way, it can have less bits, and more appropriate for a given usage ("cram it" with the flags - these go first); 
													Then, one can also KEEP TRACK of the first 'ContinuedElement' [helps, when it's > 0]; 

													ANOTHER number stored - count of recursive 'ContinuedElement'-s [allows to sometimes finsh early]; 
													IDEA: 
														Keep not the FIRST 'ContinuedElement', but the one that is THE FARTHEST! 
														Then, what we have is - the largest possible speed increase; 
														This also requires an ADDITIONAL number getting stored: separate BitArray (for the index that is skipped); 
														Remainder are linearly walked-through; 

														BETTER IDEA STILL: 	
															Store it upon the 'CountedElement'-s that are using '.value'; 
															THEN - there is no need for the flag, as one ONLY works with 'the next' value; 

										1.2. Proceed until the happenstance that it becomes equal to the sought-after index, OR 
											the difference between the current one, and the next becomes "too far" to be used as an intermediate-point; 
											From there - one does a literal jump (as there are no more "nested" indexes left); 
										
									Therefore, our '.indexes' is capable of getting the current index at about O(k), where 'k' is the table size (number of tokenizations); 

									IMPORTANT: the '.indexes' is actually A 'ContinuedElement' ITSELF; 
										Thus, all techniques can be used on it also...; 
						4. indexSync() - see '3.->2.->IMPORTANT'; 
						5. getIndex(prevIndex: number): number - for iteration, see 'Fields->2.'; 

				Fields: 

					1. .bound: ContinuedElement[] - the array of items; 
					2. .indexes: number[] - the list of indexes that are NON-REMOVED [used for iteration]
						IDEA: instead of keeping it as a list of numbers, use 'BitArray'-s, with a changing memory outfit; 
						This permits to (greatly) economy the memory, because to see if an index is "present" one just 
							marks it as 1/0; 
						The "nested" ones will require more memory; 

						(
							Possibly... One could save *some* memory with a complex bit-layout?
							Thus, for instance, having, 01 mean 'non-nested, present', 00 - 'non-nested/nested, missing', 
							'1' - 'nested' [only a single (first) bit used]; 

							Also - there'd be another - 'GrowingNumber' (new library data structure; this time - a single-element UInt8Array/UInt16Array/UInt32Array as a base, not UInt8Array); 
							This one'll keep a single number; NOTE: the thing is needed at all even because of the '2^53 - 1' number-bound (namely, that one doesn't like it...)
							This is (basically) a wrapper around a number to allow for quick manipulations that are ALSO memory-efficient

							'Nested' ones will USE THE SEPARATE INDEXES inside the '.indexes: (...)[]' array; 

							Then - one'll be able to have ANOTHER method on this, called 'getIndex'; 
							It'll return the next non-missing iteration index, based off the current one; 
						)

						Also - regarding the 'automatic' empty return from 'tokenization' function - this won't work here (too much per-element overhead); 
						Conclusion - one has to check the return-array length FROM THE START; 

						PROBLEM: consider this - '.indexes' becomes PARTITIONED! 
							More importantly - a given portion now becomes more about "missing" than "present" 
								non-nested bits; 
							SOLUTION: reverse the bits inside the given 'nested' bit; 
							To determine WHEN to do it - count the "missing" and "present" parts; 

					3. .batches: [any, number][] - the list of indexed elements inside the array
					4. .syncInfo: [number, number, number] - the list of REPLACEMENTS that has been done; 
						Used for increasing speed (namely - DELAYING the slow index-sync operation); 
						Has form of '[batchHappen, replacementLength, replacementIndex]'
						
						1. Because one KNOWS that for all things that are AFTER the 'batchHappen', the index is "fixed", we also know
							that only 'batchHappen' batches need be checked at all - O(k^2), where k^2 is the size of the table; 
						2. Because one KNOWS that there are '.minIndex' and '.maxIndex' for a given batch (and it's ordered), 
							one can find out whether one has the desired changed indexes - O(1)
						3. Because one has the indexes as NUMBERS, one can use binary search - O(log n)

						IDEA: for an even BETTER optimization - to have better memory consumption, a more complicated structure: 

							1. STORE THE INDEX-ADDITION INFO on a per-batch-basis' (buckets)
							2. Store them in a linearly ordered fashion (id est, so as to PRESERVE the values...); 
						
							QUESTION: how much will this book-keeping piece cost? (the 'number' triple only requires O(1))
								To do this, however, one is in need of: 

									1. get the triple; 
									2. put it into one of the 'batch-baskets' of the '.syncInfo' (based off 'batchHappen'); 
									3. in the chosen basket - '.push([index, length])'; 

									[post-creation, when calling '.sync']
									4. later - get the value from the appropriate bucket (they are ITERATED from 0 to 'curr-batch-index' for '.batches'); 
									5. apply all the transformations to the given batch; 
								
							ANSWER: not much in terms of performance; 
								ALSO - it's actually better in terms of cache-locality (because there's less jumping around); 

							ALSO: the two numbers are kept as GrowingNumber-s! 
		Result [theory]: 
		1. O(n) additional memory usage (where 'n' - number of final tokens)
		2. O(1)-O(n) performance gain (depends on number of tokenized items, least waste in cases when there's a lot)
			The current "speedy" O(n^2) (O(n) - walk through the items; O(n) - the '.splice' in replace due to index-keeping) 
				can be replaced with a "slowish" (meaning - one with a possibly significant factor) O(n).
			This ability is important, and in certain applications can be considerable; 
			Only questions are: 

				1. When can this be used? (use-cases)
				2. [Most important] Can use-cases be identified prior? 
			
			Generally, for: 

				1. Large inputs; 
				2. Large (> 15 items) tokenization-tables; 
			
			This could make some sense. 

				1. The '.splice' requires one to go SEVERAL TIMES through the 
					ALREADY TOKENIZED elements (the reason for inner O(n))
				2. The 'isType' requires performing redundant checks; 

			In expense of memory and: 

				1. Book-keeping cost (a noticeably more complex algorithm that runs in O(n) linear time + lots of index-keeping); 
			
			this implementation would permit to remove these; 	

			Final performance is: 

				O(nk^2log n), where 'n' - number of tokenized elements, k - table size

		CONCLUSION [post micro-benchmarking]: 
			1. After some little fiddling with benchmarks, one found out that (actually), 
				'.splice' is optimized to be extremely fast; 
			2. One'll need to check the optimization difference betwee this and '.splice'
				(very well may be that this turns out to be slower regardless)
			3. (Theoretically), the '.splice' STILL copies the array, even though it happens on a 
				MUCH lower-level (C++, Asm); IF it's faster (which is quite likely, indeed), 
					that is only due to the purely numeric part; 
			
			4. Important: 
				(From what is known insofar) The only "really" slow part of the algorithm, 
					that genuinely would ruin the relative performance to repeated '.splice' (which, for a large enough input would take literally forever...), 
						is the "setting up". 

				Namely, the process of "guessing" the number of 'nested' indexes [their tree]; 
				One would be required to "grow" it accordingly, with a relatively few size-increases for it...; 

			5. Before adding ANY memory-saving data structures 
				1. Benchmark for memory; 
				2. Benchmark for speed; 

				Because if either is absent, the whole thing could simply collapse due to speed/memory assumptions going horribly wrong; 

			6. FINAL: if it's slower, REMOVE; 
				First - implement and benchmark performance for some large inputs (see if faster at all, likely not); 
				Second - see the 'time-difference/memory-difference' ratio, see if it's worth it (very likely not); 
				Third - either delete, or keep; 

	2.32. Performance concern: '.push(...elems)' FAILS for large arrays; 
		Problem - with the '...elems' spread operator: it creates a new array; 

		1. Do '.push()'-es in batches, or avoid them completely....; 
			[Idea: use ContinuedList-s instead? - new data structure
				Same thing as 'ContinuedElement', but without the '.value' to store, 
					and that references ANOTHER ContinuedList, instead of just an array]; 
		2. Better idea - where possible, use "mixed signatures"
			(example: 'string[] | ContinuedList | string[][]' for 'StringCollection.push'); 
			Then - the methods in question have a higher versatility; 

	2.33. [new type] BitArrayHash; 
		Accepts a BitArray, returns a value from it; 
			A hash based off bit-shifts (<<, >>); 
		Use for defining the '.init', '.curr' and other choice functions in the StreamClass; 
		BitArray - a UInt8Array (or other) array. Look into the '2.35' (pattern-tokenizer speedup) for details

	2.34. [TypeScript; clarity] Add the 'type ForcePresence<T, K extends keyof T> = {[x in K]-?: T[]}'
		This (and similar ones), for adding/removing various type features, are handy for refactoring (appears at least in StreamClass); 
		ALSO: 

			1. Doing this COULD allow one to be switching from the "composite interfaces" to polymorphic ones with optional properties; 
				(Even create a SINGLE RIGID 'Stream' interface! [NOTE: the OLD ones AREN'T GOING ANYWHERE - they're just getting re-formulated...]; 

					interface Stream<Type = any> extends Summat {
						pos?: number
						buffer?: FreezableBuffer<Type>
						state?: Summat
						prev?: () => Type
						isStart?: boolean

						init?: (...x: any[]) => Stream<Type>
						navigate?: (pos: Position) => Type
						finish?: () => Type
						rewind?: () => Type

						curr: Type
						next: () => Type
						isEnd: boolean
					}

					Plus, one COULD start using the OPTIONAL per-element interfaces [then, using the 'ForcePresence' with a "stricter" interfaces]; 
					This, in particular givest the benefit of being able to recognize properties on a "lower" type WITHOUT requiring an 'as' (no significant type downgrading); 

					2.49.1. ALSO  - create a single RIGID 'StreamClass' interface! [for the implementation - write down the optional '.pos, .buffer', and so forth... Missing currently...]; 
				)
	
	2.35. Use C preprocessor with this code; 

		Getting tired of typing "optionalValue(this, value)", and other such things in repetative situations (like 'init.ts'); 
		Abstract those away with a good old '#define' + '#include'; 
			Then - introduce C preprocessor tools to the project's build; 
				The build process, then, would then be : 

					1. steps: 
						prep/ (with-preprocessor dir, via 'make') -> ts/ (pure TypeScript) -> dist/ (pure JavaScript)
					2. extension: 
						change from '.ts', to stop the VSCode from doing its thing; 
					3. create a basic theme for this "mixed" TypeScript + C-preprocessor thingie [VSCode extension idea]; 
						For syntax highlighting

		Due to library's (intended) reliability in therms of name-usage, it ouhgt to be ok; 
		ALSO - do this only AFTER everything else in the library was ALREADY DONE! 
		Due to the fact one doesn't want to touch its code again after the v0.4, this particular "hybrid" 
			language shouldn't become a thing to cause problems [that would need separate solving] during development; 
	
	2.36. Liberate the 'Parser/' directory as well? 
		Due to the reformulation of the library's key terms through 'StreamParser', it has become depressingly empty; 
		Also - introduces some unnecessary higher-level "bunching"; 
		This way - there'd be global functions that are exported from the package - PatternTokenizer, PatternValidator, for starters...; 
		Although - there MAY be come new things coming into the 'Parser' (the new memory-hungry patter-tokenization algorithm, for instance?)

	2.37. About the continiously occurring '[A, B][]' types in code: 
		Fix those; Replace with 'Pairs'; 
	
	2.38. Increase the Library's natural debuggability: 
		ADD names to all the nameless functions. 
		This is due to the fact that in error logs, they would appear A LOT more clearly if one did do it. 
		ALSO - do the same for 'one.js'; 
	
	2.39. [maybe?] Add another 'hash' function for the 'HashMap'? 
		In particular, use the 'extension' for 'set', 'replaceKey', 'delete' (by-original-key), 
			and another hash (the 'indexHash') for 'index' [this one will be optional and default to 'extension']; 
		This is in parallel with how the 'IndexMap' has it.
		Motivating example: 

			1. Indexes are objects with a field 'T: keyof X'; 
			2. Indexed are the objects with a field 'R: keyof X'; 
			3. One wants to create a HashMap with 'N = HashClass(...)', such that '(new N(...))(T) == smth'; 
				Problem here is that one EXPECTS the same hash to work for both the 'index' and 'delete'. 
				In general, the 'HashClass' does NOT support the categorization (even though in the case of 'SimpleTokenType', it works); 
	
	2.40. [new class] Traveller, a point on a Stream; 

		Prior sketches of the v0.3 included a '.navigate' method that implemented an absolute positioning inside a Stream.
		This was scratched in favour of relative positioning, which is more widespread. 
		The absolute positioning, however, has a very powerful use case, provided more things are added: 

			1. StreamClass - a new flag '.size', for remembering the precise size of a Stream, in terms of the last position that 
				is visitable; 
				This is useful for: 

					1. Remembering a Position, at which the portion of the Stream starts; 
					2. Traversing the precise distance until the end; 
						The hard numeric bound gives a speedup due to the lack of need for checking for '.isCurrEnd()'
							(more precisely, one can replace it with a 'this.pos < this.size'). 

						For this, the 'isCurrEnd' has to be __'.writable'__; 

				For efficient implementation of this, one must be able to get to the beginning of the portion in question; 
				Therefore, a custom implementation of '.travel' is necessary. 
				Although, one can define a default that works by an efficient implementation of '.navigate()' and '.rewind()': 

					[sketch]
					travelDefault = function (pos: Position) {
						if (isNumber(pos) && this.pos - pos < pos)
							return this.navigate(pos - this.pos)
						else {
							this.rewind()
							return this.navigate(pos)
						}
					}

				Then, one can have: 

					1. A WrapperStream, on a given Stream, with '.size = true'; 
					2. Remember a Position, where WrapperStream starts on a Stream using a 'Traveller'; 
					3. Define an efficient '.rewind()' on the WrapperStream, that would use the 'Traveller' in question, after fully iterating through it; 
						Another reason for formalizing the WrapperStream; 
				
				This is absolutely priceless for lazy parsing (skipping pieces of data in order to gain information about it first 
					- then doing the parsing, on a need-to-know basis). 

				List of Stream-creation classes to make into WrapperStream-s: 

					1. LimitedStream; 
					2. NestedStream; 
					3. PredicateStream; 
				
				Like StreamClass (on which they'd be based), these are all generated classes;  

			2. In accordance with '1.', a new default method for StreamClass - '.travel' (does an absolute navigation around the Stream)
			3. To remember particular Positions on a particular Stream, that may need to be recalled later (as per '1.'), one has a new 'Traveller' class: 

				[sketch]
				Traveller extends PositionObject<Position> {
					value: Position
					stream: TravelableStream
					convert() {
						return positionConvert(this.value)
					}
					travel () {
						return this.stream.travel(this.value)
					}
				}
			
	2.41. Problem: non-reactive '.isEnd' property; 

		[Note: WrapperStream - a Stream with '.value' being another Stream]
		This is the issue with keeping it as a property, and not a getter. 
		Consider this: 

			1. A Stream is made; 
			2. A WrapperStream around it; 
			3. The underlying Stream is modified (example: .travel() to the middle of it)
			
			Result:

				1. The underlying Stream's '!.isEnd', whereas the WrapperStream '.isEnd'; 
			
		Conclusion: one needs another (more general) interface to express dynamic relationships of Streams' '.isEnd' property. 
			In particular: 

				1. The scenario in question ONLY occurrs when one needs to be able to SAVE a WrapperStream around a given Stream; 
				2. Therefore, one can safely say when it is reused, and (therefore), when one needs to update the '.isEnd' for veracity; 
					Conclusion [1]: add a new method for this '.updateIsEnd()'; 
						It does precisely this [in StreamClass implementation]: 

							this.isEnd = this.value.isEnd

					Conclusion [2]: For this, one'll need to formalize the 'WrapperStream' interface (because, otherwise there's no guarantee that the 'value' is indeed a Stream); 

	2.42. Refactor creation of PropertyDescriptor-s; 
		They are all over the place, order it, make an ordered, simple and maintainable signature,
			that would allow one to express PropertyDescriptorMap-s as objects; 
	
	2.43. [refactoring] Add a new util: 
		tokenCompare = (tt: TokenType) => (x: any, y: any) => tt.is(x) && tt.is(y) && value(x) === value(y); 

	2.44. TreeStream: flag optimizations; 
		Allow 'TreeStream' to be a Stream-class-generation function also; 
		Permit the TreeStream to have overloads for: 	

			1. navigate (work with number-indexes, when '.buffer' is present); 
			2. next		(same thing - '.buffer' + '.pos')
			3. prev		(same thing - '.buffer' + '.pos')
			... [and so forth]
		
		Will allow to have the benefits of flag-presence via the overloads; 
	
	2.45. [static method] MultiIndex.fromPosition(stream: TreeStream, position: Position): MultiIndex
		This is a method for converting a 'position' from the given TreeStream to a 'MultiIndex'; 
		Does NOT '.rewind()'; 
	
	2.46. [Big one] Add support for proper PatternMatching;
		Serves as an abstraction over the 'RegExp'; 
		Allows one to match 'Token'-s using 'MatchablePattern'-s; 
		Would have: 

			[1. matching functions]
			1.1 StreamMatcher - works on Stream-s of items that are '.match(...)'-able [as arguments for 'MatchablePattern.match']; 
				Expects as input a 'MatchableStream', or a similar thing; 

			[2. MatchablePattern-s]
			2.1. [StreamClass] MatchableStream 	- walks through a given Stream of items, producing new 'MatchablePattern' on each '.match()' [ex: calls '.next()' on every '.match()']; 
				Depending on the output of the '.match' ('true', 'false', 'null', number, or PredicatePosition), either: 

					1. 'true' 					- goes forward 1 position; 
					2. 'false' 					- halts (mismatch)
					3. 'null' 					- goes backward 1 position;
					4. number   				- goes forward/backward specified number of positions
					5. PredicatePosition 		- goes forward/backward until specified condition on a 'MatchablePattern' is met; 

			2.2. FlexibleMatchable				- a 'MatchablePattern' defined by a particular user-given function; 
			2.3. [ChildrenTree] MatchableTree	- a 'ChildrenTree', that chooses one of the matching "paths" depending on the result of the '.match' function of its own; 
				Useful in combination with 'TreeStream'+'MatchableStream' - can create "matching trees"; 

			[Possibly, add more stuff... LOOK FOR CASES OF APPLICATIONS!]

	2.47. [codebase consistency] The '.init' methods for StreamClass DO NOT return 'this': 
		Fix this - let them. All the other '.init' methods do it; 
		(This, in particular, generally permits usage like: 

				const t = new InitializedClass().init(...)
			
		In cases, when the constructor is NOT the same as the '.init(...)';
		Likewise, a different semantics [copying] for '.init', when a partial copy is necessary: 

				const t = new InitializedClass(...)
				const r = t.init(...)
				t === r // false
		)
		
	2.48. [maybe?] Turn the 'TableMap' and 'MapWrap' into 'FlexibleFunction' derivatives? 
		This would [pro]: 

			1. introduce some consistency into the library; 
			2. reduce "dynamic" addition of properties to objects (particularly - functions); 

		However [con]: 

			1. may cause a performance penalty (check); 
		
		IF using FlexibleFunction is NOT slower (in terms of function calls), then do just that...; 
		The 'FlexibleFunction' is always going to be slower on creation (due to: 1. constructor; 2. runtime-parsing of FlexibleFunction), 
			but this shouldn't be too costly overall (besides, when string is static, it well may be cached to improve performance); 

	2.49. Change the tests using one's the 'tests.js' library; 

	2.50. Add more complex 'Tree' utilities. Based off previous 'one.js' v0.3.1, add: 

		1. deepSearch - searching inside the given Tree for a value with a given 'prop'
		2. depth - calculating the depth of the given Tree
		3. treeCount - counts the values of a given predicate for all items in the tree

		2.51.1. Add new methods to 'ChildrenTree': 

			1. .reverse() - recursively reverses the given tree

	2.51. [maybe?] Do some amazing TypeScript jiggery-pokery:
		Sometimes, the type can be more complex to predict [but STILL might be possible via templates, type-inferences and special functions like 'Parameters'...]: 

			[LinearIndexMap/methods.ts]
			1. extend
			2. extendKey

	2.52. Type Simplification: 

		Library boasts some pretty complex types. 
		Particularly, this applies to: 

			1. StreamClass
			// ... [expand the list]
		
		Use utility types to make life easier. 
		Also - REMOVE some of the excess types. 

	2.53. Naming conventions - the post-name-'Type' doesn't look NEARLY as good as the classic prefix-'I'; 
		Replace [note: with LOCAL imports *only* - things like 'PointerType', say...]. 

	2.54. CREATE A proper website with documentation for the library. 
		Do benchmarks, et cetera...; 
		After doing GitHub Wiki for v0.3, see if it cuts it (spoiler - it likely won't one STILL wants the ravishing types for the library's docs to be present!)
			For this: 

				1. Learn a new CSS-based tool (either for generating/hosting docs/docs-css-styles like TypeDoc/readthedocs , 
					OR a new library to create one's own doc-styles like Tailwind CSS); 
				2. Create a JSON parser in parsers.js and COMPARE it against the [https://chevrotain.io/performance/], with ops/sec. 
					See, WHICH libraries have managed to beat parsers.js v0.4., and WHAT can one do about it (see their optimizations, specifically...); 
					2.1. ALSO: create a benchmark for MEMORY USAGE [more concretely - compare parsers.js with the others...]; 
						The memory usage should be noticeably better than speed...; 

					2.54.1. One MAY want to adopt a new Tokenizer: 
						Specifically, a more "traditional one", which CAN use regular expression 'RegExpTokenizer' and/or 'RegExpLexer' (for num-positions...). 
						Reasons: 
							1. StreamParser as a tokenizer SHOULD be performant (if used correctly), BUT it can't work with regular expressions [no notion of "consuming tokens"]; 
							2. PatternTokenizer is SHOULD be SLOW AS FUCK because of how simple its algorithm is...
								PROFILE IT LATER [again, when creating new parsers...]
							
					2.54.2. One MAY want to adopt a Chevrotain-style rule-based parser (RuleParser): 
						Reason - 'StreamParser' (the "MAIN" parser present in the library) is LAYER-BASED, 
							in other words - it is ONLY intended to be used with some hyper-fast and hyper-simple things, which 
								END UP being "stacked up" for several layers, causing a rather unpleasant kind of coefficient-jump in O-complexity of the final algorithm; 
						This, however, does not necessarily signal a need for change and/or adoption of such a thing
						Although - it may be useful as of itself regardless...; 
							The Chevrotain's CstParser API is (quite frankly) horrible and uniodiomatic (which appears to be library's general problem - that includes the code)
								[
									1. the need for 'this.performSelfAnalysis'
									2. moronic FLATUPPERCASE conventions
									3. the unnecessary DSL,
									4. requirement of using classes for parsers
									5. this.SUBRULE2-like things...
									... et cetera
								]

						Reasons to use a RuleParser: 
							1. [Potentially] improved performance
							2. [Potentially, for some cases] Less tedious to write
							3. Simpler final project architecture
							4. Natural correspondence with Rules' languages 
								(can parse rules, and create respective parsers from them - provided the right kind of API, Chevrotain's a bad example)
							5. More intuitive AST-formation process

						Reasons to NOT use a RuleParser: 
							1. More complex implementation
							2. Worse maintainability
							3. Worse modularity 
							4. Less intuitive parser reversability [layers help there]

						In order to achieve a good case of parser extensibility (A MUST for parser.js - Chevrotain doesn't have it), 
							one'll be needing to define the 'Rule' class, which would permit creation of general extensible rules, of which RuleParser-s be 
								created. 

						IDEA for Rule implementation [bit of a jumble, but is good...]: 	

							0. IdentityRule - this is the "identity" rule, which simply copies the Token as-is [that being, it CHECKS for whether the PRECISE TokenInstance/TokenType is present]; 
								Equiv RegExp: /./
							1. ElementaryRule(type) - this is equivalent to creating a rule for a token with type '.type';
							1. .template: RuleTemplate - a set of operations, based off which a Rule is defined; 
								1.1. Fundamentally, 'RuleTemplate' is a method with '.inject(rules: CompiledRule[]): CompiledRule' - called during 'Rule.compile()'
								1.2. The 'CompiledRule' has '.match(x: Token): boolean' method, for checking whether or not a certain item fits the given thing. 
									These are used (finally) by the 'RuleParser' on the given 'TokenStream' 
										(interface for the thing one obtains as a result of either 'RegExpTokenizer' or 'StreamParser')
									to produce a new `TokenStream`;

									1.2.1. 'TokenStream' - another one of library's problems is ITS FUNDAMENTAL LACKING;  
								1.3. '.operators: Operator[]' (per instance) - a list of operators, each one "consuming" a given number of items (like the *binary* 'a | b', or *binary* 'a b' - following)
									1. Each one of the operators has to be defined in the '.table' OperatorTable (and be referenced by the respective name); 
									2. Each one of the operators has to have its own definition (predicate) for determination of whether or not it (specifically) matches; 
									3. The library provides a default PEG-like 'OperatorTable' and a 'RuleTemplate' class specifically for it...; 
								1.4. '.table: OperatorTable' - a '.prototype'-based property for keeping the available 'Operator's (it's also an interface...); 
							2. .sub: Rule[] - rules inserted into the RuleTemplate [note: recursively '.compile()'-d]
							3. .compile() - CompiledRule (new object, with a new definition for '.invoke' method) - the result of substitution of '.sub' into '.template', optimized; 
							3. RuleParser: 
								1. .rules: Rule[]
								2. .compile() - compiles the '.rules'; Returns the 'CompiledRuleParser' (a function); 
									The CompiledRuleParser drills through a tree of rules based off a given 'TokenStream'. 
							4. '.type: Type' - analogous to 'Token.type', used for element categorization and quick identification in 'HashMap'/'LinearIndexMap'/other-places; 
								Permits using 'Rule' as 'TokenInstance' [of which 'TokenStream' is a Stream]; 

						Performance increase: 
							1. removes the need for recursive 'this.isCurrEnd()' checks, when advancing to a new high-level token;

						Performance decrease [PROBLEM!]: 
							1. potentially - a need to "look" for the right rule [linear search - bad]
								SOLUTION: create optimization techniques for them - specifically: 

									1. COLLECT all the Rules that start with 'IdentityRule' [includes DRILLING]
									2. For ElementaryRule-s, COLLECT the rules that start with them by underlying '.type' (0th element): 
										THEN - proceed with this, KEEP collecting a bucket inside-the-bucket by Token-'.type' for 1st element, 
											and so forth, UNTIL all buckets have a size of 1, and we can CHAIN the given 'Stream', 
												by means of repeatedly doing 'bucket.get(x.type)', for each new 'x'; 
									3. Create an '.order'-based optimization 
										(namely - if '.type' is a number/orderable, one uses binary search of '.type' for the '.order' [number/orderable] 
											on each one of the given array of 'Rules'); 
									4. All the other rules - collect them into a single "linear" bucket; 
												
									This way, we are capable of killing two birds with one stone:
										1. choose a 'Rule'
										2. advance the current 
										3. minimize backtracking ["deeply" checking for a rule];
											The PRECISE order of attempts would be determined by the user. 
											Default is - start with 'IdentityRule', proceed with '.type-at-[0]'-based
												(max backtracks per element of output Stream - ((sum of cardinalitities of these two sets) - 1) )
									
									The optimization structure, however, will need to be separate from the 'ElementaryRule'; 
									PROBLEM: 
										How to organize it?
										The RuleParser HAS to be general, but one also wants to add this optimization.
										IDEA: make it part of 'RuleParser.compile()', via 'instanceof IdentityRule' and 'instanceof ElementaryRule' (then, use '.type' to group...). 
											As these things are happening AT COMPILE TIME for the given parser, 
												one can afford the lavishes of actually doing all that...; 
	
	55. MultiMap - [IndexMap-like] this returns THE LAST matched item, running multiple results [with REQUIRED function-values, and only unless early termination has been demanded by a return value]; 

[maybe, for v0.4!]
3. Dependency management: 
	3.1. [maybe???] Create a 'refactor' library, specifically designed to refactor common design patterns; 
		In particular, it would have: 	

			1. 'delegate' functions [from here]; 
			2. A 'PropertyDescriptor' class; 
				2.1. This one is for creation of 'Object.defineProperty' signature-objects (the "property descriptors"); 
			3. The 'Constructor' type; 

	3.2. Create automatic tool for performance of redundancy analysis of JavaScript functions/classes/components; 
		Basically - it walks through the project tree, looking for functions/things that aren't GETTING USED/Called ANYWHERE/ANYHOW; 
		Permits one to remove items that are of no importance, simplifies development...; 