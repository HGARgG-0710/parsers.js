[GENERAL]
1. Check if this works with Deno; 	

[v0.3]

Order of TODO-elimination: 
	0.4.
	A.0 [fundamental]
	A.1 
	A.2
	A.3
	
0.4. FIX THE TESTS [again, for the final time, hopefully...]: 
	0.4.1. Re-order the directory structure [the 'NavigableStream' - make it 'StreamClass/utils.ts']; 
	0.4.2. MultiIndexModifier;
		0.4.2.1. Add the '.init' method test; 

A. testing: 

	0. FUNDAMENTAL: 
		0.1. INDEX [with 'number's] the 'signatures' for test-suites with INDEXES [thus, one will be able to know which one precisely failed...]; 
		0.2. Decide how much of generics must be used inside the test-signature types [some use a lot, while others are satied with 'any']; 
			0.2.1. DECISION: walk through the tests, make a set of ad-hoc desisions... [sometimes, they're good, sometimes they're just useless fluff]; 
				LIST them first; 
		0.3. Add '.toString()' to all the 'it' and 'describe' calls, when arguments for tests are used [to be able to distinguish failures and successes]; 

		0.4. Create the 'run.ts' script for running ALL the test specific suites AT ONCE [in sequence, that is...]; 
			0.4.1. This will ensure one's ability to see the global code correctness despite making small changes to a particular (possibly sensitive) area of the project...; 
		
		0.5. Decide what to do with things like 'ambigiousMethodTest' - that require a SPECIAL 'compare' function; 
			Whether one should not just compare by references instead [where possible/makes sense]; 
			This'd simplify both the tests, and the suites (albeit, make the suites mildly uglier due to need for pre-caching and partial creation of recursive/table-structures); 

			DECISION: YES, do that; 

	1. Changes to tests [for modules]: 

		1. Stream-s: 
				1.1.1. Create a plain '.init' test; 
					All StreamClass-es have it; 
					Namely, this is a 'ReInitializationTest', which (fundamentally) looks to verify correctness of behaviour upon RE-INITIALIZATION of the given Stream; 
						It is done like: 

							1. '.init' is called with given test arguments [this sequence 1.-... is done several times]; 
							2. The entire test-suite is RE-DONE, but WITHOUT needing to create an instance...; 
								2.1. A different signature is given; 

							So, basically, one needs to use a ChainClassConstructorTest together with '.init'; 
							REFACTOR THIS [once again...]; 
				1.1.2. TEST the '.pos', where it is relevant; 
					There must be SPECIFIC checks for it (namely - a SEPARATE method, that goes through the entire Stream, comparing the differences between the '.pos' - that they ALWAYS be 1); 
					This should be a single method, that is used in ALL the StreamClass-based .pos-generated test suites; 
				1.1.3. FIX the 'streamNavigateTest' - IT MUST also check for the correct value of '.curr' [NOT JUST the return value...]; 	
				1.1.4. [Parser/] Do the tests for 'StreamTokenizer' - this one must be tested like a StreamClass [follow the scheme that the others have already used...]
	2. Design Utils' tests (and write suites for them...): 
		These are tested in a regular manner via the 'utilTest'; 
		Re-do them; 
		List the utils to be tested for each particular class, add tests for them; 
		DO THEM ACCORDINGLY [refactor using 'lib/lib.ts' on-the-spot]
	3. Implement test suites: 
		3.1. Stream-s: 
			3.1.0. Methods:
				To implement [generally]:
					generalIteratorTest (based off 'iterationTest'); 

				To add per class: 
					[predefined - all]
					3.1.0.1. generalIsEndTest; 
					3.1.0.2. generalIsStartTest; 
					3.1.0.3. generalRewindTest; 
					3.1.0.4. generaFinishTest; 
					3.1.0.5. navigateTest (based off 'streamNavigateTest' + comparisons); 

				General plan [to be iterated for each Stream-class test suite]: 
					1. per-case defined (.copy, .navigate); 
						1.1. .navigate is for all Streams; 
					2. Boilerplate-implementations (generalIsEndTest, generalIsStartTest, generalRewindTest, generalFinishTest); 
					3. Comine into a Stream class test-suite; 
			
			3.1.1. "WrapperStream"-combinations; 
				Thus, for instance, one has to test ALL of: 

					LimitedStream-NestedStream-InputStream;
					TransformedStream-LimitedStream-TreeStream;
					NestedStream-TransformedStream-InputStream;
						And more...
				
				FOR THIS, one needs to be able to do CompositionClassTests; 
				Introduce a new function 'classCompose', which does '(...classes) => trivialCompose(...classes.map(classWrapper))'
					[produces a function for creation of an instance that is the result of a chain (new X)<-(new Y)<-... of classes]

					The tests for them work THE SAME WAY as the top-level cases, BUT they have a different constructor; 

			3.1.2. Test Instances: 	
				3.1.4.1. They branch: 
					3.1.4.1.0. Empty Stream instance
					3.1.4.1.1. Single-element Stream instance
					3.1.4.1.2. Two-element Stream instance
					3.1.4.1.3. Long Stream instance (>= 20 elements)
				3.1.4.2. There are optimization-specific benchmarks; 	
					3.1.4.2.1. In particular, IDENTIFY THE BOTTELNECKS IN DEFINITIONS [if there are any]; 
		
		3.2. Parser/
			Test Instances:
				3.2.1. There is a set of Stream-s, for which they're tested; 
				3.2.2. With appropriate Collection-s; 
				3.2.3. They branch [by 4.1. and 4.2.]: 
					3.2.3.1. The Empty Stream; 
					3.2.3.2. The One-element Stream; 
					3.2.3.3. The Two-element Stream; 
					3.2.3.4. The many-element Stream; 

				[Grouping by test suite type, for each type of parser:]	
				1. LayeredParserTest - itself
				2. TableMapTest - itself
				3. GeneralParserTest: 
					1. PatternEliminator
					2. PatternTokenizer
					3. PatternValidator	
					4. BasicParser; 
					5. SkipParser; 
						5.1. FixedSkipParser; 
							5.1.1. StreamParser; 
					6. StreamLocator; 
					7. StreamValidator; 
					8. PositionalValidator; 

		3.3. regex/
			Re-structure to fit the current mini-testing framework (expand the framework appropriately); 

		3.4. benchmarks/	
			4.4.1. Stack benchmarks: 
				4.4.1.1 NestedStream - find out how deep a given NestedStream must be in order for the "default" Stack to break; 
					Use as metric later
			4.4.2. Parse really long strings: 
				4.4.2.1. Try to parse some incredibly long strings;
					See the speed limits for different string sizes
			4.4.3. "Cached" TokenInstance vs "Non-Cached" TokenInstance: 
				4.4.3.1. Auto-generate a GIGANTIC tree in a simple syntax made entirely of TokenInstance-s, then do a version for BOTH cached and non-cached 
				4.4.3.2. Iterate through them [TreeStream]; Measure time; 
					NOTE: it is expected that the "cached" 'TokenInstance' will be A LOT faster to build/do; 
				
		3.5. imports/ 
			The imports are flawed - test them properly [import each and every layer inside the library...]; 

		3.6. Pattern/
			4.6.1. Test instances: 
				4.6.1. Empty; 
				4.6.2. One-element; 
				4.6.3. Two-element; 
				4.6.4. Long; 
		
		3.7. constants/
			Check that those correspond to the constant values expected 'assert.strictEqual';
		
		3.8. IndexMap/
			Every single class/case has 2-3 different instances [depending on the need for thoroughness...]; 
		
B. documentation (change/fix it...);
	B.1. Create new docs pages:
		B.1.1. Home: 
			B1.1.1. Motivation (write that the library was created/conceived primarily as a mean of refactoring of frequent parsing tasks/data-structures); 
			B1.1.2. "One good approach" principle (write that the library was made to have a single "intended" best approach for all the things that are possible to do using it); 
			B1.1.3. Examples [projects done using it]; 
			B1.1.4. Structure - explain how the library structure works (interfaces., methods., classes., utils.); 
			B1.1.5. Known issues (this lists known problems that the library currently has);
			B1.1.6. Terminology
				This mentions that the library has a distinct terminology [albeit, not very large], that is useful for understanding its concepts; 
		B1.2. Properties: 
			This describes individual properties (and methods!) that can ocurr on different classes; 
			ONLY describes the properties, methods that ocurr more than once in the library [and have the same meaning]; 
		B1.3. Classes: 
			Describes individual classes; 
			THEY ARE LISTED AS EXTENSION-HIERARCHIES; 
		B1.4. Class-Factories: 
			Describes functions for creation of classes (ex: StreamClass, IndexMap, HashMap); 
		B1.5. Interfaces; 
			Lists the library's interfaces. 
			They are described via an, extends-implements hierarchy [where classes that implement the interfaces are the lowest level elements of the tree (the "leaves")]; 
		B1.6. Usage Notes: 
			Minor notes for the library usage. 
					
				NOTE [1]: identifying NestedStream-s that are elements as a part of a recursively-defined StreamTokenizer; 
					To define this is (typically) trivial, via having '.type' of a given 'Stream' to be 'undefined', or 
						not a part of a given NestedStream's '.typesTable';

	B.2 [while working on the concrete documentation]: 
		1. Patch up the type definitions: 	
			Pay PARTICULAR attention to the methods return values; 
			Those are especially likely to be mangled up, due to the way that the methods are defined...; 

	WHEN WRITING DOCUMENTATION, make a note that the '.hash' and '.extension' functions SHOULD BE ABLE TO HANDLE null-values! 
C. rewrite the CHANGELOG [too much has been altered since the "original" v0.3]; 
	C.1. Walk through the last commit's state of v0.2.1, noting changes and writing them down [destructive]; 
	C.2. Walk through the v0.3 changes, and write them down [constructive];

[v0.4]

1. STACK! 
	The library is not too stack heavy, BUT it does have an (ultimate) stack-limitation
		when working with NestableStream-s and the 'nested' util (generally, anything that is NON-FLAT);
	This is very much impossible to solve on its own; 

	SOLUTION: use the CPS style. 
		Re-implement library (whilst keeping the non-CPS version) in CPS, 
		with an infinite 'stack'; 

	! THIS WILL REQUIRE FOR ONE TO FIRST IMPLEMENT THE INFINITE CPS-BASED STACK LIBRARY!
	That one should: 
		0. Be written in TypeScript; 
		Include Types: 
		1. InfiniteStack; 
		2. CPSArray (this is, ultimately, a CPS version of JS array);

	As, CPS is (quite) time-costly to actually run (due to amount of stack usage [primary] + the stack unwinding + additional checks...),
		this sort of thing would be useful for applications where speed of little importance
			(that is: NOT things like apps/websites/interpreters, and so forth; Stuff like AOT-compilers, perhaps);

	HOWEVER: 

		1. As said previously, it IS quite costly to do CPS, so: 
			1.1. See, whether it's feasible from time-perspective (how much slower it is, whether it's possible, or whether it's way too slow); 
			1.2. If not feasible - abandon the whole CPS idea completely; 

			To find out - benchmark; Write a sketch for parsing a very nested expression [sufficiently nested to crash the current API] 
				using the Infinite-Stack-CPS approach, then - measure and decide...; 
	
2. New Streams/Parsers + some minor stuff/code-quality/minimalism: 

	2.1. bufferize(tree, size) - util

		Given a 'Tree', returns an array of values of size `size`: 

			{
				childNumber: number,
				value: any
			}

		The `size` is a fixed number >= 0, 'childNumber' is the number of children, ahead of current node in the array.
		To be used for creating a persistent linear data structure for working with the given AST. 
		Good for cases, when the number of items is known, and is large (part of the the "big input" optimizations); 

		Would permit user to store a '.state.size: Pattern<number>' property (Pointer(number)) on the ParserState-s, 
			for counting the size of the input in terms of nodes, then re-use it with 'bufferize', 
				and use the tree for evaluation; 

	2.2. TreeStream: create two new versions; 

		The 'TreeStream' (currently) is the 'LL' pre-order tree traversal algorithm Stream-implementation; 
		There should be ANOTHER - the 'LR' post-order tree traversal; 

		1. Rename the 'TreeStream' to 'TreeStreamPre', and create an implemnetation for the 'TreeStreamPost'; 
			This is (particularly) useful for some types of interpreters; 

			The Pre visits the node, then its children one-by-one starting from the beginning, then the siblings; 
			The Post visits first the children, then the siblings, only THEN the parent [when there are NO MORE children - then the parent is visited]; 

			Basically, Pre (parent-to-children) returns the nodes 'as they go' in the left-to-right traversal in the tree,
				whereas Post returns them in 'reversed' order: for the parent to return, children must all return first; 

			[Example usage: transform the binary '+' -- a + (b + c * (d + (k + f * r))) -> a + b + c * (d + k + f * r)]; 

		2. Create a 'TreeStreamSwitch', which would be capable of "choosing" [based off underlying IndexMap] 
			whether to give items in the 'Post' order (parent first - children after), 
				or the 'Pre' order (children first - parent after);
			
			This would be a class-factory; 

	2.3. Implement a 'PartialValidator': 
		This, ultimatly, works in THE SAME WAY as the 'PatternValidator' and 'PatternTokenizer'
		Gets the recursive structure with all the 'split' and 'inserted' bits; 
		The 'PartialValidator' would RETURN THE INFORMATION FOR INFERENCE OF CORRECT/INCORRECT PARTS! 
		[Which it does by 'numbering' particular portions of a given Pattern]; 
		This would allow for a greater ability to analyze/point-out the errors in the original input;

		Unlike plain PatternValidator, it would return ALL the possibly available error information, not just the stuff about the index of the "failed" check, or coverage; 

	2.4. Implement a 'level-by-level TreeStream': 
		It would get an initial given 'level' (tree root), walk through all of its '.children', 
			then walk each of the '.children' themselves as levels (one after another...), THEN descending lower...; 
		Would do it via calling '.getJointLayer' function (then - caching for later '.prev'); 
		This (basically) takes the nodes from the previous 'joint layer', gets their respective children,
			CONCATENATES THEM into a new layer, then iterates it; 

	2.5. Add a module SPECIFICALLY for working with strings/identifiers/Sequence-s (currently: the 'Indexed' interface); 
		In particular, functions/abstractions: 

			1. shorten(names); 
				Given a sequence of "names" (strings/sequences),
					and a 'comparison' predicate for each of its elements,
						it would implement a name-shortening algorithm, 
							that would preserve the name uniqueness;
				The return value is an IndexMap; 

			2. renameable(names);
				Given an IndexMap of names (which could come from, for instance, 'shorten'), 
					the method: 

					1. Re-orders;
					2. Creates new name-map entries;

				In a fashion that would allow straightforward implementation of a sequential "exact replacement algorithm" based on the output: 
					1. Loop through a list of name-maps; 
						2. [In the loop] Rename current encounters of a name with its mapped value;	
					
				There are 2 operations that may be necessary: 
					1. creation of temp-names (when collisions in present names are far too high to re-order);
					2. re-order; 
				
				The algorithm for the 'renameable' function: 
					0. Keep the cached (met) names in a 'Set'; 
					1. Loop through the 'names' given: 
						1.0. If the name is in the cached Set, continue; 
						1.1. If the current 'value' of the 'name' is already present amongst 'keys' (another loop): 
							1.1.1. put the keys in question BEFORE the current value;
							1.1.2. '.swap' the two indexes;
							1.1.3. go one position back (because now one needs to check the "others" now); 
							1.1.4. cache the current amongst the checked names in a 'Set', (so that one doesn't get into an infinite loop);

				This is useful when implementing things like mass renamings/name shortenings in code mangling software; 

	2.6. RELAX overly demanding interface and types definitions/requirements;
		Example: inputStreamCopy, inputStreamNavigate; 

			These do not NEED to have all the parts of the 'InputStream' definition (even though they are originally implemented to work with it);
			RE-DEFINE them...;

	2.7. [Idea?] Create an error-handling API for the 'v0.4';
		Create means of: 

			1. Stacking different levels of exceptions (the 'try-catch' blocks); 
			2. Assigning relevant debug information to them;

		This is (primarily) for the development process of the parsers; 
		
		Define a Catcher [rough sketch]: 

			function Catcher (info: DebugInfo, logger: Function) {
				return function (thing: Function, thisArg: any, args: any) {
					try {
						thing.call(thisArg, ...args)
					} catch (e) {
						logger(info)
						throw e // NOTE: THIS here is to represent DEPTH [as parsers can be VERY recursive indeed, it may be needed to eliminate the recursive errors on a case-by-case basis]; 
					}
				}
			}

		Although... This is rather general. Perhaps, better implement as a separate package; 
		Likewise, there'd be ways to count levels of recursion, and assign types to them [specialized signature]; 

	2.8. [maybe?] BreakableStream - a major generalization of the library's patterns: 
		Returns a class for producing Stream-s: 

		1. Have an '.input' (another Stream); 
		2. Has an 'Indexable' of 'BreakPattern's (they are objects) pre-given, which define how the Stream instances behave: 
			2.1. type: string - one of: 
				2.1.1. limit - works like LimitedStream; 
				2.1.2. nest - works like NestedStream; 
				2.1.3. halt - stops the stream [no more items after that]; 
				2.1.4. navigate - finds: 
					2.1.4.1. Either the next item [or the first item AFTER THE BEGINNING] that obeys a given property (PredicatePosition)
					2.1.4.2. Goes to a static position [relative or absolute]; 
					
					ALSO, has a boolean arg 'isRelative' (default: true, basically - whether the values for 'navigate' are to be handled absolutely or not...); 
				2.1.5. transform - applies a given transformation on the '.input' (calls a function); Returns the result;
			2.2. args: object - the way that the 'type' is resolved (equivalent of constructor-arguments/.init-arguments); 
			2.3. buffer: boolean | () => boolean - this one corresponds to whether the current value should be bufferized; 
				When a function is passed, it is called on the 'BufferizedStream' in question to get the value; 
		
		The result of matching 2. to the '.input.curr' is (effectively) evaluated to be the Stream's '.curr'; 

		This todo is a 'maybe?' because all the said things can (pretty much) already be done by the user using StreamTokenizer and the rest
			- this really only just provides a minor abstraction over the whole thing (frees the user from needing to use the exports of the library); 
		This kind of thing is (somewhat) against the library's "all-minimialism" approach; 

		On the other hand, it would permit one to use a very domain-specific language to describe operations on streams conventionally, 
			thus - simplifying semantics + unifying a very large number of different abstractions that are creatable via the library using just this one; 	
		Conclusion: think about it, more of a YES, tha a NO currently;
	
	2.9. [reminder] Optimize minitua (that is, just go through the code, removing unnecessary repetitions of things, useless work and so forth...); 
		When the same (even tiny) operations get repeated a large number of times, the speed decay accumulates; 
		Try to make the library code that avoids this stuff fundamentally; 

		2.11.1. DO ACTIVE MICRO-BENCHMARKING!
			Having general/particular implementations in mind, MICRO-BENCHMARK, 
				and on the basis of those, optimize particular functions/approaches; 
	
	2.10. utility - 'nestedInfo'; 

		1. Finds a depth of a given Stream, based off 'inflate' and 'deflate'; 
			NOTE: the MAX depth achievable; 
		2. Finds its length - how many positions (in 'number') must one walk before the current nestedness ends, from the initiated position; 

		Returns it as a pair; 

	2.11. BufferizedStream - a wrapper around a given Stream, to enable the storage of a buffer
		for its operations; 

		Once '.next' is called, it stores the '.curr' inside the inner 'buffer', ONLY IF the user-given '.predicate'
			HOLDS!	 

	2.12. Idea for a method: ProlongedStream.prolong; 	

		This (utilmately) modifies the 'this.streams' [modifying the '.isEnd' accordingly]; 
		Also, there'd be '.shorten(n: number)' - it'd delete a certain number of elements from '.stream'-s, 
			optionally changing '.curr', and set '.isEnd = true' (if the .curr is in one of the deleted 'Stream'-s); 
	
	2.13. [Maaayybe?] Create a 'samples' directory; 
		This is for 'common-case' parsing - generally, walk through the various syntaxes that were 
			PRESENT within the previous parsers, use them?
		
		Or, better, make this into a separate mini-project called 'parsing-samples'; 
		[EXAMPLE: handling the 'escaped' strings/sequences all the time];

		Think about it [this particular refactoring is VERY fine and case-specific, though, it can come in useful...]; 

		No, if one is to do it, the thing should: 

			1. Work with general parsing patterns, that would be CONFIGURABLE for every case; 
			2. Provide low-level Token-s with specialized given names; 

	2.14. IDEA: generalize the type of 'T<K> = CollectionLike<K | T<K>>' properly; 
		It appears at least in 2 places inside the library: 

			1. Tree (InTreeType)
			2. NestedStream;
		
		This might (also) resolve some 'interface'-related issues for the 'NestedStream'; 
	
	2.15. v8-specific optimizations: 
		Having done some benchmarking on some typical Array methods, one arrived at a...
	
		CONCLUSION: 
			1. Handwritten methods are *often* __MUCH__ faster than the builtins ON LARGE PIECES OF DATA; 
				On the more common-place cases, however, they are inferior in time performance; 
			2. THEREFORE, one should do: 
				Re-organize the library (how?) to permit usage of the current methods on LARGE pieces of data; 
				Prior - do more benchmarking; 

				About re-organization: 
					1. Think - whether to introduce explicit low-level 'size-controls' [possibly costly, when run a lot]; 
						Although, it's likely to be optimized/negligible; 
					2. Or, whether to split the thing onto 2 chunks [where the 'size' is precomputed...]; 
						This is less flexible (and thus, error-prone), plus will be more difficult to re-structure; 
					
				Current vote is for 1.; 

				ALSO, about bounds - those should be CAREFULLY BENCHMARKED; 
		
		MORE GENERALLY - try to adapt the library for work on LARGE datasets; 
		It'll make it useful for working with big projects/pieces-of-text; 

		THINGS OF ISSUE TO THINK ABOUT IN PARTICULAR: 	

			1. What "LARGE" sizes are practical? [meaning - sources of what sizes can occur in the wild?]
				Take them as large as one can. At least 10000000 symbols (~30MB, ~100000 lines of 90-100 chars), maybe try more; 
			2. How much of a performance difference can one get by doing these optimizations?
				In particular - try to sketch out a parser for something (the smtf format of one's own? it's simple to parse), perform 
					profiling on a REALLY big auto-generated file, then: 
						
						1. take overall time measurements (performance.now()); 
						2. profile, break the execution down on several tiny pieces; Then - benchmark and optimize them accordingly; 
							For this, use results from samples from one's benchmarks library;  
	
	2.16. [maaayyybe??] Bring the old planned stuff for v0.3 back; 
		That is the 'Parsers/utils.ts' - 'transform', 'delimited', 'consume' [equivalent of 'LimitedStream', uses 'Collection'-s]; 
	
		Do only if there are code aspects suffering from this significantly: 

			1. readability/accesibility; 
			2. performance; 
		
		Primary cause for this todo is that, despite being "slow" (memory allocation), 
			they did still have the benefit of semantic simplicity; 

		The only cause for performance issues with OOP Stream-s API is overhead from re-structuring the original loop so much
			[need for calling the '.isCurrEnd()' two times more]; 
		Again, do this ONLY if the performance difference is SIGNIFICANT!
	
	2.17. util - enumerateTree; 
		Given an array of "types" of nodes, it recursively converts their properties (given in the 'shapes' array), 
			to single-array-siblings form (the current ChildrenTree); 	
		This would permit a more "variable" set of trees (those that have properties with fixed names), 
			to be enumerated accordingly...; 

		Example: 

			{
				a: any
				b: any
				c: any
			}

		Becomes: 

			{
				value: [
					// ... a, b, c
					// ... or b, a, c
					// ... or any other order
				]
			}
		
	2.18. About generics - relax them; 
		Use ONLY WHEN NEEDED; 
		For example: a method makes use of X generics, out of which Y < X is needed [id est, used somewhere beyond the 'this: ...' in the parameters list]; 
			REMOVE the unneeded ones; 
			RE-ORDER THEM, if need be; 
			Try to make them more minimal...; 

	2.19. [maybe?] Rename the 'array' (Parser/utils.ts) into 'collection' [name clarity]; 
	2.20. [maybe?] Create a module for randomized Stream-generation (`random`);
		(given, say, an InputStream with an Array of expected values inside of it [that can be picked for Nested], 
			it will choose a pair of them for different [distinct] boundries); 
		Similarly - one could give an Array of limiting characters for a LimitedStream to be defined by...; 
	
	2.21. New type of Tree - a BacktrackableTree, takes up more memory, but is (ultimately) faster to use in TreeWalker [when going backwards inside the Tree]; 
		In the 'TreeWalker.renewLevel', it does the 'this.stream.pos.slice', which is O(n) complexity relative to 'n' - number of levels in a tree; 	
		A 'BacktrackableTree' [interface] is one that has a method: 

			1. backtrack(pos: number)

		Which returns the 'pos'-parent [that being, goes "up" the level in the tree]; 
		Make the old 'Tree' into the 'BacktrackableTree', create a 'ParentTree' [implementation], where 'backtrack' is defined as: 

			backtrack = (pos: number) => {
				let result = this
				while (pos--) result = result.parent
				return retult
			}
		
		Which is O(1) to get the immidiate parent, and O(p) relative to the number of levels to go back (`p`); 
		For this to work (the '.parent' property), the Tree must be CONSTRUCTED in a certain fashion; 
		Add more items to the interface: 

			1. pushChildren(children: any[])
			2. clearChildren() 
		
		The 'ParentTree' implementation would (therefore) assign '.parent = this' on all the pushed children in '.pushChildren', and do 'this[propName] = []' in 'clearChlidren'; 
		[IMPORTANT!: Make a 'ParentTree' implementation work with the 'ChildrenTree'-s, that is it'd have its own 'propName']; 

		THEN, SPLIT the 'renewLevel' onto 2 functions [one of which uses the 'this.stream.curr' (in '.goPrevLast'), and the other - that uses '.backtrack(1)']; 
			Better still - for optimization purposes, let it CHOOSE which implementation to use DEPENDING on the number of parent levels and pre-parent levels;

	2.22. Add a fast implementation of '.finish' to the 'TreeStream'; 
	2.23. Re-structure the tests to be made on a per-method basis? 
		Current per-class suites are rather verbose (don't do this until v0.4, already spent too much time on the tests...); 
	
	2.24. INTRODUCE the '.state' property for state-keeping; RE-INTRODUCE the '.copy()' method and 'Copiable' interface; 
		Comes inside the 'StreamClass' (optionally) via the 'hasState' property;
		Will additionally be supported by: 

			1. 'StreamClass(...).prototype.copy()' method; 
				Solution for '.copy': 
					1.1. Keep the instance variables inside the single '.vars' property ('.preventsExtension' cast upon it); 
					1.2. Keep the user-defined variables under '.state'; 
				
				Definition [loose, has to have overloads, depending on presence/absence of '.pos']: 

					function uniCopy (stream: ...) {	
						const copy = new (Object.getPrototypeOf(stream).constructor)()
						copy.init(stream.vars, stream.state)
						return copy
					}

			2. 'StreamClass(...).prototype.init(vars, state)': 
				Will (by default) assign the '.vars' to be 'vars' and '.state' to be 'state'; 

		Then, the various 'Indexable'-s can use the '.state' inside the '.index' call; 

		ALSO, ADD: 
				1. New 'HashMap' (and IndexMap) aliases to work with ParsingState; 
				2. New 'HashMap' (and IndexMap) aliases to work with '.state' property; 
		
3. Dependency management: 
	3.1. [maybe???] Create a 'refactor' library, specifically designed to refactor common design patterns; 
		In particular, it would have: 	

			1. 'delegate' functions [from here]; 
			2. the 'classWrapper' function; 
			3. the 'AssignmentClass' function; 
		
		Expand the list; 