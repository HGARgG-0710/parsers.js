[GENERAL]
1. Check if this works with Deno; 	

[v0.3]

REMINDERS: 
	1. [when running the 'import' tests] The 'methods.ts' for 'TokenizablePattern' and 'ValidatablePattern' CAN FAIL (a lot of change there has happened - need an update); 

4. [Part of the docuementation effort] Add the JSDoc; 
	Let it be brief (very), and work like: 

		1. Contains a short (SHORT) description of the given class/method; 
		2. Contains a reference to the proper documentation; 
	
	The only benefit of actually having the thing is the ability to reference the "big docs"

6. FIX the 'tests/run.ts' to be able to run ONLY SPECIFIED LOCATIONS: 
	6.1. introduce a path-map [for names shortcuts used from command-line]; 
	6.2. allow multiple names to be used; 
		6.2.1. Write a simple parser for this format [use plain JavaScript - not the library itself]; 


Order of TODO-elimination: 
	A.3.3	
	A.1.1.1.

	A.0.10. 
	A.3.1.1.
	A.3.
	A.0.2. 
	A.0.7.
	
	A.0.9.
	A.0.8. 
	6.

	[fix test compilation errors]
	[run the tests]

	B. 
	4. 
	C.
	
A. testing: 

	0. FUNDAMENTAL: 
		0.2. Decide how much of generics must be used inside the test-signature types [some use a lot, while others are satied with 'any']; 
			0.2.1. DECISION: walk through the tests, make a set of ad-hoc desisions... [sometimes, they're good, sometimes they're just useless fluff]; 
				LIST them first; 
		
		0.7. Tidy up the test suite signatures - make uniform names for similar properties, introduce naming conventions; 
		0.8. Refactor the 'let times = X; while (times--) it('${X - times}. ${STRING}', FUNCTION(X - times))' into 'timesTest'; 
			When one needs to run a certain test several times, basically...; 

			0.8.1. Use the 'i. ...' thing with the in-signature arrays; 
			[Example: incTimes inside of 'MultiIndexModifier']; 

		0.9. Introduce the 'instance' it-function [to contrast with 'times', and to also specify the INSTANCE that is used...]; 
			Use on the top-level of suites; 
			PASS the 'i' along with the 'signature' inside 'signatures' to acommodate for this...; 

		0.10. Add proper testing mini-framework support for the checking predicates; 
			For "composite" Streams' tests one will HAVE to be doing the 'comparing'
				(because it'll be much easier to specify a "structure" to be checked, instead of trying to fish out the references, then use them for constructing tests); 
			THEREFORE - when it comes to this, do; 

			0.10.1. AFTERWARDS - modify the Stream-s test suites to include OBJECTS [not just the things comparable by reference directly]; 

	1. Changes to tests [for modules]: 

		1. Stream-s: 
				1.1.1. Create a plain '.init' test; 
					All StreamClass-es have it implemented INDIVIDUALLY [due to different signatures - v0.4. won't have such an issue]; 
					Namely, this is a 'ReInitializationTest', which (fundamentally) looks to verify correctness of behaviour upon RE-INITIALIZATION of the given Stream; 
						It is done like: 

							1. '.init' is called with given test arguments [this sequence 1.-... is done several times, depending on the '.init' arguments]; 
							2. The entire test-suite is RE-DONE, but WITHOUT needing to create an instance...; 
								2.1. A different signature is given; 

							So, basically, one needs to use a ChainClassConstructorTest together with '.init'; 
							REFACTOR THIS [once again...]; 

	3. Implement test suites: 
		3.1. Stream-s: 
			3.1.1. "WrapperStream"-combinations; 
				Thus, for instance, one has to test ALL of: 

					LimitedStream-NestedStream-InputStream;
					StreamTokenizer-LimitedStream-TreeStream;
					NestedStream-StreamTokenizer-InputStream;
						And more...
				
				FOR THIS, one needs to be able to do CompositionClassTests; 
				Introduce a new function 'classCompose', which does '(...classes) => trivialCompose(...classes.map(classWrapper))'
					[produces a function for creation of an instance that is the result of a chain (new X)<-(new Y)<-... of classes]

					The tests for them work THE SAME WAY as the top-level cases, BUT they have a different constructor; 

			3.1.2. Test Instances: 	
				3.1.4.1. They branch: 
					3.1.4.1.0. Empty Stream instance
					3.1.4.1.1. Single-element Stream instance
					3.1.4.1.2. Two-element Stream instance
					3.1.4.1.3. Long Stream instance (>= 20 elements)
				3.1.4.2. There are optimization-specific benchmarks; 	
					3.1.4.2.1. In particular, IDENTIFY THE BOTTELNECKS IN DEFINITIONS [if there are any]; 
		
		3.2. Parser/
			Test Instances:
				3.2.1. There is a set of Stream-s, for which they're tested; 
				3.2.2. With appropriate Collection-s; 
				3.2.3. They branch [by 4.1. and 4.2.]: 
					3.2.3.1. The Empty Stream; 
					3.2.3.2. The One-element Stream; 
					3.2.3.3. The Two-element Stream; 
					3.2.3.4. The many-element Stream; 

				[Grouping by test suite type, for each type of parser:]	
				1. LayeredParserTest - itself
				2. TableMapTest - itself
				3. GeneralParserTest: 
					1. PatternEliminator
					2. PatternTokenizer
					3. PatternValidator	
					4. BasicParser; 
					5. SkipParser; 
						5.1. FixedSkipParser; 
							5.1.1. StreamParser; 
					6. StreamLocator; 
					7. StreamValidator; 
					8. PositionalValidator; 

		3.3. regex/
			Re-structure to fit the current mini-testing framework (expand the framework appropriately); 

		3.4. benchmarks/	
			TODO: TO ENSURE the usefulness of the 'imports' tests, use the '.'-imports here (meaning: 'import {X} from "main.js"; const {...} = X[...]...'); 	
				This (a number of samples) + the overall thoroughness of the 'imports/' testing process will ensure that every piece of the library is reachable as intended...; 

			4.4.1. Stack benchmarks: 
				4.4.1.1 NestedStream - find out how deep a given NestedStream must be in order for the "default" Stack to break; 
					Use as metric later
			4.4.2. Parse really long strings: 
				4.4.2.1. Try to parse some incredibly long strings;
					See the speed limits for different string sizes
			4.4.3. "Cached" TokenInstance vs "Non-Cached" TokenInstance: 
				4.4.3.1. Auto-generate a GIGANTIC tree in a simple syntax made entirely of TokenInstance-s, then do a version for BOTH cached and non-cached 
				4.4.3.2. Iterate through them [TreeStream]; Measure time; 
					NOTE: it is expected that the "cached" 'TokenInstance' will be A LOT faster to build/do; 
				
		3.6. Pattern/
			3.6.1. Test instances: 
				3.6.1. Empty; 
				3.6.2. One-element; 
				3.6.3. Two-element; 
				3.6.4. Long; 
			
		3.8. IndexMap/
			Every single class/case has 2-3 different instances [depending on the need for thoroughness...]; 
		
		3.9. utils/	
			1. Walk through the project tree, enumerating the 'utils.ts' files; 
				1. IndexMap/utils.ts [design - check]
				2. HashMap/utils.ts [design - check]
				3. PersistentIndexMap/utils.ts [design - check]
				4. Parser/utils.ts [design - check]
				5. Pattern/utils.ts [design - check]
				6. Token/utils.ts [design - check]
				7. TokenizablePattern/utils.ts [design - check]
				8. ValidatablePattern/utils.ts [design - check]
				9. Position/utils.ts [design - check]
				10. InputStream/utils.ts [design - check]
				11. StreamClass/utils.ts [design - check]
				12. Tree/utils.ts		 [design - check; VERIFY TESTS FIRST...]
				13. src/utils.ts		 [design - check]
		
B. documentation (change/fix it...);
	B.1. Create new docs pages:
		B.1.1. Home: 
			B1.1.1. Motivation (write that the library was created/conceived primarily as a mean of refactoring of frequent parsing tasks/data-structures); 
			B1.1.2. "One good approach" principle (write that the library was made to have a single "intended" best approach for all the things that are possible to do using it); 
			B1.1.3. Examples [projects done using it]; 
			B1.1.4. Structure - explain how the library structure works (interfaces., methods., classes., utils.); 
			B1.1.5. Known issues (this lists known problems that the library currently has);
			B1.1.6. Terminology
				This mentions that the library has a distinct terminology [albeit, not very large], that is useful for understanding its concepts; 
			B1.1.7. Common naming conventions: 'is', repeating util/method names ('fromPairsList', '.getIndex', '.add', ... so forth); 
		B1.2. Properties: 
			This describes individual properties (and methods!) that can ocurr on different classes; 
			ONLY describes the properties, methods that ocurr more than once in the library [and have the same meaning]; 
		B1.3. Classes: 
			Describes individual classes; 
			THEY ARE LISTED AS EXTENSION-HIERARCHIES; 

			B1.3.1. Describe abstract classes: 
				1.3.1.1. FlushablePattern; 
				1.3.1.2. BasicPattern; 
				1.3.1.3. BasicSubHaving; 
				1.3.1.4. StreamClass({...}); 
			
		B1.4. Class-Factories: 
			Describes functions for creation of classes (ex: StreamClass, IndexMap, HashMap); 
		B1.5. Interfaces; 
			Lists the library's interfaces. 
			They are described via an, extends-implements hierarchy [where classes that implement the interfaces are the lowest level elements of the tree (the "leaves")]; 
		B1.6. Usage Notes: 
			Minor notes for the library usage. 
					
				NOTE [1]: identifying NestedStream-s that are elements as a part of a recursively-defined StreamTokenizer; 
					To define this is (typically) trivial, via having '.type' of a given 'Stream' to be 'undefined', or 
						not a part of a given NestedStream's '.typesTable';
					
				NOTE [2]: 'DelegateValidatablePattern': 
					
					1. the '.validator' is expected to ALSO remove any non-'Type' elements out of the 'result'; 
					2. when calling '.validate', the 'result[0]' should be 'true' ONLY when the source IS validatable (i.e. not already determined valid - contains only the 'Type' elements); 
					
		B1.7. Utils: 
			Categorize the utils ('is'-functions, 'from'/'to'-functions [ex: fromPairsList, toInputStream], 'uni'-functions, and so forth...); 

	B.2 [while working on the concrete documentation]: 
		1. Patch up the type definitions: 	
			Pay PARTICULAR attention to the methods return values; 
			Those are especially likely to be mangled up, due to the way that the methods are defined...; 

	WHEN WRITING DOCUMENTATION, make a note that the '.hash' and '.extension' functions SHOULD BE ABLE TO HANDLE null-values! 
C. rewrite the CHANGELOG [too much has been altered since the "original" v0.3]; 
	C.1. Walk through the last commit's state of v0.2.1, noting changes and writing them down [destructive]; 
	C.2. Walk through the v0.3 changes, and write them down [constructive];

[v0.4]

1. STACK! 
	The library is not too stack heavy, BUT it does have an (ultimate) stack-limitation
		when working with NestableStream-s and the 'nested' util (generally, anything that is NON-FLAT);
	This is very much impossible to solve on its own; 

	SOLUTION: use the CPS style. 
		Re-implement library (whilst keeping the non-CPS version) in CPS, 
		with an infinite 'stack'; 

	! THIS WILL REQUIRE FOR ONE TO FIRST IMPLEMENT THE INFINITE CPS-BASED STACK LIBRARY!
	That one should: 
		0. Be written in TypeScript; 
		Include Types: 
		1. InfiniteStack; 
		2. CPSArray (this is, ultimately, a CPS version of JS array);

	As, CPS is (quite) time-costly to actually run (due to amount of stack usage [primary] + the stack unwinding + additional checks...),
		this sort of thing would be useful for applications where speed of little importance
			(that is: NOT things like apps/websites/interpreters, and so forth; Stuff like AOT-compilers, perhaps);

	HOWEVER: 

		1. As said previously, it IS quite costly to do CPS, so: 
			1.1. See, whether it's feasible from time-perspective (how much slower it is, whether it's possible, or whether it's way too slow); 
			1.2. If not feasible - abandon the whole CPS idea completely; 

			To find out - benchmark; Write a sketch for parsing a very nested expression [sufficiently nested to crash the current API] 
				using the Infinite-Stack-CPS approach, then - measure and decide...; 
	
2. New Streams/Parsers + some minor stuff/code-quality/minimalism: 

	2.1. bufferize(tree, size) - util

		Given a 'Tree', returns an array of values of size `size`: 

			{
				childNumber: number,
				value: any
			}

		The `size` is a fixed number >= 0, 'childNumber' is the number of children, ahead of current node in the array.
		To be used for creating a persistent linear data structure for working with the given AST. 
		Good for cases, when the number of items is known, and is large (part of the the "big input" optimizations); 

		Would permit user to store a '.state.size: Pattern<number>' property (Pointer(number)) on the ParserState-s, 
			for counting the size of the input in terms of nodes, then re-use it with 'bufferize', 
				and use the tree for evaluation; 

	2.2. TreeStream: create two new versions; 

		The 'TreeStream' (currently) is the 'LL' pre-order tree traversal algorithm Stream-implementation; 
		There should be ANOTHER - the 'LR' post-order tree traversal; 

		1. Rename the 'TreeStream' to 'TreeStreamPre', and create an implemnetation for the 'TreeStreamPost'; 
			This is (particularly) useful for some types of interpreters; 

			The Pre visits the node, then its children one-by-one starting from the beginning, then the siblings; 
			The Post visits first the children, then the siblings, only THEN the parent [when there are NO MORE children - then the parent is visited]; 

			Basically, Pre (parent-to-children) returns the nodes 'as they go' in the left-to-right traversal in the tree,
				whereas Post returns them in 'reversed' order: for the parent to return, children must all return first; 

			[Example usage: transform the binary '+' -- a + (b + c * (d + (k + f * r))) -> a + b + c * (d + k + f * r)]; 

		2. Create a 'TreeStreamSwitch', which would be capable of "choosing" [based off underlying IndexMap] 
			whether to give items in the 'Post' order (parent first - children after), 
				or the 'Pre' order (children first - parent after);
			
			This would be a class-factory; 

	2.3. Implement a 'level-by-level TreeStream': 
		It would get an initial given 'level' (tree root), walk through all of its '.children', 
			then walk each of the '.children' themselves as levels (one after another...), THEN descending lower...; 
		Would do it via calling '.getJointLayer' function (then - caching for later '.prev'); 
		This (basically) takes the nodes from the previous 'joint layer', gets their respective children,
			CONCATENATES THEM into a new layer, then iterates it; 

	2.4. Add a module SPECIFICALLY for working with strings/identifiers/Sequence-s (currently: the 'Indexed' interface); 
		In particular, functions/abstractions: 

			1. shorten(names); 
				Given a sequence of "names" (strings/sequences),
					and a 'comparison' predicate for each of its elements,
						it would implement a name-shortening algorithm, 
							that would preserve the name uniqueness;
				The return value is an IndexMap; 

			2. renameable(names);
				Given an IndexMap of names (which could come from, for instance, 'shorten'), 
					the method: 

					1. Re-orders;
					2. Creates new name-map entries;

				In a fashion that would allow straightforward implementation of a sequential "exact replacement algorithm" based on the output: 
					1. Loop through a list of name-maps; 
						2. [In the loop] Rename current encounters of a name with its mapped value;	
					
				There are 2 operations that may be necessary: 
					1. creation of temp-names (when collisions in present names are far too high to re-order);
					2. re-order; 
				
				The algorithm for the 'renameable' function: 
					0. Keep the cached (met) names in a 'Set'; 
					1. Loop through the 'names' given: 
						1.0. If the name is in the cached Set, continue; 
						1.1. If the current 'value' of the 'name' is already present amongst 'keys' (another loop): 
							1.1.1. put the keys in question BEFORE the current value;
							1.1.2. '.swap' the two indexes;
							1.1.3. go one position back (because now one needs to check the "others" now); 
							1.1.4. cache the current amongst the checked names in a 'Set', (so that one doesn't get into an infinite loop);

				This is useful when implementing things like mass renamings/name shortenings in code mangling software; 

	2.5. RELAX overly demanding interface and types definitions/requirements;
		Example: inputStreamCopy, inputStreamNavigate; 

			These do not NEED to have all the parts of the 'InputStream' definition (even though they are originally implemented to work with it);
			RE-DEFINE them...;

	2.6. [Idea?] Create an error-handling API for the 'v0.4';
		Create means of: 

			1. Stacking different levels of exceptions (the 'try-catch' blocks); 
			2. Assigning relevant debug information to them;

		This is (primarily) for the development process of the parsers; 
		
		Define a Catcher [rough sketch]: 

			function Catcher (info: DebugInfo, logger: Function) {
				return function (thing: Function, thisArg: any, args: any) {
					try {
						thing.call(thisArg, ...args)
					} catch (e) {
						logger(info)
						throw e // NOTE: THIS here is to represent DEPTH [as parsers can be VERY recursive indeed, it may be needed to eliminate the recursive errors on a case-by-case basis]; 
					}
				}
			}

		Although... This is rather general. Perhaps, better implement as a separate package; 
		Likewise, there'd be ways to count levels of recursion, and assign types to them [specialized signature]; 

	2.7. [maybe?] BreakableStream - a major generalization of the library's patterns: 
		Returns a class for producing Stream-s: 

		1. Have an '.input' (another Stream); 
		2. Has an 'Indexable' of 'BreakPattern's (they are objects) pre-given, which define how the Stream instances behave: 
			2.1. type: string - one of: 
				2.1.1. limit - works like LimitedStream; 
				2.1.2. nest - works like NestedStream; 
				2.1.3. halt - stops the stream [no more items after that]; 
				2.1.4. navigate - finds: 
					2.1.4.1. Either the next item [or the first item AFTER THE BEGINNING] that obeys a given property (PredicatePosition)
					2.1.4.2. Goes to a static position [relative or absolute]; 
					
					ALSO, has a boolean arg 'isRelative' (default: true, basically - whether the values for 'navigate' are to be handled absolutely or not...); 
				2.1.5. transform - applies a given transformation on the '.input' (calls a function); Returns the result;
			2.2. args: object - the way that the 'type' is resolved (equivalent of constructor-arguments/.init-arguments); 
			2.3. buffer: boolean | () => boolean - this one corresponds to whether the current value should be bufferized; 
				When a function is passed, it is called on the 'BufferizedStream' in question to get the value; 
		
		The result of matching 2. to the '.input.curr' is (effectively) evaluated to be the Stream's '.curr'; 

		This todo is a 'maybe?' because all the said things can (pretty much) already be done by the user using StreamTokenizer and the rest
			- this really only just provides a minor abstraction over the whole thing (frees the user from needing to use the exports of the library); 
		This kind of thing is (somewhat) against the library's "all-minimialism" approach; 

		On the other hand, it would permit one to use a very domain-specific language to describe operations on streams conventionally, 
			thus - simplifying semantics + unifying a very large number of different abstractions that are creatable via the library using just this one; 	
		Conclusion: think about it, more of a YES, tha a NO currently;
	
	2.8. [reminder] Optimize minitua (that is, just go through the code, removing unnecessary repetitions of things, useless work and so forth...); 
		When the same (even tiny) operations get repeated a large number of times, the speed decay accumulates; 
		Try to make the library code that avoids this stuff fundamentally; 

		2.11.1. DO ACTIVE MICRO-BENCHMARKING!
			Having general/particular implementations in mind, MICRO-BENCHMARK, 
				and on the basis of those, optimize particular functions/approaches; 
		2.11.2. Do rabid caching: 
			Example - for 'IndexMap', always cache the '.keys' and '.values' (sometimes, they are called DYNAMICALLY!)
	
	2.9. utility - 'nestedInfo'; 

		1. Finds a depth of a given Stream, based off 'inflate' and 'deflate'; 
			NOTE: the MAX depth achievable; 
		2. Finds its length - how many positions (in 'number') must one walk before the current nestedness ends, from the initiated position; 

		Returns it as a pair; 

	2.10. BufferizedStream - a wrapper around a given Stream, to enable the storage of a buffer
		for its operations; 

		Once '.next' is called, it stores the '.curr' inside the inner 'buffer', ONLY IF the user-given '.predicate'
			HOLDS!	 

	2.11. Idea for a method: ProlongedStream.prolong; 	

		This (utilmately) modifies the 'this.streams' [modifying the '.isEnd' accordingly]; 
		Also, there'd be '.shorten(n: number)' - it'd delete a certain number of elements from '.stream'-s, 
			optionally changing '.curr', and set '.isEnd = true' (if the .curr is in one of the deleted 'Stream'-s); 
	
	2.12. [Maaayybe?] Create a 'samples' directory; 
		This is for 'common-case' parsing - generally, walk through the various syntaxes that were 
			PRESENT within the previous parsers, use them?
		
		Or, better, make this into a separate mini-project called 'parsing-samples'; 
		[EXAMPLE: handling the 'escaped' strings/sequences all the time];

		Think about it [this particular refactoring is VERY fine and case-specific, though, it can come in useful...]; 

		No, if one is to do it, the thing should: 

			1. Work with general parsing patterns, that would be CONFIGURABLE for every case; 
			2. Provide low-level Token-s with specialized given names; 

	2.13. IDEA: generalize the type of 'T<K> = CollectionLike<K | T<K>>' properly; 
		It appears at least in 2 places inside the library: 

			1. Tree (InTreeType)
			2. NestedStream;
		
		This might (also) resolve some 'interface'-related issues for the 'NestedStream'; 
	
	2.14. v8-specific optimizations: 
		Having done some benchmarking on some typical Array methods, one arrived at a...
	
		CONCLUSION: 
			1. Handwritten methods are *often* __MUCH__ faster than the builtins ON LARGE PIECES OF DATA; 
				On the more common-place cases, however, they are inferior in time performance; 
			2. THEREFORE, one should do: 
				Re-organize the library (how?) to permit usage of the current methods on LARGE pieces of data; 
				Prior - do more benchmarking; 

				About re-organization: 
					1. Think - whether to introduce explicit low-level 'size-controls' [possibly costly, when run a lot]; 
						Although, it's likely to be optimized/negligible; 
					2. Or, whether to split the thing onto 2 chunks [where the 'size' is precomputed...]; 
						This is less flexible (and thus, error-prone), plus will be more difficult to re-structure; 
					
				Current vote is for 1.; 

				ALSO, about bounds - those should be CAREFULLY BENCHMARKED; 
		
		MORE GENERALLY - try to adapt the library for work on LARGE datasets; 
		It'll make it useful for working with big projects/pieces-of-text; 

		THINGS OF ISSUE TO THINK ABOUT IN PARTICULAR: 	

			1. What "LARGE" sizes are practical? [meaning - sources of what sizes can occur in the wild?]
				Take them as large as one can. At least 10000000 symbols (~30MB, ~100000 lines of 90-100 chars), maybe try more; 
			2. How much of a performance difference can one get by doing these optimizations?
				In particular - try to sketch out a parser for something (the smtf format of one's own? it's simple to parse), perform 
					profiling on a REALLY big auto-generated file, then: 
						
						1. take overall time measurements (performance.now()); 
						2. profile, break the execution down on several tiny pieces; Then - benchmark and optimize them accordingly; 
							For this, use results from samples from one's benchmarks library;  
			3. Optimize for smaller sizes ONLY when it doesn't impair the memory severely; 
				When it impairs it at all, choose whether or not to do it based off the chosen memory/speed difference measurements...; 
				
				3.1. FOR THIS [maybe] - have specialized "Heuristics" constant-space in 'constants.ts'; 
					3.1.1. IF doing them - create the heuristics for DIFFERENT ENGINES; 
					3.1.2. IF oding them - make the 'Heuristics' only the "default value" for transition of sizes; 
						THE USER must be able to set their own boundries; 
					3.1.3. IF DOING THEM - check how to "bounrdies" for the increase in efficiency when using 
						the custom implementation "shifted" as the versions of Node progressed - TRY IT with different node, v8 
							versions; 
					3.1.4. IF DOING THEM - other engines to test: 
						3.1.4.0. V8 (Chrome)
						3.1.4.1. SpiderMonkey (Firefox)

					[maybe?] Try building the engines from nil, running the benchmarks for heuristics standalone...; 
	
	2.15. [maaayyybe??] Bring the old planned stuff for v0.3 back; 
		That is the 'Parsers/utils.ts' - 'transform', 'delimited', 'consume' [equivalent of 'LimitedStream', uses 'Collection'-s]; 
	
		Do only if there are code aspects suffering from this significantly: 

			1. readability/accesibility; 
			2. performance; 
		
		Primary cause for this todo is that, despite being "slow" (memory allocation), 
			they did still have the benefit of semantic simplicity; 

		The only cause for performance issues with OOP Stream-s API is overhead from re-structuring the original loop so much
			[need for calling the '.isCurrEnd()' two times more]; 
		Again, do this ONLY if the performance difference is SIGNIFICANT!
	
	2.16. util - enumerateTree; 
		Given an array of "types" of nodes, it recursively converts their properties (given in the 'shapes' array), 
			to single-array-siblings form (the current ChildrenTree); 	
		This would permit a more "variable" set of trees (those that have properties with fixed names), 
			to be enumerated accordingly...; 

		Example: 

			{
				a: any
				b: any
				c: any
			}

		Becomes: 

			{
				value: [
					// ... a, b, c
					// ... or b, a, c
					// ... or any other order
				]
			}
		
	2.17. About generics - relax them; 
		Use ONLY WHEN NEEDED; 
		For example: a method makes use of X generics, out of which Y < X is needed [id est, used somewhere beyond the 'this: ...' in the parameters list]; 
			REMOVE the unneeded ones; 
			RE-ORDER THEM, if need be; 
			Try to make them more minimal...; 

	2.18. [maybe?] Create a module for randomized Stream-generation (`random`);
		(given, say, an InputStream with an Array of expected values inside of it [that can be picked for Nested], 
			it will choose a pair of them for different [distinct] boundries); 
		Similarly - one could give an Array of limiting characters for a LimitedStream to be defined by...; 
	
	2.19. New type of Tree - a BacktrackableTree, takes up more memory, but is (ultimately) faster to use in TreeWalker [when going backwards inside the Tree]; 
		In the 'TreeWalker.renewLevel', it does the 'this.stream.pos.slice', which is O(n) complexity relative to 'n' - number of levels in a tree; 	
		A 'BacktrackableTree' [interface] is one that has a method: 

			1. backtrack(pos: number)

		Which returns the 'pos'-parent [that being, goes "up" the level in the tree]; 
		Make the old 'Tree' into the 'BacktrackableTree', create a 'ParentTree' [implementation], where 'backtrack' is defined as: 

			backtrack = (pos: number) => {
				let result = this
				while (pos--) result = result.parent
				return retult
			}
		
		Which is O(1) to get the immidiate parent, and O(p) relative to the number of levels to go back (`p`); 
		For this to work (the '.parent' property), the Tree must be CONSTRUCTED in a certain fashion; 
		Add more items to the interface: 

			1. pushChildren(children: any[])
			2. clearChildren() 
		
		The 'ParentTree' implementation would (therefore) assign '.parent = this' on all the pushed children in '.pushChildren', and do 'this[propName] = []' in 'clearChlidren'; 
		[IMPORTANT!: Make a 'ParentTree' implementation work with the 'ChildrenTree'-s, that is it'd have its own 'propName']; 

		THEN, SPLIT the 'renewLevel' onto 2 functions [one of which uses the 'this.stream.curr' (in '.goPrevLast'), and the other - that uses '.backtrack(1)']; 
			Better still - for optimization purposes, let it CHOOSE which implementation to use DEPENDING on the number of parent levels and pre-parent levels;

	2.20. Add a fast implementation of '.finish' to the 'TreeStream'; 
	2.21. Re-structure the tests to be made on a per-method basis? 
		Current per-class suites are rather verbose (don't do this until v0.4, already spent too much time on the tests...); 
	
	2.22. INTRODUCE the [optional] '.state' property for state-keeping; RE-INTRODUCE the '.copy()' method and 'Copiable' interface; 
		Comes inside the 'StreamClass' (optionally) via the 'hasState' property;
		Will additionally be supported by: 

			1. 'StreamClass(...).prototype.copy()' method; 
				Solution for '.copy': 
					1.1. Keep the instance variables inside the single '.vars' property; 
					1.2. Keep the user-defined variables under '.state'; 
				
				Definition [loose, has to have overloads, depending on presence/absence of '.pos']: 

					function uniCopy (stream: ...) {	
						const copy = new (Object.getPrototypeOf(stream).constructor)()
						copy.init(stream.vars, stream.state)
						return copy
					}

			2. 'StreamClass(...).prototype.init(vars, state)': 
				Will (by default) assign the '.vars' to be 'vars' and '.state' to be 'state'; 

		Then, the various 'Indexable'-s can use the '.state' inside the '.index' call; 

		ALSO, ADD: 
				1. New 'HashMap' (and IndexMap) aliases to work with ParsingState; 
				2. New 'HashMap' (and IndexMap) aliases to work with '.state' property; 

	2.23. For future: improve the 'constructor' tests. 
		Refactor them, permit the ability to properly test the '.copy()'-ed instances using the constructor
			tests (namely - the "prototypeProps" and "ownProps" thing - missing currently); 
		
	2.24. [maybe? clarity] Rename the 'StreamTokenizer' into 'TokenizerStream' [or, outright, TransformedStream], move it to 'Stream'; 
		Reasoning:	
			1. Currently, it is THE ONLY 'new (...x: any[]) => StreamClassInstance' implementation located outside the 'src/Stream/' directory; 
			2. The only reason that it is inside 'Parser/' is that it can be (and, historically, was being) used for creating Tokenizers; 
			3. Tokenizers are only one of its applications - the thing can be used for pure stream-transformation; 
			4. It used to have (a kind of) a parallel with the 'StreamValidator' and 'StreamLocator' (both require iterating the Stream); But now, save for the application, it is almost inexistent; 
			5. Its tests are completely different from anything in the 'Parser/' directory;
		
	2.25. 'regex' - allow for methods for creation of regular expressions that use the extended regex-syntax (enabled by the 'v' flag); 
		Namely: 
			1. intersection: 	/[[...]&&[...]]/v
			2. subtraction: 	/[[...]--[...]]/v
			3. string literals: /[q{...}]/v
		
	2.26. [maybe?] Re-locate the 'SubHaving' interface into the 'src/interfaces.ts'; 
		Make it global [besides usages - no reason for 'IndexMap' to keep it...]; 
		Also - how about uniting the '.sub' with '.input' [have a single name for delegate object-properties]; 
	
	2.27. [prototypes, generality] make all the prototype 'value's (in particular - methods) OVERRIDABLE! 
			via doing: 

				Object.defineProperty(_class.prototype, {	
					methodName: { value: ..., writable: true }
				})
			
			Currently, they are ALL non-overridable; 
			This will enable the user to more liberally use the library classes [they are, presently, highly single-purposed (intentionally)]; 
	
	2.28. Create a new 'IndexMap' implementation: 'SetMap'; 
		This would be a: 
			1. IndexMap interface implementation; 
			2. generalized wrapper around a Set (in a way that HashMap is a wrapper around Object and Map); 
				2.1. Would have its own 'extension'; 
				2.2. For 'values' would contain true/false; 

		This is useful for working with cases that require the 'or'-predicate; 
		Example: 'nested' utility; 
		How it is done [pseudo-code]: 

			const SpecialCaseSetMap = SetMap(...)
			const processNested = (x) => nested(TableMap(new SpecialCaseSetMap([...])), TableMap(new SpecialCaseSetMap([...])))(new WhateverStream(x))
		
	2.29. Add the 'TravelableStream' interface, with a '.travel()' method. 
		It would exist to remedy the: 

			while (pos--) stream.next()

		lines of the 'uniNavigate'; 
		For some streams (example: InputStream), this can be easily replaced with: 
			stream.pos += pos
		Which is a single instruction; 

		CURRENTLY, this is precisely what the 'InputStream' is doing, HOWEVER, because of this, 
			the '.navigate' has become rather useless (in the sense that it is not centralized); 
		Then: 

			1. InputStream.navigate() [and similar 'deviating' cases] would be re-implemented to use ABSOLUTE position instead; 
			2. 'uniNavigate' will become 'uniTravel' (and 'navigate' default-method of StreamClass will be renamed into 'travel'); 
				2.1. '.navigate' will be re-implemented on 'StreamClass' by default (via '.rewind() + .travel()'), for 'ReversedStreamClass'-cases; 
			3. 'skip' will call the '.travel' on the given stream (instead of using the 'uniTravel', as it does now); 
		CONCLUSION: 'skip' and 'has' utilities optimized, interfaces unified; 

	2.30. Add 'Sizeable' Stream-s; 
			An interface, serving as a generalization of 'InputStream' and 'BufferizedStream'; 
			Basically, one'd have '.size' property on Stream-s,
				which would permit the number of elements once the Stream has been walked
					(WITHOUT needing to Bufferize -- When kinds of 'Stream'-s ARE DIFFERENT, BUT, the number of elements IS THE SAME); 
			After that has been done - one can either '.rewind()' it, or create a new one, with the same input, but this time - with 'size'; 
			This will permit one to SAVE TIME on the 'isEnd' calling [and the other methods underneath in the StreamClass]; 

			For InputStream, the size is known at the beginning, so it (fundamentally) equates iterating the Stream to 
				iterating the underlying array/stream; 

			This is especially good for practical applications - one can get something like a 
				'StreamValidator' for a complex non-bufferized Stream, then - store the '.size' 
					after validating successfully, THEN - use the size to iterate more rapidly; 
		
		30.1. IMPORTANT: make the '.size', '.buffer', '.state' OPTIONAL PROPERTIES of the 'StreamClass' (like '.pos'); 
			The '.buffer' property would REPLACE the 'BufferizedStream' - likewise, the 'SizedStream'-s would be replaced by this; 
			Let it be: 
				1. If '.pos' is present - define '.size' as a getter: '() -> (this.pos + 1)'; 
				2. If '.buffer' is present - define '.size' as a getter: '() -> (this.buffer.size)'
				3. If neither are present, do '.size = 0' iff '.defaultIsEnd() === false', otherwise - '.size >= 1', with increasing value as the Stream size progresses; 
			
			30.1.1. HOWEVER, for performance reasons, one'll now have the need of several-variable 'cached' functions:
				0. [add to 'one.js'] the 'multiCached(props: string[])(object: object)' function: 
					0.1. Based off an object: 
						Takes in an object, with predefined list of properties; 
						This'll be using NESTED 'Map'-s; 
							Meaning, that 'i'-th property in the 'keyList', would have the 'i'-th-depth Map; 
				1. 'multiCache' everything:
					1.1. StreamClass [in terms of given signature objects]; 
					1.2. Functions that call the StreamClass for BaseClasses [the 'InputStreamBaseClass', ... others]
					1.3. The GeneratedStream-functions for creation of classes (based off THE SAME sub-signature...); 
					1.4. the 'baseStreamInitialize' function (that chooses the appropriate definition for the '.init' method...); 

					This may be important, because - when writing libraries, the user may want to override some 
						of the functionality (for instance - add/remove '.bufferized', '.pos'), and end up creating A NEW 
							class; 
							This will cause a slowdown [in a long chain, or when done a sufficient number of times - a noticeable one...]; 
		
	2.31. refactor the 'imports' tests; 
		1. more use of 'specificChildImports'
		2. the 'prefixedNames' + 'namesCapitalized' - appears lots more than once, take out; 
		3. Make better use of the 'namesCapitalized'; 
	2.32. [general; TypeScript - readability] Replace the 'o.x as Type' (where: 'o.x === undefined' is possible) with 'o.x!'; 
		Shorter, more concise - better communicates intent; 

	2.33. [important] Replace the 'Parser/' functionality completely with special cases of 'StreamTokenizer': 
		Because: 
			34.1. '.result' can be replaced with '.curr'; 
			34.2. '.finished' can be replaced with '.isEnd'; 
			[as '.next']
			34.3. '.parser' can be defined in terms of '.input/.sub'; 
			34.4. '.change' can be defined in terms of '.input/.sub'; 

		The only interesting thing about 'GeneralParser' (that can't be directly replaced), is its level of dynamicity: 
			the '.next', '.isEnd' of the 'StreamTokenizer' can't be quite
				so easily (explicitly) modified from within '.baseNextIter', '.isCurrEnd', so forth, 
					precisely BECAUSE of the underlying behaviour; 
				
		SOLUTION: 
			1. get rid of the 'GeneralParser'; 
			2. replace its implementations with cases of 'StreamTokenizer'; 
			3. allow '.writab'-ility for the StreamClass properties; 
				3.1. add an optional '.configure' method, for changing them (interface: ConfigurableStream); 
					Works in a similar fashion to '.init'; 
			4. RELOCATE those of 'Parser/'-classes that are (currently) implemented via GeneralParser-functions and ARE Stream-s, 
				into 'Stream/'. Let ONLY the Parser-s that aren't that remain: 

					[currently]
					1. TableMap
					2. LayeredParser
					[ALSO - move these three under TokenizablePattern/, ValidatablePattern/ and EliminablePattern/!]
					3. PatternTokenizer 
					4. PatternValidator
					5. PatternEliminator	
			5. MERGE the 'TableMap' and 'LayeredParser' into the 'src/classes.ts'
			6. DELETE the 'Parser/' directory altogether; 
				The final project top-level tree would look like: 

					1. IndexMap
					2. Pattern [this includes the new stuff for working with Indexed-based Pattern-s - Sequence-s]; 
						AND this includes 'GeneralBuffer' (as they are also 'Pattern'-s); 
					3. Position
					4. regex
					5. Stream
					6. Tree
				
		34.5. The ability to work with several '.stream'-s can be replaced with a utility [pseudocode]: 

			[NOTE 1: ADD this to the library]; 
			function streamSwap(stream: (whatever) BasicStream, i: number) {
				const swapped = stream.state.streams[i]
				stream.state.streams[i] = stream.input
				stream.init({
					input: swapped
				})
			}
	
	2.34. create implementation for : 'GeneralBuffer' - new array-generalizing interface; 
		The thing used by the 'BufferizedStream', 'bufferize'; 
		It is also a 'Collection' (in that, there is a '.value' and '.push' properties), 
			so it is usable with 'array' utility and others; 
	
	2.35. Add the 'export default' from 'classes.ts' and 'interfaces.ts' files for their "main" stuff; 
		Example [1]: a 'classes.ts/interfaces.ts' with THE ONLY export will have it as default, besides referencing by name; 
		Example [2]: when a 'classes.ts/interafaces.ts' has an export with THE SAME name as the module, the export is default (besides being present as of itself); 

	2.36. Make more active use of 'abstract' classes; 
		Examples: BasicSubHaving, BasicPattern - they STILL can be instantiated (unlike FlushablePattern), 
			but ARE 'abstract', because provide no functionality as of themselves (Pointer is the exception, it's a semantic-marker); 

			ALSO - turn the 'AssignmentClass'-es into abstract classes
				[ones that aren't redundant - remember the 'replace .input with .sub' 
					and '.state is an optional StreamClass property' todos...]; 
				
	2.37. [large input optimizations] Investigate whether retaining the indexes information from 'matchAll' in 'tokenizeString' may not be faster on large inputs; 
		This is largely a question of how fast '.split' is compared to building the array manually using the indicies information from 'matchAll' (that is, a large sequence of '.slice()'-es); 
		Potentially, this could be a large time saving (if '.slice()'-es are relatively short), but otherwise - could be a waste of memory (additional strings allocated without need); 
			Then, of course comes the question of whether the strings are allocated at all; 
			BENCHMARK IT; 

	2.38 [implementation - cleanup] Make the '.response', '.lastLevelWithSibilngs' stuff in 'TreeStream' a part of COMPULSORY '.state' field [and '.walker' as a part of '.vars']; 
		2.38.1. REPEAT the trick with the other library Streams' properties (ex: lookAhead, hasLookahead, and others...); 
		2.38.2. ALSO - make the '.pos: MultiIndex' on 'TreeStream' a part of '.state' ALSO! 

		Basically, EVERY stream must be describable ONLY in terms of: 

			1. .vars (compulsory, for the '.init(vars)' or '.init(vars, state)')
			2. .state [optional]
			3. .pos [optional]
			4. .buffer [optional] 
			5. .size [optional]
			6. methods' list; 

		This will ensure a greater level of predictability, that DOES NOT hinder the flexibility (because '.state' exists), nor the code structure
			(because they are all 'StreamClass'-es with differently implemented methods). 
		
	2.39. IDEA: remove the purely semantic '.input' and '.sub' in favour of '.value' - make EVERYTHING with delegates inside the library into a 'Pattern'; 
		Then, do: 
			1. LIBERATE the sub-directories of 'Pattern/': 
				1. Token
				2. EnumSpace
				3. Collection
				4. GeneralBuffer 
				5. Sequence
				6. TokenizablePattern
				7. ValidatablePattern
				8. EliminablePattern
			2. Make the 'Token.value' into 'export const value = (x: Pattern) => x.value' - an export of 'Pattern/utils.ts'; 
			3. Make the 'Token.type' into 'export const type = (x: TokenInstance) => x.type' - an export of 'Token/utils.ts'; 
			4. Make the 'Token.is' into 'export const isToken = ...' - an export of 'Token/utils.ts'

	2.40. Introduce a new Position class - MultilinePosition
		Basically, a pair of numbers: 
			(linesNum, symsNum)	
		Create a generalization of it (where 'newline' is a context-defined term, like a particular "separation token", or something...): 
		Which correspond to prior number of newlines and prior number of symbols; 
		Used frequently throughout for things like debug information; 
		The '.convert()' would play very nicely with it...; 

	2.41. After re-structuring, walk rigorously through the 'src/utils.ts' - delete things that are no longer necessary (like 'firstStream', for instance...); 
	2.42. [clarity, refactoring] Create a type 'TypePredicate<T> = (x: any) => x is T';  Use all over the place...; 
	2.43. Refactor the 'isType: (x: any) => x is Type' in: 
		1. TokenizablePattern; 
		[and other places where it appears...]
	2.44. About validation and parsing: 
		This is a somewhat worrysome part of the library, currently, as:

			1. Validation happens separate from parsing, despite using the same abstraction (ex: ValidatablePattern and TokenizablePattern); 
			2. '1.' causes the entire process of 'validate + parse' to take 2x the time (need another loop); 

		For this reason, a TODO: 

			Create a new abstraction that would encompass BOTH parsing and validation; 
			Granted, it would take more time than a simple parsing procedure, 
				but (and this is CRUCIAL) it must take LESS time than combined parsing + validation (in exchange for increased memory); 
			
		This is already doable (manually) using 'StreamTokenizer', but one would love to make it a part of the library (general abstractions + interface + algorithm); 
		'PatternTokenizer' and 'PatternValidator' are a no-go here. It should be a case of 'StreamTokenizer'; 

		Ideas: 
			0. It is a StreamClass (a configurable class of StreamTokenizer-s, more precisely); 
			1. Let stream elements be '<OutType>[valid, token]: [boolean, OutType]'; 
			2. Contains a field of '.validator', return values are, then ' const token = this.handler(this.input); return [this.validator(token), token]'

[DEFINITELY, for v0.4!]
3. Dependency management: 
	3.1. [maybe???] Create a 'refactor' library, specifically designed to refactor common design patterns; 
		In particular, it would have: 	

			1. 'delegate' functions [from here]; 
			2. the 'classWrapper' function; 
			3. the 'AssignmentClass' function; 
			4. A 'PropertyDescriptor' class; 
				4.1. This one is for creation of 'Object.defineProperty' signature-objects (the "property descriptors"); 
			5. The 'Constructor' type; 
			6. SelfAssignmentClass; 

	3.2. All the routine writing of tests made one think whether it may not be more desireable to 
		automate even the process of their WRITING; 

		Create a library like this containing: 

			1. a path-resolver (for relative/absolute imports); 
			2. a JS code-generator (based off the 'parsers.js' + the JS parser of one's own...); 
			3. various functions for configurable code-template creation; 
			4. functions for recursive directory-traveral, automated file-creation, types for specifying a directory structure; 
		
		The combination of the three will permit A LOT of different code to just be "magicked away"; 
		USE THOSE to "re-write" ["pack", the .ts files will be generated (.gitignore-d) - then run], 
			the tests/their designs for 'parsers.js' v0.4 [the repetative parts]; 