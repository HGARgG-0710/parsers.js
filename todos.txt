[GENERAL]
1. Check if this works with Deno; 	

[v0.3]

A. testing: 
	A.1. Create list of modules to be tested; 

		Testing choice [per-method/per-class] and implementation are defined by: 

			1. Stream Classes are tested in the following fashion [they branch]:
				1.1. They branch: 
					1.1.0. Empty Stream instance
					1.1.1. Single-element Stream instance
					1.1.2. Two-element Stream instance
					1.1.3. Long Stream instance (>= 20 elements)
				1.2. There are benchmarks; 	
					2.2.1. In particular, IDENTIFY THE BOTTELNECKS IN DEFINITIONS [if there are any]; 
			2. Pattern Classes are tested in the following fashion [they branch]: 
				2.1. Empty; 
				2.2. One-element; 
				2.3. Two-element; 
				2.4. Long; 
			3. Parsers are tested as follows (with according Stream-s): 
				3.1. There is a set of Stream-s, for which they're tested; 
				3.2. With appropriate Collection-s; 
				3.3. They branch [by 4.1. and 4.2.]: 
					3.3.1. The Empty Stream; 
					3.3.2. The One-element Stream; 
					3.3.3. The Two-element Stream; 
					3.3.4. The many-element Stream; 
			4. Generally, provided an instance of a given class, its methods are tested: 
				4.1. All methods are tested; 
				4.2. If a method is mutating, there is a new instance created (a temp copy), then the operation performed on it; 
				4.3. If a method is non-mutating, it is performed without copying; 
			5. There are benchmarks: 
				5.1. Stack benchmarks: 
					5.1.1 NestedStream - find out how deep a given NestedStream must be in order for the "default" Stack to break; 
						Use as metric later
				5.2. Parse really long strings: 
					5.2.1. Try to parse some incredibly long strings;
						See the speed limits for different string sizes
				5.3. "Cached" TokenInstance vs "Non-Cached" TokenInstance: 
					5.3.1. Auto-generate a GIGANTIC tree in a simple syntax made entirely of TokenInstance-s, then do a version for BOTH cached and non-cached 
					5.3.2. Iterate through them [TreeStream]; Measure time; 
						NOTE: it is expected that the "cached" 'TokenInstance' will be A LOT faster to build/do; 
			6. Rewrite/restructure the 'regex' tests; 
			
		Left to make tests for: 

			1. IndexMap/ [specifics]
			2. Pattern/  [specifics - underlying]
				2.1. Token/ [specifics]
				2.2. EnumSpace/ [specifics]
				2.3. TokenizablePattern/ [specifics]
				2.4. ValidatablePattern/ [specifics]
				2.5. EliminablePattern/ [specifics]
				2.6. Collection/ [specifics]
			3. Tree/ [specifics]
			4. Stream/

				To test:

					1. Position [re-design]; 
					2. InputStream; 
					3. PositionalStream; 
					4. TreeStream; 
						4.1. TreeWalker; 
						4.2. MultiIndex; 
							4.2.1. MultiIndexModifier; 
							4.2.2. Slicer; 
					5. TransformedStream; 
					6. PredicateStream; 
					7. ProlongedStream; 
					8. LimitedStream; 
					9. ReversedStream; 

				4.0. One tests on a class-basis; 
					4.0.1. Classes are tested per-method; 
					4.0.2. A single class is a collection of methods, ergo, there MUST BE A WELL-DEFINED SET OF 
						general wrapper-tests: 
						4.0.2.1. These would be based OFF passed "checks"; These would (fundamentally) 
							compare the result of the given Stream having a given method called with given arguments, 
								with the GIVEN EXPECTED RESULTS. 
							
							The 'compare' would be class-based; 

							THERE WOULD BE SPECIAL-CASE PER-CLASS DEFINITIONS FOR IT! 
							They would be used with the actual Interfaces; 

				4.1. The tests have TO BE ORDERED [because various interfaces are intertwined]; 

					In particular: 

						1. All PositionalStream-s must come AFTER Position; 
						2. All the sub-interfaces of TreeStream must be checked BEFORE the 'TreeStream'; 
						3. "WrapperStream"-s (such as 'NestedStream', 'TransformedStream', 'LimitedStream' and others)
							must ALL be tested on different combinations/variations of Stream-classes; 

							Thus, for instance, one has to test ALL of: 

								LimitedStream-NestedStream-InputStream;
								TransformedStream-LimitedStream-TreeStream;
								NestedStream-TransformedStream-InputStream;

							And more...

				4.2. There are SEVERAL TYPES of tests for the same method:
					4.2.1. Basic functionality (comparisons, and so forth...); 
						This (typically), just calls the method once, + does some comparison work on the result; 
						These are separated by predicates used for comparison.

					4.2.2. Combined functionality; 
						This is relative to how 'Stream'-s are supposed to "behave" in certain conditions; 
							Generally: 

								1. For '.isEnd', the 'generalIsEndTest': 
									1.1. start from current, point; 
									1.2. COMPARE the '.curr' on each iteration, then '.next()'
										1.2.1. do appropriate 'assert' + USE A GENERAL 'COMPARISON' OPERATION!
										1.2.2. Update the 'i' (index); 
										1.2.3. Save the 'stream.curr' into a separate variable BEFORE doing the 'stream.next()'; 
									1.3. Compare the 'i' counter of walked-through items [.length, or whatever]; 
									1.4. Compare the LAST ITEM WITH ITSELF (to check for correctness of the definition of the 'StreamClass'-specific items, the '.isEnd'); 
								2. SAME for '.isStart' and '.prev'; 
								3. For '.rewind()', '.finish()' - 'generalRewind', 'generalFinish': 
									3.0. Walk up to the end/start ('while (!stream.isEnd) stream.next()')
										3.0.1. Save the items into A BUFFER ('bufferCopy')
									3.1. Do '.rewind()'/'.finish()'; 
									3.2. Walk again; 
										3.2.1. Add into 'bufferRepeat'
									3.3. Compare the 'bufferCopy' and 'bufferRepeat' via 'arraysSame'; 

					4.2.3. Utils; 
						These are tested regularly via the 'utilTest'; 
						4.2.3.1. ORGANIZE THESE! 
							Let each 'util' go to its appropriate interface directory...;

					4.2.4. Constructors: 
						Make them "typical" (in the sense they uses the 'is'-functions); 

						ADD THE 'is'-utils: 

							1. For the 'StreamClass' ('StreamClassInstance')
							2. For each of the 'implementations' of the 'StreamClass'; 
								These would be 'and'-s with SPECIFIC properties; 
								LIST: 

									1. StreamTokenizer (Parser/) ;

									['is' done - constructor left];
									2. InputStream 					
									3. LimitedStream 		
									4. NestedStream 		
									5. PositionalStream 	
									6. PredicateStream 		
									7. ProlongedStream 		 
									8. ReversedStream		 
									9. TransformedStream	
									10. TreeStream; 		 

									NOTE: when writing the 'is'-functions for the interfaces of Streams that USE THE 'StreamClass', 
										DO NOT use the 'isStreamClass' utility as a part of their "identification util"'s definition; 

										CONCLUSION: the tests WILL have to have THEIR OWN 'is'-functions, based on the ones that 
											are intended for the primary "facade" interfaces; 

					4.2.5. Methods:
						4.3.5.1. StreamClassInstance-specific (general): 
							[predefined]
							4.3.5.1.1. generalIsEndTest; 
							4.3.5.1.2. generalIsStartTest; 
							4.3.5.1.3. generalRewindTest; 
							4.3.5.1.4. generaFinishTest; 

							[per-case-defined]
							4.3.5.1.5. navigateTest (based off 'streamNavigateTest' + comparisons); 
							4.3.5.1.6. copyTest (based off 'streamCopyTest'; for InputStream and TreeStream); 

							4.3.5.1.7. 'isStream(.input)', the "missing" '.from' and other such particular values would be tested
								AS PART OF THE APPROPRIATE (__specific__) METHODS' definitions/comparisons; 

					4.2.6. Refactoring: 
						4.3.6.1. In tests, rely HEAVILY on 'navigate' (once tested); 
						4.3.6.2. Don't test optimizations (like 'uniNavigate') INSIDE the test-code; 

					NEXT ITEM-s: 

						1. The 'is'-functions, CONSTRUCTORS for every single type of Stream: 
						2. per-case defined (.copy, .navigate); 
							2.1. .copy is for InputStream and TreeStream only (can be generalized?); 
							2.2. .navigate is for: 
								2.2.1. InputStream; 
								2.2.2. TreeStream; 
								2.2.3. LimitedStream; 
						3. Boilerplate-implementations (generalIsEndTest, generalIsStartTest, generalRewindTest, generalFinishTest); 
							3.1. Re-design the 'Position' [re-check what could have been missing - been adding some new definitions...]; 
						4. [Finally] combining the testing code together FOR EACH one of the classes, via signatures; 

						5. ALSO : add a 'streamIterator' TEST [look for other such "minor" utils, if any remains...]; 

			5. Parser/
			6. Stateful/
			7. benchmarks/
			8. imports/ 
				NOTE: there has to be a WHOLE SEPARATE testing procedure for the 'imports'; 
		
	A.2. Write out dependencies to be tested for each one of them; 

		1. Classes (classes.ts) - ALWAYS
		2. Utilities (utils.ts) - OCCASIONALLY
		
	A.3. Write the tests (module-by-module); 

B. documentation (change/fix it...);
	B.1. Create new pages:
		B.1.1. Home: 
			B1.1.1. Motivation (write that the library was created/conceived primarily as a mean of refactoring of frequent parsing tasks/data-structures); 
			B1.1.2. "One good approach" principle (write that the library was made to have a single "intended" best approach for all the things that are possible to do using it); 
			B1.1.3. Examples [projects done using it]; 
			B1.1.4. Structure - explain how the library structure works (interfaces., methods., classes., utils.); 
			B1.1.5. Known issues (this lists known problems that the library currently has);
C. rewrite the CHANGELOG [too much has been altered since the "initial" v0.3]; 
	C.1. Walk through the last commit's state of v0.2.1, noting changes and writing them down [destructive]; 
	C.2. Walk through the v0.3 changes, and write them down [constructive];

Note: the new object-oriented approach will enable better speed and lesser structure-copying (better memory use); 
	PLUS: one can have the 'Streams' in question working AS TREES without the necessity for creation of the actual TreeStream-s
		(as the recursive Stream-s more or less serve the same function - only problem is populating them with the appropriate kinds of objects); 

[v0.4]

1. STACK! 
	The library is not too stack heavy, BUT it does have an (ultimate) stack-limitation
		when working with NestableStream-s and the 'nested' util (generally, anything that is NON-FLAT);
	This is very much impossible to solve on its own; 

	SOLUTION: use the CPS style. 
		Re-implement library (whilst keeping the non-CPS version) in CPS, 
		with an infinite 'stack'; 

	! THIS WILL REQUIRE FOR ONE TO FIRST IMPLEMENT THE INFINITE CPS-BASED STACK LIBRARY!
	That one should: 
		0. Be written in TypeScript; 
		Include Types: 
		1. InfiniteStack; 
		2. CPSArray (this is, ultimately, a CPS version of JS array);

	As, CPS is (quite) time-costly to actually run (due to amount of stack usage [primary] + the stack unwinding + additional checks...),
		this sort of thing would be useful for applications where speed of little importance
			(that is: NOT things like apps/websites/interpreters, and so forth; Stuff like AOT-compilers, perhaps);
	
2. Add various optimization data structures + utils + parser properties: 

	2.1. bufferize(process, size)(tree) - util

		Given a 'Tree', returns an array of values: 

			{
				childNumber: number,
				value: any
			}

		This (ultimately) does THE SAME thing
			as the 'TreeStream', BUT without losing the children's Tree-information. 
		Unlike the 'TreeStream', this is intended as
			a more persistent data structure with faster, 
			and NON-LINEAR (important) access than TreeStream.
		
		Would return a 'NodeBuffer' (which is just a kind of an array); 
		Indexing an array could be (potentially) noticeably faster than walking through a Tree 
			(particularly so with the 'TreeStream'); 

		The 'index' is the index FROM THE BEGINNING of the buffer to the current Node. 
		The 'childNumber' is the number of children that come AFTER the current node. 
			Children are read between positions from 'index' to 'index + childNumber', [where 'index' is the NODE'S POSITION! Keep it or keep it not...]; 
				with each 'child' being checked for its own children
					(which, if nonzero, are excluded from the current children, and are instead counted as grandchildren). 
		
		The 'value' is the actual value at the index (note: being 'process'ed - this is intended for removing/keeping children, primarily); 

		If size is used, it will ONLY use this many items from the tree (up to the maximum amount - the rest of positions will be assigned to 'undefined'); 

		BASICALLY, this only works for WHENEVER the 'size' is given; 

			Insted of doing: 

				const final = []
				while (!stream.isEnd && size--) 
					final.push(stream.next())
				return final
				
			It does: 

				const final = Array(size).fill(null)
				const staticSize = size - 1
				while (size--) 
					final[staticSize - size] = stream.next()
				return final
			
			Which is faster, as it (basically) doesn't require running the '.push' repeatedly, 
				AND it doesn't risk having 'undefined' values; 

			Except here, the 'stream' [TreeStream] is replaced by the tree:
				so it recursively traverses it, preserving the SAME buffer and index across different recursive calls;

	2.2. Add a 'state.size' property to the parsers. 
		This (ultimately) counts the number of items inside the tree.
		Can be used with 'bufferize': 

			bufferize((x) => x, pstate.size)(pstate.result)	

		IDEA: generalize this; 

			Let the 'array' permit an OPTIMIZATION of replacement with 'bufferize' [not for a 'Tree', but a 'Stream']; 
				Thus, whenever there's a DISTINCT LENGTH, it'll simply do 'Array(size)' [or, more exactly, for some general interface 'GeneralBuffer(size)'...], 
					then convert it to a 'Collection'; 

			This will permit minorly optimizing the collections that have known length;

	2.3. TreeStream: create two versions; 

		The 'TreeStream' (currently) is the 'LL' pre-order tree traversal algorithm Stream-implementation; 
		There should be ANOTHER - the 'LR' post-order tree traversal; 

		Rename the 'TreeStream' to 'TreeStreamPre', and create an implemnetation for the 'TreeStreamPost'; 
		This is (particularly) useful for some types of interpreters; 

		The Pre  visits the node, then its children one-by-one starting from the beginning. 
		The Post visits first the children, only THEN the parent.

	2.4. Implement a 'PartialValidator': 
		This, ultimatly, works in THE SAME WAY as the 'PatternValidator' and 'PatternTokenizer' (GENERALIZE THEM TO A SINGLE FUNCTION ALREADY!)
		Gets the recursive structure with all the 'split' and 'inserted' bits; 
		The 'PartialValidator' would RETURN THE INFORMATION FOR INFERENCE OF CORRECT/INCORRECT PARTS! 
		[Which it does by 'numbering' particular portions of a given Pattern]; 
		This would allow for a greater ability to analyze/point-out the errors in the original input;

		Unlike plain PatternValidator, it would return ALL the possibly available error information, not just the stuff about the index of the "failed" check, or coverage; 

	2.5. Implement a 'level-by-level TreeStream': 

		It would get an initial given 'level' (tree root), walk through all of its '.children', 
			then walk each of the '.children' themselves as levels (one after another...), THEN descending lower...; 

	2.6. Add a 'HashClass'-implementation of 'MapClass'; 

		Basically, takes the 'hash' function (rough sketch) and returns a hash-table based off it...: 

			function HashTable (hash) {
				// The HashClass
				return function (values, keys = new Set(), default = null) {
					// The '.index' function; 
					return function (x) {
						return !keys.size || keys.has(x) ? values[hash(x)] : default
					}
				}
			}

		Great for generalizing optimizations of tables/hashes/maps...

		Implement special  cases of this ALSO (PARTICULAR hash-functions...): 

			1. LengthHash: 

				This is excellent for string-identifiers; 
				Given a set of strings (THAT CAN BE UPDATED!),
					one has a hash function: 

						(string) => string.length

				The thing in question is (supposed) to map a given string to an array/collection 
					of other strings of equal length, then map in this collection (which has a different interface...);

				NOTE: ANYTHING with a '.length' property is usable with this;
				
			2. TreeHash (a kind of 'Hash', based off 'Tree'-s): 
				Basically, given an 'Indexed<Type>' (WITH A '.length'), and given an 'array-tree', one does (rough sketch, indexation): 

					function ArrayTreeHash (tree) {
						return function (x) {
							let current = tree
							for (let i = 0; i < x.length; ++i) current = current[x[i]]
							return current
						}
					}

				NOTE: the actual 'TreeHash' would be more general and use 'x' as a MULTIINDEX!
					(tree) => tree.index(x)
					BUT: for this, it'd require a more 'generalized' version (in particular, USE LOOPS INSTEAD OF '.reduce')

				This can be used with strings/

		NOTE [on optimizations]: this is especially useful when
			static-lengthed and long-term parsing tables are concerned; 
				The HashMaps are IDEAL for this because
					(on large inputs, with DISTINCT, and/or integer-keys), 
					there is A LOT of choosing between different values; 
				
			IDEA: generalize the 'TokenType'-s, by EXPOSING their 'type'-s (whenever they're 'number'-s, or 'string'-s); 
				WHENEVER they are guaranteedly disjoint, it allows for an immidiate
					one-to-one match; 

				Example of hash-table implementation based off objects [rough sketch]: 

					function ObjectHash (hash, default) {
						return {
							hash, 
							default, 
							index: function (key) {
								return this.hash[key] || default
							}
						}
					}
				
				Thus, one'd want some sort of data structure to (assuming disjoint-ness) 
					convert the given '.type'-s into HashMap-s (equivalent of 'TypeMap', but for the newly exposed '.type' not '.is'); 

					ALSO:
						one may want to implement: 

							1. hashFromKeys: 

								Basically, creates a 'hash' function from a given 'Map' (inverses it); 
									Basically does: 

										function hashFromKeys(keys) {
											const map = new Map()
											for (let i = 0 ; i < keys.length ; ++i) map.add([x, i])
											return (x) => map.get(x)
										}

								The resulting function can be used in conjunction with the 'keys' (provided they're static); 

							2. hashFromObject: 

								A way to implement the 'ObjectHash' ORGANICALLY [without the need to change the '.hash' type]: [crude sketch]

									function hashFromObject(object) {
										return (key) => object[key]
									}

									THEN, 

										ObjectHash := (object) => HashClass(hashFromObject(object))

								This is (particularly) for cases, when the given map has integers/strings for keys; 
								Useful in conjuction with the 'SimpleTokenType'; 

									ONE HAS TO BE ABLE to (somehow) get use the 'SimpleTokenType' with this; 
									Possibly?: 	

										// Parsing table; 
										const basPairs = mutate([
											[TokenTypeA, ...], 
											...
											[TokenTypeN, ...]
										], ([Type, Handler]) => [Token.type(Type), Handler])

										const baseObj = toObject(new Map(basePairs)) // known that THE KEYS are all strings/numbers + disjoint; 
										const hashMap = ObjectHash(baseObj)	

										
										TODO: implement the: 

											3. disjointKeysParseTable(tokenPairs)

												This (basically) accepts pairs of '[SimpleTokenType, Function]';
												And spits out a 'HashMap', like: 

													disjointKeysParseTable := (pairs, default) =>
														ObjectHash(
															hashFromObject(	
																dekv(fromPairsList(pairs.map(
																				([Type, Handler]) => [Token.type(Type), Handler])))), 
															default)
												
												? perhaps, make a new version for one.js that would have a new method for optimizing transformation from pairs-list to an Object? 

												That would be desired for two reasons: 

													1. Performance (when the table consists of SimpleTokenType-s and is used A LOT); 
													2. Common-ness of the pattern: 
														This thing can be VERY frequently used in actual parsers;	

				Also, there'd need to be a way to get the '.type' of the 'TokenType'-s directly; 

				IDEA: create a structure 'SimpleTokenType', 
					which would have the '.type' property on it; 

					THEN, re-purpose '.is' to allow for more complex constructors
						(in particular, MAKE THE CURRENT 'TokenType' into 'SimpleTokenType', 
							whilst creating the NEW 'TokenType',
								permitting to set the '.is' AS-DESIRED!)

	2.7. Add a module SPECIFICALLY for working with strings/identifiers/Sequence-s (currently: the 'Indexed' interface); 

		In particular, functions/abstractions: 

			1. UniqueSequenceTree;

				This is an 'IndexMap'+'Tree'-implementation working like: 

					1. '.index' of a given 'string' (or another '.length'-having structure) BASICALLY, finds the first "unique" level it's on; 

						So, for instance, having a set: 

							{ ann, ant, ben }

						Would yield an IdentifierTree: 

							0   1 2
							a,b-n-n,t

						This (basically) uses recursive objects + indexation
						(
							ant is found in 3 iterations; 
							an is found in 2 iterations; 
							'ben' is found in 1 iteration (only 'b' is stored);
						)

				Note: this implements a HashMap's 'hash'; 
				This is VERY good for frequent accesses to THE SAME value; 

			2. BinarySearchTree; 

				Same as 'UniqueSequenceTree', BUT it implements a Binary Search Tree.
				It works based on a provided comparison (a very rough indexation sketch): 

					function BinarySearchTree(tree, comparison) {
						return function (x) {
							let current = tree
							let greater
							while (current.length) current = current[+comparison(current, x)]
							return current.value
						}
					}
				
				NOTE: with (x, y) => x > y, can be used with identifiers; 

			3. shorten(names); 
				Given a sequence of "names" (strings/sequences),
					and a 'comparison' predicate for each of its elements,
						it would implement a name-shortening algorithm, 
							that would preserve the name uniqueness;
				
				The return value is an IndexMap; 

			4. renameable(names);
				Given an IndexMap of names (which could come from, for instance, 'shorten'), 
					the method: 

					1. Re-orders;
					2. Creates new name-map entries;

				In a fashion that would allow straightforward implementation of a sequential "exact replacement algorithm" based on the output: 

					1. Loop through a list of name-maps; 
						2. [In the loop] Rename current encounters of a name with its mapped value;	
					
				There are 2 operations that may be necessary: 

					1. creation of temp-names (when collisions in present names are far too high to re-order);
					2. re-order; 
				
				The algorithm for the 'renameable' function: 

					0. Keep the cached (met) names in a 'Set'; 
					1. Loop through the 'names' given: 
						1.0. If the name is in the cached Set, continue; 
						1.1. If the current 'value' of the 'name' is already present amongst 'keys' (another loop): 
							1.1.1. put the keys in question BEFORE the current value;
							1.1.2. '.swap' the two indexes;
							1.1.3. go one position back (because now one needs to check the "others" now); 
							1.1.4. cache the current amongst the checked names in a 'Set', (so that one doesn't get into an infinite loop);
		
	2.8. Expand on the 'Position' type further;

		Allow for creation of more "exotic" types;
		Example: 

			"go up to nearest point in the past defined by predicate 'P', then another 5 positions", 

		should be something like: 

			[P, 5] // with 'P.direction = false'; 
		
		Allow "nesting" positions like so, create a proper PositionEvaluator (current thing is VERY limited); 

		The 'RelativePositionEvaluatorStream' would (ultimately) create a 'Stream' of values, defined by this "sequence"
			of Position-s (call it 'PositionSequence = Position[]'), and which would be entirely relative to one another; 
			Example: given [3, 6] it would get the 3'rd, then the (3 + 6) = 9'th positions of the Stream; 

		The 'AbsolutePositionEvaluatorStream' would (similarly) interpret a sequence of 'Position's,
			but relative to the initial Stream (not one another).
			Example: given [3, 6] it would get positions 3 and 6 relative to the initial Stream; 
		
		Then, GENERALIZE THIS to a 'PortionPosition' [which is the pair '[Position, Position]' used in 'LimitedStream'],
			to also use this with 'LimitedStream';

		So, 'PositionSequence = (Position | [Position, Position])[]' will define a new Stream based of a given one
			via the 'AbsolutePositionEvaluatorStream' and 'RelativePositionEvaluatorStream'; 

	2.9. RELAX overly demanding interface and types definitions/requirements;
		Example: inputStreamCopy, inputStreamNavigate; 

			These do not NEED to have all the parts of the 'InputStream' definition (even though they are originally implemented to work with it);
			RE-DEFINE them...;

	2.10. [Idea?] Create an error-handling API for the 'v0.4';
		Create means of: 

			1. Stacking different levels of exceptions (the 'try-catch' blocks); 
			2. Assigning relevant debug information to them;

		This is (primarily) for the development process of the parsers; 
		
		Define a Catcher [rough sketch]: 

			function Catcher (info: DebugInfo, logger: Function) {
				return function (thing: Function, thisArg: any, args: any) {
					try {
						thing.call(thisArg, ...args)
					} catch (e) {
						logger(info)
						throw e // NOTE: THIS here is to represent DEPTH [as parsers can be VERY recursive indeed, it may be needed to eliminate the recursive errors on a case-by-case basis]; 
					}
				}
			}

		Although... This is rather general. Perhaps, better implement as a separate package; 
		
	2.11. limitedStreamNavigate: FLAWED!
	
		1. Should handle the case where the '.from' is A FUNCTION [but the 'position' ISN'T]; 
			THIS [fundamentally] needs the separation between the 'relative' and 'absolute' '.navigate'; 

				Once these are implemented, there'll be no problem whatsoever... 

				The (primary) issue with '.navigate'-ing using predicates is that it's NOT possible to navigate (generally) ABSOLUTELY using them; 
				Only the relative navigation is possible; 

				SEPARATING the navigation into 'absoluteNavigate' and 'relativeNavigate' (and defining the actual '.navigate' as the CONJUNCTION of the two), 
				will largely solve the issue. 

					ALSO: when '.pos' is a function, one FIRST calls '.navigate(.pos)' [relative], then does so with the '+n' (absolute); 

		2. Should be able to tell when the result of '.navigate' is 'outside the bounds' [NOT "finished" in a regular sense - that checks for equality of '.lookAhead']; 
			Namely [define a new util for that...]: 

				[forward-crossing]
				1. for a numeric 'this.input.pos' and 'this.to': newInputPos > this.to; 
				2. for a predicate 'this.to': Via looking for whether `!positionEqual(this.to, this.input)`; 
				3. for a predicate 'this.input.pos' and numeric '.from, .to': always return 'true' [never out of bounds]

				4. When looking at backward-crossing..?
					Perhaps, implement a 'lookBehind'?
			
	2.12. MemoizableStreamClass: 

		This is a generalization of StreamClass; 
		The StreamClassInstance-s are ONLY capable of memoizing the very first value obtained via 'initCurr'; 

		This would (additionally) have two more methods: 

			1. memoize() - memoizes the value (any further .next() changes the value of the Stream, but '.curr' remains THE SAME); 
			2. unmemoize() - returns back to the flow of the Stream; 

	2.13. Minor refactoring (StreamClass): 

		Create a special case for the 'StreamClass' - the 'WrapperStreamClass'; 
			Basically, this works in a fashion similar to the 'InputStream', 'TransformedStream', and 'NestedStream', and others such (generally, UnderStream-s and Inputted): 

				1. There is 'iter' function (for iteration); 
				2. There is 'curr' function (for 'initGetter' and the definition of 'baseNextIter'); 
				3. Then, the 'baseNextIter' is defined as: 

					{
						iter.call(this)
						return curr()
					}

	2.14. [mayyyybee??] Try re-doing the project's type-system? [again...]

		The direct use TypeScript's 'interface's cause huge issues surrounding the 'conditional' properties
			(in particular, the need to duplicate to make everything work). 

		See, if not (mayhaps), it is feasible to re-write everything via 'extends BasicStream'-sort of thing
			(meaning, the needed StreamType-s would be "inserted" where desired...); 

		Examples [interfaces]: 

			1. TransformedStream; 
			2. NestedStream; 

	2.15. BreakableStream - a stream-class that produces instances, which : 

		1. Are capable of being "broken" via different positions; 
			1.1. can eliminate parts of the given stream, IF it's a 'number' or a StaticPosition,
			1.2. Gets a chunk of the given 'Stream' if it's: 
				1.2.1. DualPosition [a LimitedStream: NOTE: this literally builds a sub-LimitedStream]; 
				1.2.2. PositionPredicate [a property to be followed by the next chunk]; 
		2. Have a sub-stream of '.input', on which they operate; 
		3. Take in a sequence of (Position | DualPosition)[]
		
		NOTE: this is, basically, same stuff as the 'AbsolutePositionEvaluatorStream', BUT, 
			it's "not-picky" in the sense that the FIRST of the allowed occurences will be used, 
			AND, it will be used without preference; 

			Also - this works UNTIL THE END OF THE GIVEN Stream, NOT until the end of THE EVALUATED ARRAY; 

		SO, what is given is an Array, each of possibilities in which are "checked" for getting picked for 
			the Stream, then the first one getting its "go" as the '.curr'; 

		BUT, the given 'Array' will actually be several, which are ORDERED BY ATTEMPT in the following fashion: 

			1. limits[] - the array of 'DualPosition's; These define 'LimitedStream'-s [NOTE: these can contain pairs of predicates also!]; 
				1.1. In particular, this defines the predicate-limiting on the given Stream (in the sense, that, the piece matched by the first)
			2. nest[] - the array of dual 'StreamPredicate'-s used to define the nesting; 
			3. preserve[] - a set of 'Position'-s for IMMIDIATE matches; 
				3.1. These are added to the Stream ONE-BY-ONE and ONLY if none of the above [.predicates, .limits] match; 
			4. skip[] - a Position-s array; Will be used FOR SKIPPING items sequentially; 
				5.0. THIS is only matched if NOTHING from 'preserve' is matched; 
				5.1. StaticPosition: This will take the next one that is 'x: StaticPosition' '.next'/'.prev' (depending on direction) calls away
				5.2. PredicatePosition: This will skip the items FOR AS LONG as the given predicate holds...;
			5. halt[] - this is a set of Positions, that, if met/reached will TERMINATE the given Stream's consumption; 
			6. default - this is a 'special' action, if NOTHING amongst these matches, it'll do one of 'preserve', 'halt' or 'skip' [or other...]; 

		Whether the 'predicates', 'limits', 'preserve', or 'skip' are checked for first, the user can choose; 

		IDEA: 
			No, don't do that; 
			INSTEAD: categorize; That is, a list of things is given (BreakPattern): 

				1. BreakPattern = ((Position | DualPosition) | BreakPattern)[] // tree-like structure
				2. Each of the elements of 'BreakPattern' is one of 'type: "..."' (one of the above)
				3. WHEN ARRAY IS PASSED, the given thing is being 'sub-broken'; Thus, one can work through trees; 

				ABOUT ITERATION: 

					Once having achieved the '.break'-ing of a BreakableStream, one does the following: 
						
						1. keep the 'currType' property on the 'BreakableStream', which IS 
							one of the allowed types for a given thing; 
						2. Based off them, RESPOND; 
							2.1. The response can (for instance) be based off a 'TransformedStream', or 'NestedlyTransformed' stream; 
		
		ALL the 'BreakableStream'-s are RELATIVE [in the sense that the current position is used to grab the future '.curr'-s]; 
		
		These can serve as a wonderful alternative to PatternTokeniz-ing via Regular Expressions; 
		Moreover, 'BreakableStream'-s are (very much) capable of replacing sequences of operations inside of 'IndexMap'-s for StreamTokenizer-s; 
			In that, it's entirely feasible to define any operation currently present within the parser.js library using them; 

			Example: parse string: "..."

				(string) => BreakableStream(string, {})

	2.16. CREATE a new file 'constants': this will keep track of in-library constants; 
		Put the 'PRE_CURR_INIT', and other such things THERE; 
	
	2.17. MINOR ISSUE: the 'PredicateStream' - that one DOESN'T have a very well defined 'defaultIsEnd' predicate; 
		In other words, IT REQUIRES for the given thing to be able to tell WHETHER OR NOT in the given Stream, there is
			an item with given predicate; 

		Which is something of a problem, as it requires iterating over the Stream (and is, thus, useless); 
		
		Practical solution for this: 

			{
				// ...
				const hasValue = has(pred)(stream) // changes the 'Stream' position
				if (hasValue) {	
					const S = PredicateStream(stream, pred) // the '.curr' guaranteedly becomes the '.curr' in the 'S'
					// ...
				} else {
					// ... handle the alterior case...
				}
			}

		Think whether an elegant solution may be found (or whether this is to be simply left to the docs...); 

	2.18. Optimize minitua (that is, just go through the code, removing unnecessary repetitions of things, useless work and so forth...); 
		When the same (even tiny) operations get repeated a large number of times, the speed decay accumulates; 
		Try to make the library code that avoids this stuff fundamentally; 

		2.20.1. DO ACTIVE MICRO-BENCHMARKING!

			Having general/particular implementations in mind, MICRO-BENCHMARK, 
				and on the basis of those, optimize particular functions/approaches; 
		
	2.19. More "conditional" properties (like the conditional '.finish', '.rewind', and so forth...); 

		note: these MUST depend on assumptions regarding '.finish'-ness (and so forth) of underlying stream/-s, WHEN they are present. 
			This is because otherwise, the whole "optimization thing" isn't existent
				(example ProlongedStream's '.finish' MUST have all its underlying Stream-s being checked); 
			
		Properties: 

			1. finish (complete covergae);
			2. rewind (likewise, complete coverage); 
			3. navigate - where appropriate; 
			4. prev - where possible/makes sense; 

			5. copy - ALMOST NEVER (turned out to be a bad idea); 
				Trying to implement it on Streams that operate on other Streams, whilst ensuring preservation of index 
					is too difficult (mainly due to the 'Position'-s, and the flaw of having JUST ONE '.navigate' definition); 

	2.20. utility - findNestedEnd: 

		Given a 'ReversibleStream'/'NavigableStream', it does: 

			1. Walk through the Stream (similarly to 'nest' - counting 'depth');
			2. Finds the location, at which the end of the nested block lies (note: A NUMBER); 
			3. Returns that location, whilst also altering the location of the 'Stream' given; 

		This, basically, allows one to "know" where to end the parsing of a given thing (WHEN it's needed to be parsed LATER)!

	2.21. BufferizedStream - a wrapper around a given Stream, to enable the storage of a buffer
		for its operations; 

		Once '.next' is called, it stores the '.curr' inside the inner 'buffer', ONLY IF the user-given '.predicate'
			HOLDS!
	
	2.22. Use prototypal inheritance to save memory; 

		Restructure code a little - provide A SINGLE prototype, that is to be used by the 'constructor' (which are, in practice,
			really just 'Object.create(INITIAL)' + setting of additional properties that are passed)
		
		The user can do this sort of thing themselves, but for operations that are heavy on 'Object.create' usage, it may be useful
			to provide an alias; 

		Thus, there'd be BOTH exports for things that are StreamClass-es (functions for producing base prototypes), and the prototype 
			objects themselves; 

		Same goes for objects that are used within the definitons of different 'Parsers"; 

		NOTE: some types (such as 'IndexMap') would benefit from such introductions INCREDIBLY!

		ESPECIALLY use the prototypal inheritance for the 'Tree'-s (can make the Big ones A LOT more compact memory-wise, need only store the '.value'); 

	2.23. Introduce a new [additional] object model (significantly improves memory concerns for temp-Stream-managment objects...); 

		The idea is: separate the processes of object-creation and initialization; 
		This permits the re-usage of the same interfaces on DIFFERENT targets 
			(making it: 
				
				1. equivalent to a function call [memory-wise] - no need for wasting time on additional allocations/de-allocations;
				2. preserve the "lazy evaluation" (saving memory relative to the constant usage of buffers); 
			); 

		Example ("LimitedStream" vs. "StreamLimitor"): 

			How it is currently: 
				1. LimitedStream: 

					function parseSomething (input) {
						const limited = input.limit(from, to) // creates a new Stream [memory spent on temp-operational object]
						// ... operations on `limited`, OBJECT CONTINUES TO LIVE [used as '.curr' value for upper-level stream]
						return limited
					}
				
					Here, the allocation is NECESSARY, AS THE 'stream' is USED. 
					However, this is not always the case and (sometimes), the operational 

				2. StreamLimitor: 

					const limitor = StreamLimitor() // OBJECT CREATION ___WITHOUT___ initialization; 
					
					function parseSomething (input) {
						const limited = limitor.init(input, from, to) // creates a new Stream [memory spent on temp-operational object]
						// ... operations on 'limited'
						return SOMETHING_ELSE // The resulting object is NOT the 'limited', the 'limitor' is RE-USED for the next stream/input; 
					}

			Basically, this is just "formalized" re-usage of LimitedStream objects with appropriate interfaces; 
			Also, one could allow for 'default'-initialization (during object-creation),
				and only partitially initialize it afterwards;

			Likewise, there'd be 'StreamTransformer' (TransformedStream), 'StreamNester' (NestedStream), and others; 
			Call these `StreamProxy/`; 
	
	2.24. TypeScript - use 'enum's; 

		Learned about 'enum's being present in TypeScript; 
		Use them to designate "special constants" (ALSO - take those out into separate files); 

		Examples of applications (already present): 

			1. StreamClass - PRE_CURR_INIT, POST_CURR_INIT, POST_START...; 
			2. ValidationOutput: 
				1. null - NoCoverage; 
				2. true - Coverage; 
				3. [false, n] - ValidationError(n); // GENERALIZE THE 'TokenType' to an 'Isable', with '.is(x: any): boolean'; 
															Make 'NoCoverage', 'Coverage' and 'ValidationError' into individual 'Isable'-s, 
															identifying outputs from a 'PatternValidator'; 

															GENERALLY, make this sort of distinction BETWEEN DIFFERENT KINDS OF 'Validator'-s!

		ALSO - adapt either the SCREAMING_TRAIN_CASE or PascalCase for the constants; 
		[Current preference is to the latter...]

	2.25. Idea for a method: ProlongedStream.prolong; 	

		This (utilmately) modifies the 'this.streams'; 
		Also, there'd be '.shorten(n: number)' - it'd delete a certain number of elements from '.stream'-s, 
			optionally changing '.curr', and set '.isEnd = true' (if the .curr is in one of the deleted 'Stream'-s); 

	2.26. IDEA: for compact and general implementation of '.copy' on the 'StreamClassInstance'-based StreamClasses: 

		1. Add '.constructor' (as in '.prototype.constructor') to the PROTOTYPE OBJECT of the given StreamClass; 
			1.1. Let the 'constructor' correspond to the function that is to be used for copying; 
		2. Keep the '.copy' ONLY on the 'InputStream', and 'TreeStream';
			Do not implement it on the 'Wrapper'-streams (those that have other Stream-s as '.input'); 

		The 'StreamClassInstance'-s ARE far too diverse to be implementing '.copy' on each one of them; 

		ALSO, regarding usage of 'new': 

			Due to the fact that one is going to be introducing the prototypal inheritance in v0.4, 
				let there also be (optional) 'new'. Done like: 

					funcition Constructor(...args) {
						if (!(this instanceof Constructor))
							return new Constructor (...args)
						// ... 
					}
					Object.assign(Constructor.prototype, ...) // desired prototype

				The difference from the current constructor-functions of 'parsers.js' is that those do not use 'this', 
					and they also have the 'return' statement; 

				The object that is being '.assign'-ed to the '.prototype' is the output of the according 'StreamClass({...})' call; 
					[???On account of that, perhaps rename the 'StreamClass' into 'StreamClassPrototype'?]; 
	
	2.27. LimitedStream - '.findDirection(from?, to?, input?)' method: 

		This (basically) re-initializes the values for '.from' and '.to' on the given 'LimitedStream', re-setting the '.direction' property; 
		During the course of computing the '.next/.prev' on the underlying '.input'-Stream inside the 'LimitedStream', it may come to be that 
			the values are CHANGING!

		Now, there is no presently a mechanism for this within the library (and the thing gets re-computed EVERY TIME), 
			so it may as well be more beneficial/simpler/readable to just store it as a property; 

		This would permit the user to 're-initialize' the 'LimitedStream'; 
		[Possibly] ???Replace the 'StreamLimitor' with something similar? 
			[id est, a SINGLE initialization method that does all the appropriate work?]
			Separate the initialization from allocation...;

			1. ALSO: additionally, do the similar thing for the 'positionNegate' inside the 'isEnd'; 
					THUS, the '.to' will actually contain the "negated" version,
						whereas the user will need to re-initialize the Stream MANUALLY; 

					Will reduce pointless out-of-place re-computation; 
	
	2.28. [Maaaayyybe???] Consider turning the 'StreamClass' into a 'new'-constructor+prototype also; 	
		This is due to that the cost of actually calling it is prohibitively expensive; 
		Instead, one can return CONSTRUCTORS from the 'StreamClass' [they would have prototypes defined by the user signature]; 

			The results of constructors would have their '.isStart', '.realCurr' and other dynamic per-Stream properties
				defined inside the actual returned constructor; 
			
			Then, the function returning this "basic" StreamClass-constructor, would have it copied as a '.prototype' for the other one: 

				ClassFunction.prototype = Object.create(StreamClass.prototype)

			The remainder properties/functions would be specified WITHIN ITS OWN CONSTRUCTOR; 

			This feature (in particular) would permit intantiation of different StreamClass-es, 
				without the need for copying; 

	2.29. MAKE the 'utils' TYPE-SAFE! 

		Currently, what lacks is the ability to dynamically generate the appropriate type-checking functions that may require 
			additional predicates, BUT still preserve the form; 

		Just needs more boilerplate code like: 

			interface C<Type = any> {
				X: Type
			}

			const isC = structCheck<C>(["X"])
			// this 
			const isCType = (isType: (x: any) => x is Type) => structCheck<C<Type>>({ X: isType })

		Currently, the checks can make SOME mistakes still; 
			[Noticed this while doing 'isNumericPositional' - should be refactored...]; 

	2.30. [Maaayybe?] Create a 'samples' directory; 
		This is for 'common-case' parsing - generally, walk through the various syntaxes that were 
			PRESENT within the previous parsers, use them?
		
		Or, better, make this into a separate mini-project called 'parsing-samples'; 
		[EXAMPLE: handling the 'escaped' strings/sequences all the time];

		Think about it [this particular refactoring is VERY fine]; 

	2.31. Generics for the 'isType'-functions; 	
		Learned about TypeScript generics for nameless functions (namely, how they can be used with 'as'): 

			const isType = ... as <Type = any>(x: any) => x is Type
		
		Variation on this - add throughout the library [particularly, when 'Type' is itself a generics parameter]; 

3. Dependency management: 
	3.1. Create a 'proxy-op' library SPECIFICALLY for various optimizations based on Proxy-functions on different types/kinds of objects;	
		3.1.1. These (sometimes) can help GREATELY reduce memory-consumption; Example: the Slicer; Include those there...; 
