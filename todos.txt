[GENERAL]
1. Check if this works with Deno; 	

[v0.3]

CURR TODO LIST: 

-2. CREATE implementations of 'FastLookupTable' that would be SPECIFIC to the use-cases; 
	In particular - CREATE ALIASES for the combined workflow that they are intended to be used in [with NestedStream]; 
	Think about the library workflow - DOCUMENT IT...; 
				
3.25. ANOTHER refactoring with 'one.js': 
	Update it [again]; 
	1. SPECIFICALLY, search for 'id': these (most frequently) are expressed via 
		') => ' sequence [sometimes: ') => x']; 
	2. The (x) => typeof x: 
	3. The '(x) => x === undefined' and '(x) => x === null'; 
	4. copy = (x) => [].concat(x) - in multiIndexCopy (and other places, likely also...); 
	
3.24. [maayybe?? De-refactoring] Think about what to do with interfaces like 'IterableStream<Type>'; 
	They are made [primarily] for the purpose of refactoring; However, they also trash the library's namespace (thus, contradicting minimality); 
	QUESTION: should they remain? 
	[Current decision]: get rid of them; Unneeded, make the types less useful, pain to work with; Their only benefit is refactoring, and [rather limited] potential re-usability; 

	1. Superable, Initializable - ENSURE that all the Stream-s, whose classes implement those have interfaces, THAT HAVE THEM AS WELL!
		1.1. Also - first check if 'StreamClass' has them - then, REMOVE from places that already have both them and StreamClass [redundancy]; 

Order of todo-elimination [v0.3]: 
	3.24. 
	-2 [aliases for workflows, compositions]; 
	3.25. 
	
	-1. re-add the 'is'-functions for the testing; 
		The constructor tests still need them; 

A. testing: 
	A.1. Create list of modules to be tested; 

		CHANGE OF PLANS: 

			0. Refactor the utils into the mini-testing framework; 
			1. FIX the uils;
			2. Re-do all the tests, create missing ones [note: all __relevant__ ones]

		Testing choice [per-method/per-class] and implementation are defined by: 

			1. Stream Classes are tested in the following fashion [they branch]:
				1.1. They branch: 
					1.1.0. Empty Stream instance
					1.1.1. Single-element Stream instance
					1.1.2. Two-element Stream instance
					1.1.3. Long Stream instance (>= 20 elements)
				1.2. There are benchmarks; 	
					2.2.1. In particular, IDENTIFY THE BOTTELNECKS IN DEFINITIONS [if there are any]; 
			2. Pattern Classes are tested in the following fashion [they branch]: 
				2.1. Empty; 
				2.2. One-element; 
				2.3. Two-element; 
				2.4. Long; 
			3. Parsers are tested as follows (with according Stream-s): 
				3.1. There is a set of Stream-s, for which they're tested; 
				3.2. With appropriate Collection-s; 
				3.3. They branch [by 4.1. and 4.2.]: 
					3.3.1. The Empty Stream; 
					3.3.2. The One-element Stream; 
					3.3.3. The Two-element Stream; 
					3.3.4. The many-element Stream; 
			4. Generally, provided an instance of a given class, its methods are tested: 
				4.1. All methods are tested; 
				4.2. If a method is mutating, there is a new instance created (a temp copy), then the operation performed on it; 
				4.3. If a method is non-mutating, it is performed without copying; 
			5. There are benchmarks: 
				5.1. Stack benchmarks: 
					5.1.1 NestedStream - find out how deep a given NestedStream must be in order for the "default" Stack to break; 
						Use as metric later
				5.2. Parse really long strings: 
					5.2.1. Try to parse some incredibly long strings;
						See the speed limits for different string sizes
				5.3. "Cached" TokenInstance vs "Non-Cached" TokenInstance: 
					5.3.1. Auto-generate a GIGANTIC tree in a simple syntax made entirely of TokenInstance-s, then do a version for BOTH cached and non-cached 
					5.3.2. Iterate through them [TreeStream]; Measure time; 
						NOTE: it is expected that the "cached" 'TokenInstance' will be A LOT faster to build/do; 
			6. Rewrite/restructure the 'regex' tests; 
			
		Left to make tests for: 

			1. IndexMap/ [specifics, RE-DO]
			2. Pattern/  [specifics - underlying]
				2.1. Token/ [specifics]
				2.2. EnumSpace/ [specifics]
				2.3. TokenizablePattern/ [specifics]
				2.4. ValidatablePattern/ [specifics]
				2.5. EliminablePattern/ [specifics]
				2.6. Collection/ [specifics]
			3. Tree/ [specifics]
			4. Stream/

				To test:

					1. Position [re-design]; 
					2. InputStream; 
					3. PositionalStream; 
					4. TreeStream; 
						4.1. TreeWalker; 
						4.2. MultiIndex; 
							4.2.1. MultiIndexModifier; 
							4.2.2. Slicer; 
					5. TransformedStream; 
					6. PredicateStream; 
					7. ProlongedStream; 
					8. LimitedStream; 
					9. ReversedStream; 
					10. NestedStream [RE-DEFINE]; 
						10.1. NestedlyTransformedStream;
						11.1. NestedStream; 

				4.0. One tests on a class-basis; 
					4.0.1. Classes are tested per-method; 
					4.0.2. A single class is a collection of methods, ergo, there MUST BE A WELL-DEFINED SET OF 
						general wrapper-tests: 
						4.0.2.1. These would be based OFF passed "checks"; These would (fundamentally) 
							compare the result of the given Stream having a given method called with given arguments, 
								with the GIVEN EXPECTED RESULTS. 
							
							The 'compare' would be class-based; 

							THERE WOULD BE SPECIAL-CASE PER-CLASS DEFINITIONS FOR IT! 
							They would be used with the actual Interfaces; 

				4.1. The tests have TO BE ORDERED [because various interfaces are intertwined]; 

					In particular: 

						1. All PositionalStream-s must come AFTER Position; 
						2. All the sub-interfaces of TreeStream must be checked BEFORE the 'TreeStream'; 
						3. "WrapperStream"-s (such as 'NestedStream', 'TransformedStream', 'LimitedStream' and others)
							must ALL be tested on different combinations/variations of Stream-classes; 

							Thus, for instance, one has to test ALL of: 

								LimitedStream-NestedStream-InputStream;
								TransformedStream-LimitedStream-TreeStream;
								NestedStream-TransformedStream-InputStream;

							And more...

				4.2. There are SEVERAL TYPES of tests for the same method:
					4.2.1. Basic functionality (comparisons, and so forth...); 
						This (typically), just calls the method once, + does some comparison work on the result; 
						These are separated by predicates used for comparison.

					4.2.2. Utils; 
						These are tested regularly via the 'utilTest'; 
						4.2.2.1. ORGANIZE THESE! 
							Let each 'util' go to its appropriate interface directory...;

							NOTE: the 'is'- utils ARE NOT to be tested ; 
								Only things like 'toInputStream', 'uniNavigate', and so forth...

							To test [only]: 

								1. InputStream/toInputStream
								2. IterableStream/isIterable
								3. BasicStream/isStream
								4. RewindableStream/rewind

					4.2.3. Constructors: 
						Make them "typical" (in the sense they uses the 'is'-functions); 

						1. For each of the 'implementations' of the 'StreamClass'; 
							These would be 'and'-s with SPECIFIC properties; 
							LIST: 

								1. StreamTokenizer (Parser/) [need both 'is' and the constructor]; 


					4.2.5. Methods:
						4.3.5.1. StreamClassInstance-specific (general): 
							[predefined]
							4.3.5.1.1. generalIsEndTest; 
							4.3.5.1.2. generalIsStartTest; 
							4.3.5.1.3. generalRewindTest; 
							4.3.5.1.4. generaFinishTest; 

							[per-case-defined]
							4.3.5.1.5. navigateTest (based off 'streamNavigateTest' + comparisons); 
							4.3.5.1.6. copyTest (based off 'streamCopyTest'; for InputStream and TreeStream); 

					4.2.6. Refactoring: 
						4.2.6.1. In tests, rely HEAVILY on 'navigate' (once tested); 
						4.2.6.2. Don't test optimizations (like 'uniNavigate') INSIDE the test-code; 

					NEXT ITEM-s: 

						1. Renew the 'ClassConstructor' function to include: 
							1.1. Ability to ensure that a given 'Stream' is (after all) BOTH creatable using 'new' AND regular function call; 
							1.2. That the properties on it are NOT OWN [that being, that the memory saving is real]; 
						2. per-case defined (.copy, .navigate); 
							2.1. .copy is for InputStream and TreeStream only (can be generalized?); 
							2.2. .navigate is for: 
								2.2.1. InputStream; 
								2.2.2. TreeStream; 
								2.2.3. LimitedStream; 
						3. Boilerplate-implementations (generalIsEndTest, generalIsStartTest, generalRewindTest, generalFinishTest); 
						4. [mini, side] Re-design the 'Position' [re-check what could have been missing - been adding some new definitions...]; 
						5. [Finally] combining the testing code together FOR EACH one of the classes, via signatures; 

						6. ALSO : add a 'streamIterator' TEST [look for other such "minor" utils, if any remains...]; 

			5. Parser/
			6. Stateful/
			7. regex/
			8. benchmarks/
			9. imports/ 
				NOTE: there has to be a WHOLE SEPARATE testing procedure for the 'imports'; 
		
	A.2. Write out dependencies to be tested for each one of them; 

		1. Classes (classes.ts) - ALWAYS
		2. Utilities (utils.ts) - OCCASIONALLY
		
	A.3. Write the tests (module-by-module); 

B. documentation (change/fix it...);
	B.1. Create new pages:
		B.1.1. Home: 
			B1.1.1. Motivation (write that the library was created/conceived primarily as a mean of refactoring of frequent parsing tasks/data-structures); 
			B1.1.2. "One good approach" principle (write that the library was made to have a single "intended" best approach for all the things that are possible to do using it); 
			B1.1.3. Examples [projects done using it]; 
			B1.1.4. Structure - explain how the library structure works (interfaces., methods., classes., utils.); 
			B1.1.5. Known issues (this lists known problems that the library currently has);

	WHEN WRITING DOCUMENTATION, make a note that the '.hash' and '.extension' functions SHOULD BE ABLE TO HANDLE null-values! 
C. rewrite the CHANGELOG [too much has been altered since the "initial" v0.3]; 
	C.1. Walk through the last commit's state of v0.2.1, noting changes and writing them down [destructive]; 
	C.2. Walk through the v0.3 changes, and write them down [constructive];

Note: the new object-oriented approach will enable better speed and lesser structure-copying (better memory use); 
	PLUS: one can have the 'Streams' in question working AS TREES without the necessity for creation of the actual TreeStream-s
		(as the recursive Stream-s more or less serve the same function - only problem is populating them with the appropriate kinds of objects); 

[v0.4]

1. STACK! 
	The library is not too stack heavy, BUT it does have an (ultimate) stack-limitation
		when working with NestableStream-s and the 'nested' util (generally, anything that is NON-FLAT);
	This is very much impossible to solve on its own; 

	SOLUTION: use the CPS style. 
		Re-implement library (whilst keeping the non-CPS version) in CPS, 
		with an infinite 'stack'; 

	! THIS WILL REQUIRE FOR ONE TO FIRST IMPLEMENT THE INFINITE CPS-BASED STACK LIBRARY!
	That one should: 
		0. Be written in TypeScript; 
		Include Types: 
		1. InfiniteStack; 
		2. CPSArray (this is, ultimately, a CPS version of JS array);

	As, CPS is (quite) time-costly to actually run (due to amount of stack usage [primary] + the stack unwinding + additional checks...),
		this sort of thing would be useful for applications where speed of little importance
			(that is: NOT things like apps/websites/interpreters, and so forth; Stuff like AOT-compilers, perhaps);
	
2. New Streams/Parsers + some minor stuff/code-quality/minimalism: 

	2.1. bufferize(process, size)(tree) - util

		Given a 'Tree', returns an array of values: 

			{
				childNumber: number,
				value: any
			}

		This (ultimately) does THE SAME thing
			as the 'TreeStream', BUT without losing the children's Tree-information. 
		Unlike the 'TreeStream', this is intended as
			a more persistent data structure with faster, 
			and NON-LINEAR (important) access than TreeStream.
		
		Would return a 'NodeBuffer' (which is just a kind of an array); 
		Indexing an array could be (potentially) noticeably faster than walking through a Tree 
			(particularly so with the 'TreeStream'); 

		The 'index' is the index FROM THE BEGINNING of the buffer to the current Node. 
		The 'childNumber' is the number of children that come AFTER the current node. 
			Children are read between positions from 'index' to 'index + childNumber', [where 'index' is the NODE'S POSITION! Keep it or keep it not...]; 
				with each 'child' being checked for its own children
					(which, if nonzero, are excluded from the current children, and are instead counted as grandchildren). 
		
		The 'value' is the actual value at the index (note: being 'process'ed - this is intended for removing/keeping children, primarily); 

		If size is used, it will ONLY use this many items from the tree (up to the maximum amount - the rest of positions will be assigned to 'undefined'); 

		BASICALLY, this only works for WHENEVER the 'size' is given; 

			Insted of doing: 

				const final = []
				while (!stream.isEnd && size--) 
					final.push(stream.next())
				return final
				
			It does [except, with no stream - it just walks the tree recursively]: 

				const final = Array(size)
				const staticSize = size - 1
				while (size--) 
					final[staticSize - size] = stream.next()
				return final
			
			Which is faster, as it (basically) doesn't require running the '.push' repeatedly, 
				AND it doesn't risk having 'undefined' values; 

			Except here, the 'stream' [TreeStream] is replaced by the tree:
				so it recursively traverses it, preserving the SAME buffer and index across different recursive calls;

	2.2. TreeStream: create two versions; 

		The 'TreeStream' (currently) is the 'LL' pre-order tree traversal algorithm Stream-implementation; 
		There should be ANOTHER - the 'LR' post-order tree traversal; 

		Rename the 'TreeStream' to 'TreeStreamPre', and create an implemnetation for the 'TreeStreamPost'; 
		This is (particularly) useful for some types of interpreters; 

		The Pre visits the node, then its children one-by-one starting from the beginning, then the siblings; 
		The Post visits first the children, then the siblings, only THEN the parent [in reverse order]; 

		Basically, Pre (parent-to-children) returns the nodes 'as they go' in the left-to-right traversal in the tree,
			whereas Post returns them in 'reversed' order: for the parent to return, children must all return first; 

	2.3. Implement a 'PartialValidator': 
		This, ultimatly, works in THE SAME WAY as the 'PatternValidator' and 'PatternTokenizer'
		Gets the recursive structure with all the 'split' and 'inserted' bits; 
		The 'PartialValidator' would RETURN THE INFORMATION FOR INFERENCE OF CORRECT/INCORRECT PARTS! 
		[Which it does by 'numbering' particular portions of a given Pattern]; 
		This would allow for a greater ability to analyze/point-out the errors in the original input;

		Unlike plain PatternValidator, it would return ALL the possibly available error information, not just the stuff about the index of the "failed" check, or coverage; 

	2.4. Implement a 'level-by-level TreeStream': 
		It would get an initial given 'level' (tree root), walk through all of its '.children', 
			then walk each of the '.children' themselves as levels (one after another...), THEN descending lower...; 
		Would do it via calling '.getJointLayer' function (then - caching for later '.prev'); 
		This (basically) takes the nodes from the previous 'joint layer', gets their respective children,
			CONCATENATES THEM into a new layer, then iterates it; 

	2.5. Add a module SPECIFICALLY for working with strings/identifiers/Sequence-s (currently: the 'Indexed' interface); 

		In particular, functions/abstractions: 

			1. shorten(names); 
				Given a sequence of "names" (strings/sequences),
					and a 'comparison' predicate for each of its elements,
						it would implement a name-shortening algorithm, 
							that would preserve the name uniqueness;
				The return value is an IndexMap; 

			2. renameable(names);
				Given an IndexMap of names (which could come from, for instance, 'shorten'), 
					the method: 

					1. Re-orders;
					2. Creates new name-map entries;

				In a fashion that would allow straightforward implementation of a sequential "exact replacement algorithm" based on the output: 
					1. Loop through a list of name-maps; 
						2. [In the loop] Rename current encounters of a name with its mapped value;	
					
				There are 2 operations that may be necessary: 
					1. creation of temp-names (when collisions in present names are far too high to re-order);
					2. re-order; 
				
				The algorithm for the 'renameable' function: 
					0. Keep the cached (met) names in a 'Set'; 
					1. Loop through the 'names' given: 
						1.0. If the name is in the cached Set, continue; 
						1.1. If the current 'value' of the 'name' is already present amongst 'keys' (another loop): 
							1.1.1. put the keys in question BEFORE the current value;
							1.1.2. '.swap' the two indexes;
							1.1.3. go one position back (because now one needs to check the "others" now); 
							1.1.4. cache the current amongst the checked names in a 'Set', (so that one doesn't get into an infinite loop);

				This is useful when implementing things like mass renamings/name shortenings in code mangling software; 

	2.6. RELAX overly demanding interface and types definitions/requirements;
		Example: inputStreamCopy, inputStreamNavigate; 

			These do not NEED to have all the parts of the 'InputStream' definition (even though they are originally implemented to work with it);
			RE-DEFINE them...;

	2.7. [Idea?] Create an error-handling API for the 'v0.4';
		Create means of: 

			1. Stacking different levels of exceptions (the 'try-catch' blocks); 
			2. Assigning relevant debug information to them;

		This is (primarily) for the development process of the parsers; 
		
		Define a Catcher [rough sketch]: 

			function Catcher (info: DebugInfo, logger: Function) {
				return function (thing: Function, thisArg: any, args: any) {
					try {
						thing.call(thisArg, ...args)
					} catch (e) {
						logger(info)
						throw e // NOTE: THIS here is to represent DEPTH [as parsers can be VERY recursive indeed, it may be needed to eliminate the recursive errors on a case-by-case basis]; 
					}
				}
			}

		Although... This is rather general. Perhaps, better implement as a separate package; 

		Likewise, there'd be ways to count levels of recursion, and assign types to them [specialized signature]; 
		
	2.8. Minor refactoring (StreamClass): 

		Create a special case for the 'StreamClass' - the 'WrapperStreamClass'; 
			Basically, this works in a fashion similar to the 'InputStream', 'TransformedStream', and 'NestedStream', and others such (generally, UnderStream-s and Inputted): 

				1. There is 'iter' function (for iteration); 
				2. There is 'curr' function (for 'initGetter' and the definition of 'baseNextIter'); 
				3. Then, the 'baseNextIter' is defined as: 

					{
						iter.call(this)
						return curr()
					}

	2.9. [mayyyybee??] Try re-doing the project's type-system? [again...]

		The direct use TypeScript's 'interface's cause huge issues surrounding the 'conditional' properties
			(in particular, the need to duplicate to make everything work). 

		See, if not (mayhaps), it is feasible to re-write everything via 'extends BasicStream'-sort of thing
			(meaning, the needed StreamType-s would be "inserted" where desired...); 

		Examples [interfaces]: 

			1. TransformedStream; 
			2. NestedStream; 

	2.10. [maybe?] BreakableStream - a major generalization of the library's patterns: 
		Returns a class for producing Stream-s: 

		1. Have an '.input' (another Stream); 
		2. Has an 'Indexable' of 'BreakPattern's (they are objects) pre-given, which define how the Stream instances behave: 
			2.1. type: string - one of: 
				2.1.1. limit - works like LimitedStream; 
				2.1.2. nest - works like NestedStream; 
				2.1.3. halt - stops the stream [no more items after that]; 
				2.1.4. navigate - finds: 
					2.1.4.1. Either the next item [or the first item AFTER THE BEGINNING] that obeys a given property (PredicatePosition)
					2.1.4.2. Goes to a static position [relative or absolute]; 
					
					ALSO, has a boolean arg 'isRelative' (default: true, basically - whether the values for 'navigate' are to be handled absolutely or not...); 
				2.1.5. transform - applies a given transformation on the '.input' (calls a function); Returns the result;
			2.2. args: object - the way that the 'type' is resolved (equivalent of constructor-arguments/.init-arguments); 
			2.3. buffer: boolean | () => boolean - this one corresponds to whether the current value should be bufferized; 
				When a function is passed, it is called on the 'BufferizedStream' in question to get the value; 
		
		The result of matching 2. to the '.input.curr' is (effectively) evaluated to be the Stream's '.curr'; 

		This todo is a 'maybe?' because all the said things can (pretty much) already be done by the user using StreamTokenizer and the rest
			- this really only just provides a minor abstraction over the whole thing (frees the user from needing to use the exports of the library); 
		This kind of thing is (somewhat) against the library's "all-minimialism" approach; 
	
	2.11. CREATE a new file 'constants': this will keep track of in-library constants; 
		Put the 'PRE_CURR_INIT', and other such things THERE; 

	2.12. [reminder] Optimize minitua (that is, just go through the code, removing unnecessary repetitions of things, useless work and so forth...); 
		When the same (even tiny) operations get repeated a large number of times, the speed decay accumulates; 
		Try to make the library code that avoids this stuff fundamentally; 

		2.17.1. DO ACTIVE MICRO-BENCHMARKING!
			Having general/particular implementations in mind, MICRO-BENCHMARK, 
				and on the basis of those, optimize particular functions/approaches; 
	
	2.13. utility - 'nestedInfo'; 

		1. Finds a depth of a given Stream, based off 'inflate' and 'deflate'; 
			NOTE: the MAX depth achievable; 
		2. Finds its length - how many positions (in 'number') must one walk before the current nestedness ends, from the initiated position; 

		Returns it as a pair; 

	2.14. BufferizedStream - a wrapper around a given Stream, to enable the storage of a buffer
		for its operations; 

		Once '.next' is called, it stores the '.curr' inside the inner 'buffer', ONLY IF the user-given '.predicate'
			HOLDS!	 
	
	2.15. TypeScript - use 'enum's; 

		Learned about 'enum's being present in TypeScript; 
		Use them to designate "special constants" (ALSO - take those out into separate files); 

		Examples of applications (already present): 

			1. StreamClass - PRE_CURR_INIT, POST_CURR_INIT, POST_START...; 
			2. ValidationOutput: 
				1. null - NoCoverage; 
				2. true - Coverage; 
				3. [false, n] - ValidationError(n); // GENERALIZE THE 'TokenType' to an 'Isable', with '.is(x: any): boolean'; 
															Make 'NoCoverage', 'Coverage' and 'ValidationError' into individual 'Isable'-s, 
															identifying outputs from a 'PatternValidator'; 

															GENERALLY, make this sort of distinction BETWEEN DIFFERENT KINDS OF 'Validator'-s!

		ALSO - adapt either the SCREAMING_TRAIN_CASE or PascalCase for the constants; 
		[Current preference is to the latter...]

	2.16. Idea for a method: ProlongedStream.prolong; 	

		This (utilmately) modifies the 'this.streams' [modifying the '.isEnd' accordingly]; 
		Also, there'd be '.shorten(n: number)' - it'd delete a certain number of elements from '.stream'-s, 
			optionally changing '.curr', and set '.isEnd = true' (if the .curr is in one of the deleted 'Stream'-s); 
	
	2.17. [Maaayybe?] Create a 'samples' directory; 
		This is for 'common-case' parsing - generally, walk through the various syntaxes that were 
			PRESENT within the previous parsers, use them?
		
		Or, better, make this into a separate mini-project called 'parsing-samples'; 
		[EXAMPLE: handling the 'escaped' strings/sequences all the time];

		Think about it [this particular refactoring is VERY fine and case-specific, though, it can come in useful...]; 

		No, if one is to do it, the thing should: 

			1. Work with general parsing patterns, that would be CONFIGURABLE for every case; 
			2. Provide low-level Token-s with specialized given names; 

	2.18. IDEA: generalize the type of 'T<K> = CollectionLike<K | T<K>>' properly; 
		It appears at least in 2 places inside the library: 

			1. Tree (InTreeType)
			2. NestedStream;
		
		This might (also) resolve some 'interface'-related issues for the 'NestedStream'; 

	2.19. Generalize the streams even further for general copying implementation: 

		Have to have: 
			
			1. 'props' field (StatefulStream); 
				1.1. This one '.preventsExtension'! [that is, it's a FIXED object, with a fixed number of properties];
			2. the '.copy' is then defined: 

				function uniCopy (stream: ...) {	
					const copy = new (Object.getPrototypeOf(stream).constructor)()
					copy.init(stream.props)
					return copy
				}

		The method would go on the 'StreamClass'; 

	2.20. QUESTION [minimalism]: are 'SkipParser' and 'BasicParser' really useful? 
		The former can easily be replaced with an appropriate table on BasicStream, 
			while 'BasicParser' is equivalent to a 'TransformedStream' (or NestedlyTransformedStream); 
		Consider removal...
	
	2.21. v8-specific optimizations: 
		Having done some benchmarking on some typical Array methods, one arrived at a...
	
		CONCLUSION: 
			1. Handwritten methods are *often* __MUCH__ faster than the builtins ON LARGE PIECES OF DATA; 
				On the more common-place cases, however, they are inferior in time performance; 
			2. THEREFORE, one should do: 
				Re-organize the library (how?) to permit usage of the current methods on LARGE pieces of data; 
				Prior - do more benchmarking; 

				About re-organization: 
					1. Think - whether to introduce explicit low-level 'size-controls' [possibly costly, when run a lot]; 
						Although, it's likely to be optimized/negligible; 
					2. Or, whether to split the thing onto 2 chunks [where the 'size' is precomputed...]; 
						This is less flexible (and thus, error-prone), plus will be more difficult to re-structure; 
					
				Current vote is for 1.; 

				ALSO, about bounds - those should be CAREFULLY BENCHMARKED; 
		
		MORE GENERALLY - try to adapt the library for work on LARGE datasets; 
		It'll make it useful for working with big projects/pieces-of-text; 
	
	2.22. [maaayyybe??] Bring the old planned stuff for v0.3 back; 
		That is the 'Parsers/utils.ts' - 'transform', 'delimited', 'consume' [equivalent of 'LimitedStream', uses 'Collection'-s]; 
	
		Do only if there are code aspects suffering from this significantly: 

			1. readability/accesibility; 
			2. performance; 
		
		Primary cause for this todo is that, despite being "slow" (memory allocation), 
			they did still have the benefit of semantic simplicity; 

		The only cause for performance issues with OOP Stream-s API is overhead from re-structuring the original loop so much
			[need for calling the '.isCurrEnd()' two times more]; 
		Again, do this ONLY if the performance difference is SIGNIFICANT!
	
	2.23. util - enumerateTree; 
		Given an array of "types" of nodes, it recursively converts their properties (given in the 'shapes' array), 
			to single-array-siblings form (the current ChildrenTree); 	
		This would permit a more "variable" set of trees (those that have properties with fixed names), 
			to be enumerated accordingly...; 

		Example: 

			{
				a: any
				b: any
				c: any
			}

		Becomes: 

			{
				value: [
					// ... a, b, c
					// ... or b, a, c
					// ... or any other order
				]
			}
		
	2.24. About generics - relax them; 
		Use ONLY WHEN NEEDED; 
		For example: a method makes use of X generics, out of which Y < X is needed [id est, used somewhere beyond the 'this: ...' in the parameters list]; 
			REMOVE the unneeded ones; 
			RE-ORDER THEM, if need be; 
			Try to make them more minimal...; 
		
3. Dependency management: 
	3.1. [maybe?] Create a 'proxy-op' library SPECIFICALLY for various optimizations based on Proxy-functions on different types/kinds of objects;	
		3.1.1. These (sometimes) can help GREATELY reduce memory-consumption; Example: the Slicer; Include those there...; 
	3.2. [maybe???] Create a 'refactor' library, specifically designed to refactor common design patterns; 
		In particular, it would have: 	

			1. 'delegate' functions [from here]; 
			2. the 'classWrapper' function; 
		
		Expand the list; 