[v0.3]
	
5. Source code TODO-s: 
	2.73. Do research (V8 performance): https://floitsch.blogspot.com/2012/03/optimizing-for-v8-introduction.html
		SPECIFICALLY - *inlining*. The library assumes its usage HEAVILY to remain efficient. 

	2.74. Do research (Chevrotain - source, capabilities, optimizations): 
		It is *the* fastest JS parsing library out there. See what it can do...:

			1. [basics] http://chevrotain.io/docs/tutorial/step0_introduction.html
			2. [important!] http://chevrotain.io/docs/guide/introduction.html

			[Post parsers.js v0.3; ADD this as parsers.js v0.4 todo...]; 
			3. [yes, that thing also...] https://github.com/Chevrotain/chevrotain/blob/master/packages/utils/src/to-fast-properties.ts#L2
			4. [building v8] d8 - USE IT to profile the optimizations (lack of) of the used library code [reflect upon, and change the library respectively]
			5. [Stream-specific] see if the '.curr/.next/.prev'  method re-binding optimizations were an overkill [elimination of 'this.isEnd = true/false/whatever-for-each-call']; 

	2.76. [parsers.js] Set up a GitHub Actions -> npm pipeline; ALSO - ADD A GitHub Workflows FILE for updating the latest version, and publishing to npm... + publishing the latest docs [separate]
		ALSO - create a list of presently maintained projects, for which to add such a pipeline: 
	
Order of TODO-elimination: 	
	TODO [fixed order]:	
		TODO: add `nextChar(n: number)` implementation for `ISource`-implementing classes; 
			1. The `ISource`s must implement `nextChar(n?: number = 1)`; 
	
		BIG: .peek() CYCLE: 	
			ME5. 'InputStream' - provide a '.peek(n)' method: 
				1. looks "forward" n items; 
				2. add a new interface - PeekableStream
					2.1. Add a general 'StreamClass' implementation: uses a 'PreallocBuffer' (empty by default), which 
						keeps the 'next n' items, to be looked up. 
						1. For when '.buffer' is '.isFrozen === true', this IS NOT necessary, one can (instead) just 
							index appropriately [reflect this...]; 
    			
						PROBLEM [PreallocBuffer]: what about the `.next()` calls? 
							In order to *FREE* the buffer for a sufficiently large `.peek(n)` operation after doing `.next(k)`, 
								one will have to call `.unshift()` [which is horribly slow]. 
							THEREFORE, create a class that EXTENDS the `PreallocBuffer`, and use that instead. 
							Call it `RotationBuffer`, and it will work like: 
								1. keep the internal array [pre-allocated, as per the `super` methods]
								2. keep the `.rotation: number = .length - 1 (by default)` index
									It will correspond to the `[i]` index AFTER WHICH the `.peek(1)` [respectively, `.read(0)`] value, starts. 
									It goes up to `.peek(.length - .rotation)`, after which it "wraps around", with internal `.array[0]` being `.peek(.length - .rotation + 1)`, and so forth, 
										up until `.peek(.length)`, which is the last permitted '.peek' for the chosen allocation. 
								3. keep a method for "moving" '.rotation': 
									1. .rotate(direction: boolean)
										if 'true', increases the `.rotation`, 
										otherwise - decreases it. 
    			
				Make 'StreamClassInstance' a `Partial<PeekableStream>` [add the generic `.peek()` definition for case of keeping a small '.peekBuffer' - NEW PARAMETER -- 'peekSize: number = 0']. 
    			
				1. Also - add `.next(n: DirectionalPosition = 1)` and `.prev(n: DirectionalPosition = 1)`, 
					where `n` is the number of items TO SKIP [forwards, or backwards respectively]. 
						The `this.curr` is returned INVARIABLY. 
						Note: this will require some heavy rewriting on behalf of the `StreamClass.prototype.next`; 
						1. ALSO - add the new `JumpStream = (n: number) => (value: Stream) => new StreamParser((input) => input.next/input.prev(n))(value)` [rough]:
						 	The old `parsers.js` had something similar, except not as well-designed...
    			
				2. Also - add the `.has(n)` 'Parser/utils.ts'-utils AS A METHOD, 
					of positive/negative indexes [with respective optimizations]. 
    			
				3. Then - REWRITE the `skip()` util from the `Parser/utils.ts`: 
					Will, from now on, be doing only some fairly simple conversions (PredicatePosition [/w .boolean] -> Function; 
						Then call either '.next(f/n)' or '.prev(f/n)'
					). 
		
			CC1. PreallocArray - a new 'Collection' type: 
				0. new property - 'lastInd: number = this.value.length'
				1. extends ArrayCollection
				2. new method - '.prealloc(n)': 
					1. If 'this.value.length > n', goto 5, else - continue
					2. this.value.length = n
					3. this.lastInd = this.value.length
					4. Finish
				3. change method - '.push()': 
					1. this.value[this.lastInd++] = this.value
				4. change method - '.get()' [as 'readonly number[]']: 
					1. return this.lastInd < this.value.length ? this.value.slice(0, this.lastInd) : this.value
				5. change method - '.move(n)': 
					1. this.value.length = this.lastIndex
					2. const lastValue = this.value
					3. this.value = Array(this.n)
					4. return lastValue

				Purpose: 
					1. Providing an interface for user-level preallocation of an array
					2. Providing 1. together with an integration with the 'Collection' interface

			CC2. RotationBuffer - See `.peek(n)` for more details

			ME6. `LazyStream` - add `peek(n)`, with `n <= peekSize`
				`peekSize` - new parameter [either constructor, or configurator pattern, decide which...]; 
				Since it's not a method, one very well may want to do it the constructor way... 
				
		Fix docs: 
			1. classes [add docs]: 
				1. HashMap.InternalHash.ArrayInternal
				2. HashMap.BitHash
				3. IndexMap.ObjectMap
				4. Indexmap.ArrayMap
				5. MultiIndex [fix]
				6. EXPOSED `StreamClass` [and `DefaultEndStream`]; 

				Rename: 
					1. InternalHash - new names of classes end on 'Internal'
				
				7. remove `Composition`
				8. WALK through current classes, fix the discrepancies
					Includes: 
						1. missing classes
						2. missing methods
						3. changed method signatures
				9. *describe* the `generic collection` thing [whole separate page]
					It is to be LINKING to the Autocache-based functions that make use of it
						[LinearMapClass, StreamClass, etc]; 
					Important part is: they themselves will mention of the page nothing. 

			2. interfaces: 
				1. new - ILinearMapClass
				2. new - INestedStreamConstructor
				3. removed - IStreamHandler
				4. walk through interfaces - ensure that all are present...; 
				5. new - IStreamParserConstructor
				6. new - ILimitedStreamConstructor

			7. utils:
				0. Stream.Position: 
					1. remove deleted items [positionConvert, positionSame]
				1. HashMap.fromFlags [jsdoc + permalink]
				2. Stream-related stuff: 
					1. Stream\utils.ts
						1. add missing/fix bad jsdoc [isPrevable, isStarted, isReversible, isStateful, isEmpty, etc]
						2. add missing permalinks/update utils.Stream page...; 

				3. Permalinks (missing): 
					1. utils.md	

				4. walk through utils - ensure that EVERYTHING is in order...; 
		
		2. Tests: 
			[old]
			1. Composition
			2. HashMap
			3. IndexMap: 
				1. LinearIndexMap
				2. PersistentIndexMap
			4. LookupTable

			IMPORTANT: walk through new list of exports, VERIFY that all the tests are IN ACCORDANCE with them - nothing is missing, 
				in other words. 
		
		TODO: 
			walk through the code, FIX redundand `new` type expressions. 
				More specifically: 
					1. give them names
					2. add the missing `.constructor` property where it's required (note: ON THE INTERFACE - where one uses the `Autocache` pattern); 

		1. Doc-sync

		Docs: 
			1. Stream

		3. Doc-sync 	
					
		Docs:
			1. Position [MultiIndex]		

		TODO [naming issue]: 
			minor - module `TableMap` has an export `TableMap`, 
				forcing the user to write `TableMap.TableMap` [not good...]. 
			Options: 
				1. [currently preferred, reason - these 2 are (pretty) unique in their own right...] export * from "./TableMap/classes.js" ["pouring out" the 2 exports of the module]
				2. renaming the module
				3. renaming the function
			
		TODO [docs]: 
			1. list missing wiki pages [classes]
			2. write those
			3. work on interface pages
			4. cleanup: 
				1. add the `I`- to ALL the pages (some still missing it)
				2. proper self-referencing
				3. other remaining todos [order those...]

		TODO [tests]: 
			1. finish "old" tests
			2. add the "new" tests
			3. DO TDD + DOCS [starting with `LineIndex`]

	TODO [any order]: 
		-2. [minor] source code: 
			1. fix the `abstract` classes' names: 
				1. provide a STRICT convention: 
					1. Pre < Base
					2. Base contains NO abstract methods [if it does, then it's a PRE-class]; 
					3. use the `Base/Pre` when there is no better name
				2. when returning a class instance, use `camelCase`-named classes, WITHOUT the `_`
		1. refactor `makeDelegate` into one.js [AND MAKE IT LOOK PRETTY! - refactor the function itself]: 
			1. ALSO - refactor the 'ConstructorHaving' interface into `types`
			2. ALSO - refactor the `ThisMethod` interface into `types`
		2. add permalinks for "minor" class-exports
		3. Add JSDoc for: 
			1. minor classes/functions ['classes' exports]
			2. interfaces [those without a page]
			3. big classes/interfaces
				Format for jsdoc for "big" exports: 
					1. brief description
					2. link to wiki page
		4. GitHub Workflows
		5. Special Pages: 
			1. Usage
			2. Home (finish)
		6. Tests: 
			1. New developments: 
				1. 'emptied()' - new method on `IFreezableBuffer': 
					add tests for it

			2. Add relevant (missing) tests
			3. Modify existing tests: 
				NOTES: 
					1. be ignorant of the internal workings of the thing [behaviour-based]: 
						1. currently, some tests are breaking encapsulation
							FIX, via (instead) providing necessary public methods
						2. some functionality is (currently) not testable due to 1.
							Will need to modify the source code (again)
					2. refactor using one.js v0.5
					4. FOR EACH MODULE, DO: 
						1. List the methods/properties [classes], and functions [utils]
						2. List the methods to be tested
						3. Implement the tests (from scratch)	

				Micro-behaviour plan: 
					2. walk through the tests, INTRODUCING CHANGES [do ONLY for methods that are currently, ALREADY PRESENT (stick to modular test-fixing approach)]: 
						note: THIS IS THE LIST OF *ALREADY EXISTING* tests, to be fixed...

						[EnumSpace]
						3. EnumSpace

						[IndexMap]
						4. IndexMap
						5. HashMap
						6. InternalHash
						7. LinearIndexMap
						8. LookupTable
						9. PersistentIndexMap

						[Parser]
						10. Composition
						11. TableMap

						[Position]
						12. MultiIndex

						[Stream]
						14. Stream
						15. InputStream
						16. LimitedStream
						17. NestedStream
						18. PredicateStream
						20. StreamClass
						21. StreamParser
						22. TreeStream
						
					3. eliminate the old test cases
					4. add new test cases: 
						They bear form:

							suite("ClassName (case #1)", () => {
								ClassTestObject.withInstance(
									new Class(), 
									(instance) => {
										test("ClassName.prototype.methodName1 (case #1), () => 
											instance.methodName1(...)	
										)
										
										test("ClassName.prototype.methodName1 (case #2), () => 
											instance.methodName1(...)	
										)

										...
										
										test("ClassName.prototype.methodName2 (case #1), () => 
											instance.methodName2(...)	
										)

										...
									}
								)
							})

						TODO: create
							1. `TestCounter` class
								for *counting* items within the given 
							2. `TestChain` class, 
								which has an internal `TestCounter`, 
									has a `handler` function, and 
										"inputs: any[][]" arrays of arguments to be 
											supplied to the `handler`

							USE the `TestChain` to express the chains of tests thusly. 

					5. remove the old `lib.ts` (make current `_lib.ts` into the new `lib.ts`); 
				
				2. FIX testing micro-behaviours: 
					1. add `handlers` to ENSURE that certain specified properties of test-arguments are met: 
						IF a property is NOT satisfied, THROW an error...
							SPECIFICALLY: 
								1. IF `.value` (or other such injectable property) 
									CAN be `null/undefined`, it must be checked that it isn't	

									Particular examples: 
										1. StreamClass (those that are `isPattern` - LimitedStream, ...)
								2. IF one has a "main type", from which "child types" 
									inherit, the "main type" MUST CHECK explicitly for
										properties/methods of a sub-type before calling 
											them.

									Particular examples: 
										1. StreamClass (.prev!(), .pos!, ...); 
									

					2. MAKE COPIES, 
						in other words - DO NOT rely upon a "single state"
							of the given object; 
						
						Do it thus: 
							1. take in the `instance`
							2. make A COPY of the instance
							3. FOR EACH method-test/property-test, 
								CALL them with THE COPY (which then may be altered)

					3. CHANGE the function-tests to CLASS-tests: 
						Reasons: 
							1. STORE the single `instance` property 
								on `this` (this - simplify the calls)
							2. SIMPLIFY the naming conventions for method-tests
								no more needless prefixes, just `this.methodName(...)`; 
						
						'ClassTest' is a class to be EXTENDED thus: 
							0. `ClassTest` COPIES the injected instance, during method/property tests (to permit safe local mutability)
							1. create a `static` symbol `methods`, which defines symbols for methods to be used
								1. Accessible (in general) via the `.constructor.methods` property
							2. methods TESTED DYNAMICALLY, via '.testMethod(methodName, args)'
							3. add simple method-names like 'name(...args) { return this.testMethod("name", args) }'
								Purpose: refactoring, simpler code, better modularization
							4. REMOVE the "big" tests: 
								1. remove the "signature-types": 
									Reason: 
										1. too verbose
										2. unneeded unification of testing logic
										3. bad for modularity (due to 2.)
										4. bad for readabilty (due to 1.)
					
					4. ALLOW injectable method-tests: 
						1. NAMELY: 
							1. Create CLASS-TESTS for INDIVIDUAL methods (THIS IS BASED off 'MethodTest', which calls the provided injected method-test-handler)
							2. INJECT the `this` (`instance`) to use with them
							3. INJECT the `method-test` singleton-instances INTO the class-test singleton instance
						2. They are THEN called via `ClassTest.prototype.testMethod` [which is abstracted over using properly named methods]

					5. REMOVE the "constructor tests": 
						1. those ARE NOT needed
							More specifically, REPLACE them with "interface conformance" tests. 
							These check ONLY for: 
								1. presence of all required-by-interface properties
								2. their types (neglecting the TypeScript-only mutability requirments)
							
							This ensures that ALL the methods are safe to call on the given object. 
				
				3. Add missing methods'/properties tests
					Walk through every class, list the missing methods, 
						implement tests for them, make adjustments to the 
							case-signature. 
							
				4. Add missing utils tests

				5. Remove redundant code

				6. Add the missing classes' tests
					Example: DynamicParser

				7. Add the tests for *THE NEW stuff* [notes moved from v0.4]:
				 	DO TDD with it (
						previous functinoality development 
						took so long due to lack of clear requirements
					)
					
		7. Self-referencing on Wiki: 
			1. provide links to interfaces for class pages
			2. provide links ot implementations for interfaces to interface pages

		8. To add Wiki pages [classes]: 
			[Parser]
			4. Composition
			5. PreSignature
			6. LayerSignature
			7. StateSignature
			8. DynamicParser
			9. SignatureIndexSet
			10. TableMap
			11. MapWrap

			[Position]
			12. MultiIndex [shares page with `MultiIndexModifier`]

			[Stream]
			13. InputStream
			14. LimitedStream
			15. NestedStream
			16. PredicateStream
			17. TreeStream
			18. StreamParser

			[Node]
			19. TokenNode
			20. ContentNode
			21. RecursiveNode

		9. OPTIMIZING `TypeScript` property definitions on classes: 
			When [in TypeScript], one does: 
				class C {
					prop: P
				}	 

			This DOESN'T ACTUALLY create a property of 'prop', however, doing this: 

				class C {
					props: P = ...
				}

			DOES. UPON CREATION OF THE OBJECT ['constructor' call]; 
			When one needs a PROTOTYPE PROPERTY, use the first variant (type-level only), 
				where it's an INSTANCE PROPERTY, use the second, as doing so wil avoid the 
					necessity for type-transitions in code. 

	JSDoc [on the par with wiki]:
		These are short and concise, don't get too deep into the implementation details. 
		[The "big" full GitHub Wiki will be for this...]

		Process: 
			0. add permalinks to: 
				1. minor classes interfaces [DEICIDE WHICH!!!]

			1. Walk the wiki/ tree [enumerate, COME FROM /wiki]: 
				[proper pages, + JSDoc that references them /w brief descriptions]
				2. interfaces
				3. classes

			2. for each, add one of either: 
				1. A full JSDoc
				2. A brief JSDoc + page

				DECIDE WHICH IS WHICH

			4. Special pages to write: 
				1. Usage
				2. Problems
				3. Lacking: 
					Make this a brief list (BASED OFF ONE on 'Home' page): 
						1. detalize the "various" unspecified options [provide sub-lists for those]
						2. do not expand the lists (it is complete on 'Home')
						3. write out a paragraph or two for each of the points "missing" or that are "problematic"
							1. ALSO - note about WHICH of the abstractions ARE to be deleted (like 'TokenizablePattern/ValidatablePattern', for instance...)

			5. VERIFY everthing visually, once the docuementation work is done...; 

	1. Wiki:	
		3. DOCUMENTATION POLICY: 
			1. IF `x` is an alias/util, DO NOT add a new page for it: 
				JSDoc ALONE will suffice. 
				JSDoc contains: 
					1. brief description
					2. @type tags: 	
					3. other tags...
					Read more about `JSDoc` in TypeScript at:
						https://www.typescriptlang.org/docs/handbook/jsdoc-supported-types.html	
			2. IF `x` is a class, it has a page + JSDoc with reference instead
				Page contains: 
					1. method documentation/signatures, output types
					2. public variables documentation/types
					3. *Notes* [those are important...]
				JSDoc contains
			3. IF `x` is an interface, it has a page + JSDoc : 	
				Page contains: 
					0. general info (purpose, usage, dependencies links, etc)
					1. (brief) method/property descriptions
					2. list of places where it's used
					3. provided implementations' list
			4. IF `x` is NOT an export, it is: 
				1. documented, IF it is somehow an important internal detail (ad-hoc decisions)
				2. otherwise, left without documentation (either a JSDoc, or a Wiki page)
		
		4. Wiki preface wording: 
			Work on the wording for: 
				1. `classes` pages: 
					0. low-level: `This module contains implementations for interface ...`
						Or [when multiple]: `This module contains implementations for interfaces ...`
					1. high-level: `This module contains items related to ... [process/idea/interface]`
						Or [when multiple, or contains implementations/utils/interfaces besides]: `... (as before) as well as ...`	
		
		5. Wiki page structure: 
			For structure-pages: 
				0. # *NAME*
				1. preface
				2. ## Exports
				3. [list of exports]
			
			For content pages [classes]: 
				1. # *NAME* [without the '.'-path]
				2. Description
				3. ## Methods
				4. *use the old ```ts...``` thing + descriptions for each argument
				5. [optional] per-method notes
				6. [optional] per-property notes
				7. [optional] per-class notes
		6. IMPORTANT - self-references: 
			EVERY time that a name from the library is mentioned, add: 
				1. `x` - code-stylization
				2. [...](...) - references, point to the abstraction's page
			DO IT SEPARATELY! 
		
		7. remove unused wiki-pages: WALK THROUGH THEM.
			Some capitalized-functions have themselves a separate page, 
				even though they don't really deserve it. 
			Delete those. 

	2. Workflows [GitHub Actions]: 
		1. make-tag [like in one.js]: 
			1. updates 'package.json' version; via `npm version`
			2. makes tag
			3. commits
			4. pushes
		2. npm-publish [like in one.js]: 
			0. runs the run-tests as a Reusable Workflow
			1. publishes to npm
		3. run-tests [not quite like in one.js, but similar]:
			0. builds the project and tests
			1. runs the tests

		1. AFTER you copy/write those, ADD a GitHub Gist for them: 
			1. make-tag
			2. npm-publish

			__NOT__ the `test` command. 
			Reason: too different from project-to-project. 
			These are the same. []

	TESTS [work all anew... too much has changed]: 
		1. enumerate the things to test; 
		2. walk through tests, verify them for being satisfactory; 
		3. re-do where necessary, add new cases; IMPLEMENT MISSING;
		4. re-do the 'import' tests as well; 
		5. log the 'run.ts' thingie into a special temp text file; 
			1. [make into an npm command - 'test-run' and 'test: test-compile + test-run']
			2. add to '.vscode/launch.json' [to simplify the debugging process... we'll need it]; 
			3. WORK on the failed tests from there...; 
		
		6. WRITE a single large 'tests/demo' INTEGRATION TEST, which is a JSON/hybrid-parser. 
			It would utilize: 
				0. JSON: TableMap + HashMap
				1. hybrid: TableMap + IndexMap [local mutability]
				2. StreamParser
				3. DynamicParser
				4. switching between "hybrid" and JSON-modes BASED off 'JSON' contents [global mutability]

			Reasons: 
				1. new DynamicParser is EXTREMELY complex [hence, errorprone]
				2. to test out the overall interface manageability
				
	PROBLEMS: 
		Understood that the v0.3 IS NOT YET PRODUCTION-READY
			from feature perspective - there's just too little stuff. 
		Thus, one TAKES SOME of the features from v0.4 TO IMPLEMENT HERE. 
		BUT, before then - ONE NEEDS TO *DIVIDE* the library into "condemned (dead)
			and *LIVING* pieces. 
		
		Among specific restrictions:
			2. WRITE DOCS AND TESTS FOR OLD VERSIONS FIRST, then - 
			3. implement the (a) missing feature(s) that go into the v0.3
			4. alter docs 
			5. return to 3., repeat until done
		
		TODOS for v0.3: 		
			Primary: 
							
				PO2. [new Position class] Traveller, a point on a Stream [Position/classes.ts]
						New class for remembering absolute Position on a `Stream`. 

						To be usable with:
							1. MultiIndex [on TreeStream]
							2. LineIndex [on any `IStream<string>`, or (if generic), with "newline"/"non-newline" items]; 
								Generalize to an n-tuple, possibly?

						Fast implementations through: 
							1. '.rewind + .nagivate' [default - botha re implemented manually on TreeStream]: 
								note: ACCEPTS 'IPosition'; like `navigate`
							2. '.navigate(n: number)' [default - fast default implementation for LineIndex]

						CONCLUSION: 
							THE `.travel` method WILL WORK via internal fast-implementations; 

						Default implementation [sketch]: 

							travelDefault = function (pos: Position) {
								if (isNumber(pos))
									return this.navigate((-1) ** (this.pos > pos) * (pos - this.pos)) // final 'this.pos+ = this.pos - (this.pos - pos) == pos'
								else {
									// the default WITHOUT this.pos
									this.rewind()
									return this.navigate(pos)
								}
							}
						
						The 'Traveller' class [in 'Position/classes.ts'], 
							is just a utility class: 

							[sketch]
							Traveller implements PositionObject<IPosition> {
								protected value: IPosition
								protected stream: TravelableStream
								travel () {
									return this.stream.travel(this.value)
								}
							}
							

						ALSO - TODO: add a `travel` util (analogous to `navigate`, `rewind`, etc), 
							and REFACTOR IT into the `LimitedStream.prototype.init`; 
							It would go like: 

								travel = function (this: IStream, point) {
									rewind()
									navigate(stream, point)
								}
									
				UP2. A new utility function (Parser/utils.ts) - match(word, stream): 
					For a given 'Indexable', it does: 

						// ! PROBLEM: lacks ability to check for MULTIPLE WORDS: 
						// 		* Solution: replace 'matchWord' with a more complex hand-written 'RegExp'-matching engine for the library... [uses 'Stream's instead of JS 'string's]: 
						// 			ALSO - use the *same* engine for the 'RegExpTokenizer'; 
						// rough sketch - requires the '.peek(n)' method; 
						// NOTE: the '.peek(0) === .curr' is ALWAYS a requirement...
						function matchWord(stream, word) {	
							const positions = word.length
							for (let i = 0; i < positions; ++i) 
								if (word[i] !== stream.peek(i)) 
									return 0
							return positions
						}

						// IDIOM: for skipping a word, call it 'function consume(stream, word, wrapper)' - a 'Parser/utils.ts' utility; 
						const matched = matchWord(stream, word)
						skip(stream, matched)
						if (matched) return wrapper(word)

				UM1. [maybe?] Add new `Indexed` module - works on `IIndexed` interface, and derivatives; 
					In particular, functions/abstractions: 

						1. shorten(names); 
							Given a sequence of "names" (strings/sequences),
								and a 'comparison' predicate for each of its elements,
									it would implement a name-shortening algorithm, 
										that would preserve the name uniqueness;
							The return value is an IndexMap; 

						2. renameable(names);
							Given an IndexMap of names (which could come from, for instance, 'shorten'), 
								the method: 

								1. Re-orders;
								2. Creates new name-map entries;

							In a fashion that would allow straightforward implementation of a sequential "exact replacement algorithm" based on the output: 
								1. Loop through a list of name-maps; 
									2. [In the loop] Rename current encounters of a name with its mapped value;	
								
							There are 2 operations that may be necessary: 
								1. creation of temp-names (when collisions in present names are far too high to re-order);
								2. re-order; 
							
							The algorithm for the 'renameable' function: 
								0. Keep the cached (met) names in a 'Set'; 
								1. Loop through the 'names' given: 
									1.0. If the name is in the cached Set, continue; 
									1.1. If the current 'value' of the 'name' is already present amongst 'keys' (another loop): 
										1.1.1. put the keys in question BEFORE the current value;
										1.1.2. '.swap' the two indexes;
										1.1.3. go one position back (because now one needs to check the "others" now); 
										1.1.4. cache the current amongst the checked names in a 'Set', (so that one doesn't get into an infinite loop);

							This is useful when implementing things like mass renamings/name shortenings in code mangling software; 

					Put identifier, and Indexed-related stuff there (organization, overall tidyness); 
					Problem with this: 
						1. short
						2. (potentially) generics-mania strikes again...

					ALSO - make `RegEx`/`Eliminator`/`Validator`/`Tokenizer` A PART OF THIS [specifically - export those as submodules/classes]; 
						
				IMPORTANT NOTE: `RegexTokenizer` MUST ALLOW FOR ERROR-HANDLING!!! 
					More specifically, it takes: 
						1. an `IndexMap` [or another kind of table... maybe just a `Pairs`] of tokens
						2. then - it walks through its items for a given index, trying to `.match` the respective `Regex` object
						3. IF none fit, IT PROVIDES AN `error-handling` callback, which the user may employ AS THEY SEE FIT! 
							More specifically: 
								1. an instance of an `IErrorHandler`-implementing class, contains a `nextItem()` [optional] and `error()` [mandatory]
									methods, which work like:
										1. if `nextItem(stream, info: IErrorInfo)` is present, it passes the respective `tokenStream`
											to it, and `nextItem` skips the respective nodes until an acceptable one can be 
												obtained via `stream.curr`. 
											
											The return value of `nextItem` is a new interface - `IPositionRange`:
												it's a pair of positions: [IPosition, IPosition], like [for instance]: 
													1. [ILineIndex, ILineIndex]
													2. [MultiIndex, MultiIndex]
													3. [number, number]
													4. [IPositionPredicate, IPositionPredicate]
													...

												TODO: pass an `IPositionRange` to the `LimitedStream`; 

											The returned `IPositionRange` would be correspondently treated 
												by the interface in question. 

											For the case of `TokenStream` [the "RegexTokenizer"], 
												it is [basically] a `[ILineIndex, ILineIndex]`. 
												The `IPositionRange` is then passed to an (optional)
													`.handleErrRange(this.value.buffer, range: IPositionRange)` method on the 
													Also, the `this.value.buffer` is the UNDERLYING 
														buffer on the underlying [used by the 'TokenStream'] `IStream<string>`; 
														It (basically) allows one to process the given range of positions 
															"the right way". 

											The `IErrorInfo` is a `number` - error code, 
												for one of possible errors that can ocurr. 
												[It's intended that the user will use an `enum` of `number`s for this]
										2. call error()

								[data access]
								1. is kept upon a SPECIAL class/interface-instance called `ErrorHandler`: 
									1. the `handler` has access to various state: 
										it can be respectively pre-configured  by the user
									2. `TokenStream` can have the `ErrorHandler` used INJECTABLE
									3. The "base" `ErrorHandler` would have EXTENSION-CLASSES: 
										1. [dynamic, via getter] `lineIndex` [use `dig + .value` - would keep the respective "top" Stream, from which to keep counting]
											Useful for error-throwing inside the `.error()` method; 
										2. [dynamic, via getter] `errorNode` [use `.value + .curr` - would keep the respective `Stream`, while also SERIALIZING the `Node`, 
											at which the error has happened and/or use its `.type`/`.value`]; 
											
											Useful for error-throwing inside the `.error()` method; 
										3. [static, via prototype - Configurator pattern] `nextItem()`: 
											This would be user-defined. 
											Allows, particularly, to work with

										4. [static, via prototyep - Configurator pattern] `handleErrRange()`: 
											This is user-defined. 

							ALSO: this behaviour MUST BE INJECTABLE. 
								Reason: the user may want to keep the tokenizer AS A PROPERTY in their own 
									`IStream`, which would be responsible for handling errors. 
						4. add to `samples`: 
							1. trivialCompose(IndexStream, InputStream)
							2. trivialCompose(IndexStream, LFStream, InputStream)
							... [etc, INCLUDING the `LazyBuffer`]; 

						5. generalize the "TokenStream" error-handling mechanism that employs the `ErrorHandler` object, 
							and employ (generally): 
								1. uses an `ErrorHandler` with (optional) `handleErrRange`, and (optional) `nextItem`, 
									and (mandatory, if neither other are present) `error`
								2. uses an arbitrary `IStream & IBufferized`
								3. uses an `IErrorDispatcher` - any function that can return a `type IErrorInfo<Type = any> = number | Type`: 
									more specifically, these (in practice) would be `TableMap`-s, returning either: 
										1. the `IErrorInfo` (whatever it is)
										2. the `Type` (on success)
											There should be a clear `isErrorInfo` predicate to distinct one from the other; 
											Upon `IErrorInfo`, one has the `ErrorHandler`
												dealing with it (in the said fashion, working with the `buffer`-ized `Stream`, 
													which is PROCESSED by the `ErrorDispatcher`, which ITSELF is a 
														`TableMap` for the current `IStream`; 

														Then, one needs a new kind of `Stream` for this kind of thing. 
															Call it `ErrorStream(dispatcher)(buffer, pos)` [configurator pattern]
														
														Then - one can put it into a `CommonParser`, 
															and use with a `CommonParser.prototype.init(...)`; 
															That is where creation of instances of `ErrorHandler`
																would occur. 
												). 

									PROBLEM: with using `IErrorInfo = number`: can't handle `Type = number` then. 
										Solution: allow generics? Think of something, this is nigh-trivial...
				
				IMPORTANT NOTE: 
					after adding `RegexTokenizer` [`TokenStream`], add: 
						1. 'isCRLF' utils - returns if `new Regex("\r\n")` matches AT CURRENT `stream` POSITION (this should be easy to do...)
							Put together with the `isLFSkip` util. 
							More exactly: 
								1. add the `isCRLF = (x: IStream<string>) => new RegexStream("\r\n").matchCurr(x)` [or something...]; 
								2. add the `isCRLFReplace = (x: IStream<string>) => isCRLF(x) ? "\n" : x.curr`
						2. 'isNewlineSkip = or(isLF, isCRLF)'
							This is for the user to be able to instead of transforming "CRLF -> LF", 
								just use this as-is [count the newlines]; 
						3. `LFStream` - a stream returning, for an `isNewline` predicate 
							[not INewlinePredicate, instead (stream: IStream<string>) => string], 
								INSTEAD of items that fit the `isNewline`, the result of the `isNewline`. 
							
							Something like: `return isNewline(x)`
							It's a `StreamParser`
							Put under 'StreamParser/classes'
							Purpose: to use with `Regex("\r\n")` [isCRLF], and `isLF`
								More specifically, to be able to do: 
									'toLF = (x: IStream<string>) => return isNewline(x) ? "\n" : x'
								MAKE this a proper util.
				
				IMPORTANT NOTE: 
					1. The `TokenStream` must have a candidate-elimination system. 
						More specifically, consider the following (failing) implementation strategy: 
							1. one uses `.peek()` [alone] to determine which token to accept next
							2. due to that, since `peek()` is *limited* by length of the `RotationBuffer`, 
								one has a limitation on size for tokens-'.value's
							3. consider good ol' /[a-zA-Z]/; This has arbitrary length. qed
					2. Thus, one needs a combination of `.peek()` and `.next() + .prev()`. 
						Thus, we do: 
							1. peek(n) for MULTIPLE of them
							2. keep going, until one of: 
								1. the set of those that can be acceptable is 1
									1. switch to `.next()/.prev()` [.prev() ONLY if there's an error, and this (ends up) not being a match either]; 
								2. we run out of `n` for `.peek(n)`
									1. switch to `.next()/.prev()`
									2. `.prev()` ONLY when an error is encountered. 
						
						Thus, the candidate elimination system: 
							1. no backtracking
							2. max-possible-usage of `.peek()`
								once out-used, `.next(n)` for max `n` for `.peek(n)`, 
								and then continue with `.next(1)`
							3. if no matches are found, backtrack to the beginning using `.prev(k)`, with total chars `k`, 
								and herald error
							4. else, continue until there's only 1 viable candidate
							5. IF there are 2+ viable candidates, and one is fully reachable sooner than others, then IT is chosen
							6. IF no single viable candidate can be produced as per rules 1-5. (id est, there is 2+), 
								throw an exception [this is a PARSER error, not the input error, end-user shouldn'e need to worry about it, 
									the parser-maker (direct library user) DOES
								]
				
				IMPORTANT NOTE: 
					1. `Regex` works with BOTH: 
						1. string
						2. IStream<string>: 
							reason - to allow implementations for faster/lazy algorithms

				TO2. Remove (replace):
					REASON FOR V0.3: 
						1. PatternTokenizer/PatternValidator is TOO SLOW IN PRACTICE ALGORITHMICALLY (not good - multiple passes instead of a single one)	
						2. They are unbearable. Seriously. Everything else in the library is SHINY. But one *is* missing that bloody RegExp engine...

					1. PatternTokenizer
					2. PatternValidator

					[NOTE: important - only these implementations/algorithms! Keep the interfaces, they're quite good...]
					3. ValidatablePattern
					4. TokenizablePattern

					With a NEW approach - a: 
						1. 'RegExpTokenizer'
						2. 'RegExpValidator'
						3. 'TokenStream' - output from 'RegExpTokenizer' [or, alternatively - an interface for the 'StreamParser']; 

						IMPORTANT NOTE: THESE *DO NOT* use a RegExpMap, instead a `Pairs<RegExp, ParserFunction (or whatever)>` is passed directly to them...
					
					These are function-creation functions that is based off *THE SAME SIGNATURE* as 
						the 'RegExpMap', with the difference that these get to be used with 'StreamParser/LocatorStream/PositionalValidator/...'; 
					
					The RegExpTokenizer is A FULL-FLEDGED RegExp ENGINE. 
					ALSO: 
						1. 'Regex' constructor for making the Regular Expression IS THE ENGINE: 
							Methods: 
								1. .matchAt(x: Indexed<string>, i: number)
									Checks that AT THE GIVEN `i` IN `x`, 
										there is AN IMMIDIATE MATCH. 
									This is important, as it allows the "tabular" 
										work that the RegexTokenizer and `RegexValidator` do...
								2. .sub(from: number, to: number) - returns a new Regex that is the result of LIMITING the current one
									between `from` and `to` indexes for VALID TOKENS. 

									This way, for instance: 
										(a(bc)+) has 4 tokens, ordered as: 
											1. (...) - outer bracket
											2. a 
											3. (...) - inner bracket
											4. + [relative to the outer bracket]
											5. b
											6. c

									Thus, recursion is handled BY THE FIRST OCCURENCE. 

								ALSO, add the JavaScript string-compatibility methods: 
									1. [Symbol.match]
									2. [Symbol.matchAll]
									3. [Symbol.replace]
									4. [Symbol.search]
									5. [Symbol.split]

								ALSO [imporant]: have ALGEBRAIC methods for it. 
									Meaning: ability to join them via a disjunction '|', 
										applying the quantifiers '*', '+', '{a, b}', '?', 
										applying negation '^'. 
									Each of those CREATES A NEW 'Regex'! 
									Purpose is to (basically) replace the `regex` module. 
								
								ALSO: the `Regex` implements an NFA
						2. `RegexTokenizer` is BASED OFF IT
					NOT based upon JS's one. 
					It: 
						1. Takes the signature that is MULTIPLE expressions
						2. Combines it into a `Stream`, that produces tokens according to the set rules: 
							1. consists of a PARSER for the original regular expression string 
							2. consists of a COMPILER for the regular expression AST (obtained from parsing)
							3. consists of methods: 
								1. amongst which there is '.match', from which 'Stream' is produced by doing something like: 
									new RegExpEngine(GENERICS)("REGEXP_STRING").match("TARGET_STRING")
						3. When tokens CANNOT be produced, STRINGS are produced instead
							Input (.value - it's a 'Pattern') *IS* a string...
					
					Reasons: 
						1. PatternTokenizer (AND TokenizablePattern), PatternValidator (AND ValidatablePattern) use THE SAME code
						2. PatternTokenizer, PatternValidator are (both) FAR too generic
							Used only once. 
							It's a waste of generality. 

					NOTE: the 'RegExpMap' REMAINS: 
						Reason: it can still be useful to someone (as a boilerplate saver...)

					ALSO: about `RegExpTokenizer`: 
						[maybe] - create your own flavour for 'RegExp's
						Then: 
							1. rewrite the `regex` module to INSTEAD create YOUR flavour [more modular]
								1. BUT - *do not* delete the current stuff
									Instead, add a submodule FOR YOUR flavour, and for the "builtin" JS flavour
									Together add a "convert" submodule to switch between them
							2. create a submodule for it to CONVERT between the two [JS regex <-> our regex]; 
							3. write proper docs for it
								3.1. [possibly] make a separate module; This could be too huge conceptually for the library's ontology

						It will improve upon the "simple" JS regex in the following ways: 
							0. replace the first and last items "/^$/" with /:#/
							1. add a general `^x` for NEGATING a pattern. Thus, elementary [^...] becomes composite ^[...]
								This way, for instance /^ab+/ matches "bbb", but not "ab". 
							2. add a general `(){n, k}` - limits input from 'n' symbols to 'k' inclusively
								to NOT grab it, use '(:...){n, k}' [basically, the same as the *ordinary* '{n, k}']; 
							3. remove the `\B`, `\D`, `\S`, (?!...), (?<!...), ..., other negative-classes/assertions in favour of the simpler `^x` form
							4. replace the (?:...) with (:...) [syntax simplification]
							5. provide different '\\x' characters (obviously, due to changed syntax); make the escaping redundant for the rest
							6. provide A SINGLE '\u{}' instruction instead of that horse**** with \X, \u and \c that JS has
								Will span the max possible values of the Unicode range
							7. get rid of the flags: 
								1. 'g': 
									replace the different behaviour with METHODS; make it non-exclusive

								2. 'v':
									remove, provide its functionality by default: 
										1. `P` not working - it is removed and replaced by `^p{...}`:
											By default, consumes the max possible string (remaining, if needed). 
											Limited by (...){n, k}

								3. 'i': 
									make it a new group type - that matches A GROUP regardless of the case: 
										Ideas for syntax: 
											
											1. /(#i...)/; 
											2. /(?i...)/; # more traditional...
									
									ALSO: make it possible to *inject* custom functionality 
										(utilizing the fact that `RegExpTokenizer` CAN be generic); 

								4. 's': 
									make it default
								
								5. 'u': 
									remove, included by default with `v`

								6. 'y' (????): 
									find out what this even is (and what it's used for)
									seems very much like a hack for 'regex.match/exec/search/whatever(string.slice(x))'...
							
								7. 'm': 
									remove - unnecessary
									possible to achieve the same thing using a slight modification of:
										/(?<\n)...(?=\n)/
								
								8. 'd': 
									too specific, ugly
									make these kinds of behaviours GENERIC instead
									[note: the 'RegExpTokenizer' WILL be generic...
							8. get rid of form-feed (`\f`): 
								Reason: not useful for parsing
							9. ALLOW the space in '{a,b}' (becomes '{a, b}'); 
							10. Add a possesive quantifier "*+", "?+", "++": 
								https://www.regular-expressions.info/possessive.html
							11. Add atomic groups: 
								https://www.regular-expressions.info/atomic.html
							12. Double negation inside a class: 
								^[^\w] is the same as (in JS flavour) [^\W]

								That is, it is COMLEMENT OF A CLASS of items that are NOT A CHARACTER.
								That is, it's \w. 
							13. Add relative backreferences: 
								https://www.regular-expressions.info/backrefrel.html
							14. Fix the ECMAScript behaviour of empty-matching backreferences: 
								https://www.regular-expressions.info/backref2.html
							15. Add the conditionals: 
								https://www.regular-expressions.info/conditional.html
							16. Regarding the /(#)/ - modes: 
								https://www.regular-expressions.info/modifiers.html
							17. Add subroutines (recursion): 
								https://www.regular-expressions.info/subroutine.html

						IDEA: use this regex-flavour for the HammockLang implementation later...

					ALSO - retain the `TokenizablePattern` and `ValidatablePattern` interfaces, 
						KEEP (and use) THEM for your `RegExpTokenizer`. 
						
						They are very good at (in general) at expressing the idea of a 
							"carrier object" and a its "handler function" for tokenization/validation information 
								(here - regular expressions). 

						Don't use them as-is, and INSTEAD of plain JS RegExp-s, use your flavour (+ the "iterable tables" for handles instead individual [string, handler] pairs,
							so the interfaces will still have to be changed somewhat...	
						), BUT *DO NOT* throw them out...

							IDEA: for usage with `RegExpTokenizer`, create an 'IterableMap' function [inside 'Parser/TableMap']: 
								This is a function that:
									1. takes in a LinearIndexMap [or other `IndexMap`]
									2. produces a function that: 
										0. assigns the map to its own `.table` property
										1. iterates the given map, CALLING each function-value inside of it with arguments: 
											0. 'this' [yes, it retains the `this` value...]
											1. the original value `x`
											2. the function itself (for access to `.table` property)
											n. all the other (spread) arguments
										2. returns the result of the last one

				EL1. Eliminable - get rid of it: 
					More specifically, create a `RegExpEliminator` interface, which would replace it: 

					1. Based off the same RegExp engine as `RegExpTokenizer`/`RegExpValidator`
					2. More efficient [algorithmically], as it requires a single pass over the string, instead of 
						several with multiple `.split()` calls;
					3. Uses *the same* table-interfaces to represent multiple "ordered" removals as before

					ALSO: 
						1. remove the `EliminableCounter`
						2. remove the `Eliminable` module (as well as Validatable and Tokenizable); 
						3. remove the `Eliminable` interfaces
			
				NM1. [DEFINITELY!] Create a 'samples' directory; 
					Reason for v0.3: 
						1. 'FlagHash = BitHash + ArrayInternalHash'; 
						2. TOO many prerequisites
						3. the "not-so-util-like" utilities

					This is for 'common-case' parsing - generally, walk through the various syntaxes that were 
						PRESENT within the previous parsers, use them?
					
					Or, better, make this into a separate mini-project called 'parsing-samples'; 
					[EXAMPLE: handling the 'escaped' strings/sequences all the time];
	
					Think about it [this particular refactoring is VERY fine and case-specific, though, it can come in useful...]; 
	
					No, if one is to do it, the thing should: 
	
						1. Work with general parsing patterns, that would be CONFIGURABLE for every case; 
						2. Provide low-level Token-s with specialized given names; 
	
					1. Tokens: 
	
						1. Take them from various parsing projects of yours already existing: 
	
							1. xml
							2. selector
							3. regex
	
							Unite, choose best string-names; 
							Create appropriate names for sample-token classes (via TokenInstance) [
								example: 
								
								Opbrack - (, 
								Clbrack - ), 
								OpSqBrack - [, 
								ClSqBrack - ], 
								OpBrace - {, 
								ClBrace - }, 
	
							...]; 
	
					2. arrays with common items (USED throughout...): 
	
						'binary = [0, 1]'
						'decimal = [0, 1, ..., 9]'
						'hex = [0, 1, ..., F]' - TAKE OUT of the global '/utils.ts'
						'ascii = [0, 1, ...]'
						'alphanumeric = [0, 1, ..., 9, a, ..., Z]'
	
						Also - the sets in question should be provided AS-ARE (without the 'inSet'...); 
					
					3. ALSO - join the 'samples' with the 'constants.ts' file 
						[they are pretty similar in that they BOTH provide frequently recurring ambigious constant entities with meaningful "static" names and uses]; 
					
					4. MOST IMPORTANT - generation functions, stuff for working with AST-s: 
						1. Based off ASTAnalyzer, ASTStream-s, ASTHierarchy, etc... Generally, the 'Tree.AST' module
					
					5. Stuff for Stream-transforming: 
						Namely, nested-stuff via the 'StreamParser' [WITH '.buffer']: 
	
							// A VERY common pattern...
							// Generalize to an arbitrary 'X <- LimitedStream() [here]', with 'StreamParser.init(X())'
							const l = new LimitedStream()()
							const t = new StreamParser()()
							const parse = (input) => {
								l.init(input)
								t.init(l)
								t.finish()
								return t.buffer.move()
							}
	
					6. module-related stuff IS GIVEN ITS OWN MODULE 
						Example: DelimitedStream goes into the `samples.PredicateStream.*` module
				
				UT4. REMOVE the `utilities`: 
					Put them ALL into `samples`. 
					THE ONLY 'utils' to keep are: 

						1. Parser/utils.ts
						2. IndexMap/utils.ts (without the `table`)
						3. Stream/utils.ts (only the 'byStreamBufferPos' and 'isEmpty', namely)
						4. src/utils.ts (only the `dig`)
						5. Position/utils.ts (everything, except for `is` functions)
						6. Node/utils.ts
						7. DynamicParser/utils.ts
						[pick which others to keep...]

					Those - MERGE into a single export-file called `utils.ts` (new module -- `utils`); 
					Split them in a (similar) fashion to how it was before, if you want to, except: 
						1. `dig` - this one's standalone now
						2. `Parser/utils.ts` - this combines two types (AT LEAST), hence, it cannot be well-categorized

					The `is` functions are ALL kept inside the special new `types` module.

				PA3. A new class - `LazyTree` [don't be deceived - it's a 'Stream']: 
					DESIGN: 
						1. A Bottom-Up parser
						2. Has fields: 
							1. .top [protected]: currently used `LazyNode`
								It gets to be replaced by the topmost `LazyNode`.
								`LazyNode` is a node which, itself, is a `Stream` of nodes. 
								Types of `LazyNodes` gets assigned to the current `.top` `LazyNode` instance 
									via "choosers" (pre-defined in `.tree` on construction). 
								
								The moment that one picks a new `top` is the `reduce` operation. 
								The `shift` operation is done relative to `low` via (either) 
									one of the current descendants of `.top` (the lowest of which is `.low`), 
										or the `.low` itself [when one needs to move a higher-level `LazyNode`]

								A `LazyNode` contains: 	
									1. `children` - a `Stream`, items in which are constructed using internal mechanism using the `.value` items. 
										Thus, a `.value` stream is transformed using an INTERNAL `Stream`, giving the publicly available lazy children. 
										The `children` are the items iterated in the `DFS` of the `LazyTree`. 

										COMMON PATTERNS available for construction of `LazyNode` [to add/present]: 

											1. `LimitedStream` - exists
											2. `FiniteStream` [or `ParentStream`] - ADD: 
												1. consists of a finite collection of sub-streams, 
													each of which is (Itself) gets to use parent's `.value`; 

											3. composition [arbitrary, relies on `value`]; 
											4. `StreamParser`
											5. ...etc [all the other current ones]

									2. `value` [protected] - internal, for obtaining new tokens in `children`
							2. .low [protected]: the initially used `IStream` with items of 
								some `Type`. Usually is a `TokenStream` in practical contexts; 
							3. .tree [protected readonly]: 
								This is a collection of "choosers" for each possible ocurring `type`
									within each of the `.value` streams. 

								Workflow: 

									1. Start with `.low.curr` - first item
									2. Map it to respective `LazyNode` type, of which an instance is created 
										1. Done using `.type` and the "choosers" [possibly - as an abstraction over `TableMap`]
									3. Make the instance new `.top`, assign previous `.top` to its `.value`
									4. Repeat the process 2.,3. for current `.top`, until no more reductions can be made [Reduce]
									5. DFS phase: 
										1. return .top [assigned as first '.curr' of `LazyTree`]
										2. return .top.children.curr [assigned as second `.curr` of `LazyTree`]
										3. return .top.children.curr.children.curr
										...
										n. [bottom] return low.curr
									6. 'low.next()'
									7. DFS: 
										1. return `low.curr` [second]
										...
										n. return `low.curr` [`low.isCurrEnd()`]
									8. Find next `.value` in the chain that is NOT `.isEnd`
									9. Call `.next()` on it
									10. Repeat, until all are exhausted [up until the topmost `LazyNode`]: 
										1. RULE [important]: topmost `LazyNode` MUST NOT be exhaustible, that is: 
											if it is EXHAUSTED, then one is finished. 

											That is, one must know the `LazyTree`'s ULTIMATE DEPTH from the very beginning. 
												This imposes some restrictions on how it can be structured. 

											IMPORTANT NOTE: 
												when one has various LOOKAHEAD scenarios, such as: 

													x = tz
													z = y | yz
													
												[requires infinite lookahead]

												One gets into a bit of a pickle. 
												Then, it is NECESSARY TO ENSURE parsing of THE WHOLE 
													sequence of the infinite lookahead. 
												Particularly, it is A LOT more complex to build the 
													`LazyTree` parser than it is a more conventional 
														persistent `INode`-based one. 
										2. Hence, one does `.top.next()`. 
										3. REPEAT for new `.top.curr`: 
											obtained the same way, BUILT from `low`, UPWARDS, 
												with a GUARANTEED fixed `.top` [established in the *BEGINNING*]. 

						3. Behaviour: 
							1. '.top'

					Consider this - one is given a task of transpiling a VERY large file, 
						and then putting it into another file. 
					Now, it's possible for one to READ IT (using `LazyBuffer`), and to lazily PARSE IT
						(using `Stream`s), however, the result of the parsing operation is STILL persistent. 
						And, likewise, the serialization process is ALSO persistent. 

					Thus, one wants to create a class equivalent to a Stream-transform operation. 
						Instead of evaluating a sequence of characters, and producing an in-memory AST, 
							one creates nodes ONE-BY-ONE in a Tree-like fashion. 

					Now, since this is a Recursive Descent parser, we want it to be a Depth-First-iteration 
						algorithm (the `Breadth-First` does not allow for this unfortunately...). Thus, 
							we are producing A NEW `Stream`, which provides us with tokens THE SAME WAY 
								that the `TreeStream` would. 

					The memory savings from this thing (on a scale) would be insane 
						[the program would use almost no memory at all - everything gets garbage-collected]. 
					
					It is a `StreamParser` [put under `StreamParser/classes`]. 
					Note also that it is RECURSIVELY LAZY, that is, it ONLY JUST holds the 
						amount of space needed FOR THE CURRENT ITEM. 

					Dev notes: 
						1. one will want to keep `LazyNode`s for this [lazy equivalent of `RecursiveNode`, since `ContentNode` and `TokenNode` have no children...]; 
						2. `LazyNode`s will have their children AS AN `IStream` 

					Thus, the complete workflow is: 

						WriterStream((StreamParser(serializerFunction)(LazyTree(...StreamParser(InputStream(LazyBuffer(...)))))))
					
					For `WriterStream`, see the note below...

				PA4. A new class - `WriterStream` [a new `Stream`]: 
					The reverse of a `LazyBuffer`, it WRITES its elements one-by-one into 
						the given file/other resource. 

					Can be chained, RETURNS ITEMS AS-IS (that is, it STILL has a return value, 
						even though it's almost totally impure). 

					[Maybe?] provide a wrapper around the basic Node API 'Stream' used here? 
						Reasons: 
							1. provide error-handling
							2. deal with configuration nonsense in a modular manner (.setEncoding, etc)
							3. platform-independent ontology (Deno + Bun compatibility)

			Secondary: 		
				ME3. [StreamClass] Iterator Helper functions - '.filter', '.take', ...: 
					Provide them. 
					Use them in the library definitions that actually EMPLOY them [example: PositionalValidator would benefit greatly from a single '.filter()' call]; 

					These will (each) require individual internal-class implementations + public interfaces to use.
						Reason for "internal" nature of these classes: one doesn't want to allow user to create them on their own
					
				RE1. 'regex' - allow for methods for creation of regular expressions that use the extended regex-syntax (enabled by the 'v' flag); 
					Namely: 
						1. intersection: 	/[[...]&&[...]]/v
						2. subtraction: 	/[[...]--[...]]/v
						3. string literals: /[q{...}]/v
				
			Minor: 	
				DE1. Increase the Library's natural debuggability: 
					ADD names to all the nameless functions. 
					This is due to the fact that in error logs, they would appear A LOT more clearly if one did do it. 	

				SI11. The `.prod()` method - repurpose it. 
					Replace it with a simple, general `.peek(1)` [PeekableStream]
					Thus, it will allow a better unification of the library's concepts under the 
						same terminology. 
						
				TY2. Type simplification - the `array.Pairs` overuse: 
					Rely more upon the `Iterable<[KeyType, ValueType]>`, where possible. 
					THIS INCLUDES: 
						1. IndexMap[.*]
						2. utils.IndexMap[.*]

					Reasons: 
						1. more generic code

					Also - IF one does need to use the `Pairs` IN THE END (due to mutability concerns), 
						USE THAT 
				
	5.2.73.
	5.2.74.	
	5.2.76.	
	5.2.77.

	[fix test compilation errors]
	[run the tests]

	B. 
		
	C.

	IMPORTANT: clean the git history - keep only the commits for each individual version; 
		Squash all the others + change the respective commit messages; 
		[do this via `git rebase` + `git push --force`]
	
B. documentation (wiki - change/fix it...);
	Hosted on the project's GitHub Wiki. 
	WHEN WRITING DOCUMENTATION, 
		
		0. make a note that the '.hash' and '.extension' functions SHOULD BE ABLE TO HANDLE null-values!	
		1. Note in the 'UnfreezableArray' docs that AFTER '.unfreeze()' is called, 
			the underlying Stream behaviour *DOES NOT CHANGE* (meaning - one has to create a NEW Stream...); 

		2. SPECIFY: that (at the moment) parser's best support is for TOP-DOWN RECURSIVE DESCENT PARSERS! 
		
		3.	Ways to do self-modification for DynamicParser [!!! - the Holy Grail...]: 				
			1. 'this.state.parser.layers = ..." [self-modification, GLOBAL]
				1. Also: 'this.state.parser.layers[i] = ...'; 
			2. 'parser.table = ...' [self-modification, LOCAL]

			TODO[1]: add examples for using it to the docs with 'StreamParser' + 'InputStream' [by far - *the* most common case...]; 
			TODO[2]: use the code for doing so WITHIN THE 'demo' integration test...

		4. ABOUT the '.typesTable' in `NestedStream`: 
			IT HAS A CONTRACT, of returning `null/undefined` instead of a valid index in `.getIndex`, IF a given item in the present NestedStream is NOT 
				supposed to be granted an index (and, thus, be made into a SEPARATE NestedStream), 
					THEN THE RETURNED INDEX is to be `null` or `undefined`. 

	EXAMPLES: 
		1. Currently, this is the code to take a certain `input` Stream, 
			then - take its portion and transform it: 

				// TODO: add this code as example for THE DOCS later...! 
				parserStream = new StreamParser(...)()
				limStream = new LimitedStream(...)()
				convertPortion = (input) => {
					limStream.init(input)
					parserStream.init(limStream)
					parserStream.finish()
					return parserStream.buffer.get()
				}

		2. Faster/more-elegant version of 1.: 
		
			parser = transform(...)
			limStream = new LimitedStream(...)()
			convertPortion = (input) => {	
				return parser(limStream.init(input), new CollectionClass()).get()
			}
	
C. write the CHANGELOG [too much has been altered since the "original" v0.3]; 
	C.1. Walk through the last commit's state of v0.2.1, noting changes and writing them down [destructive]; 
	C.2. Walk through the v0.3 changes, and write them down [constructive];

[v0.4]

TODO: 

	0. General (see these before anything else): 
		DE1. Development order: 
			1. Start from `Primary` and `Secondary` todo-s
			2. Finish (or try to, at least) the `Secondary` problems quickly (1-3 days)
			3. Try not to add excessive amounts of new stuff to the library: 
				1. it is already quite powerful/loaded
				2. more things will just make it more obscure

			4. [IMPORTANT] Do TDD: 
				previous versino of the library was a HELL to write, for the following reasons:
					1. originally, there was a fairly moderate list of requirements
					2. it was quickly impleemented (~ 2 weeks)
					3. then, one foudn out that it was a BAD idea to be using arrays for everything 
					4. decided to implement lazy Streams
					5. goodbye, 8 months...: 
						1. more new classes
						2. changing utils/existing classes
						3. adding new methods/functionality (constantly)
						4. ...

					Key issues were: 
						1. lack of proper list of requirements (no vision for the library)
						2. lack of good tests (development was NOT test-driven, NO intermediate testing between changes/additions)

					This, way - DO TDD: 
						1. pick an item
						2. add skeleton code [passes TypeScript compiler only]
						3. implement (many) tests
						4. fail them
						5. write/fix (a comprehensive) implementation
						6. rinse and repeat, until it works desireably, and no tests (seem) to be missing

					Accompany all this with intermediate notes (as sometimes important information for later, that CANNOT, 
						go into tests may be forgotten otherwise)

			5. [IMPORTANT] Expand JSDoc: 
				1. use it more thoroughly (give higher priority for classes and so forth - not just a brief description + Wiki link)
				2. expand its contents (argument types, @type, self-reference, etc)
				3. [IMPORTANT] document early - another one of the banes of parsers.js: 
					Due to the fact that development was (mostly) content- and feature- oriented, 
						one very much forogt about importance of spending appropriate amount of 
							time on documentation. Hence, one (slowly) started to forget what one 
								was doing, and whether it even (sometimes) made any sense. 
					Had there been documentation, certain specific poor choices may have been more avertable, and 
						some little time could have been saved. 

		FI1. Eradicate small interface files: 
			1. Let the *small* interfaces BE LOCATED inside the `class.ts` files (as with `abstract` classes); 
				Reason: fewer pointless microscopic files with < 20 loc
					We have long abandoned the "total-modularity" philosphy on this project, so why not this too...

	1. Primary [new ideas, modules, interfaces/classes, algorithms]	
		TS1. TreeStream: create a new versions; 

			The 'TreeStream' (currently) is the 'LL' pre-order tree traversal algorithm Stream-implementation; 
			There should be ANOTHER - the 'LR' post-order tree traversal; 

			1. Rename the 'TreeStream' to 'TreeStreamPre', and create an implemnetation for the 'TreeStreamPost'; 
				This is (particularly) useful for some types of interpreters; 

				The Pre visits the node, then its children one-by-one starting from the beginning, then the siblings; 
				The Post visits first the children, then the siblings, only THEN the parent [when there are NO MORE children - then the parent is visited]; 

				Basically, Pre (parent-to-children) returns the nodes 'as they go' in the left-to-right traversal in the tree,
					whereas Post returns them in 'reversed' order: for the parent to return, children must all return first; 

				[Example usage: transform the binary '+' -- a + (b + c * (d + (k + f * r))) -> a + b + c * (d + k + f * r)]; 
			
		TS2. Implement a 'level-by-level TreeStream' ('TreeStreamLevel'): 
			It would get an initial given 'level' (tree root), walk through all of its '.children', 
				then walk each of the '.children' themselves as levels (one after another...), THEN descending lower...; 
			Would do it via calling '.getJointLayer' function (then - caching for later '.prev'); 
			This (basically) takes the nodes from the previous 'joint layer', gets their respective children,
				CONCATENATES THEM into a new layer, then iterates it; 
				
		IM1. Create a new 'IndexMap' implementation: 'SetHash'; 
			This would be a: 
				1. HashClass interface implementation; 
				2. generalized wrapper around a Set (in a way that HashMap is a wrapper around Object and Map); 
					2.1. Would have its own 'extension'; 
					2.2. For 'values' would contain true/false; 

			This is useful for working with cases that require the 'or'-predicate; 
			Example: 'nested' utility; 
			How it is done [pseudo-code]: 

				const SpecialCaseSetMap = SetHash(...)
				const processNested = (x) => nested(TableMap(new SpecialCaseSetMap([...])), TableMap(new SpecialCaseSetMap([...])))(new WhateverStream(x))
								
		IM5. [IndexMap/classes.ts -> *] MultiMap - [IndexMap-like] this returns ALL the matched items, and while running multiple results 
			Can contain functions (called to obtain a value), or values themselves.
			Reason: 
				1. greater modularity
				2. modular representation of constructions like: 
					if (A(x)) B(x)
					if (C(x)) D(x)
					if (R(x)) return M(x)
					return K(x)
			
			More specifically, this is a MUCH MORE DEEPLY CONFIGURED IndexMap, one 
				that generalizes the linear search on the `LinearIndexMap`, and can persist 
					FOR LONGER [hence, needed NOT ONLY for looking for the first item]. 

			Namely, it can be configured to take MULTIPLE SEARCH PATHS, that can include: 
				1. moving to the next/previous item/call
				2. performing a call/returning item
				3. performing a check over the next key
				4. quitting, while returning the last stored value
				5. storing the item returned from the last call (includes non-function items)
			
			NOTE: the *FUNCTION* used to determine the next step CAN BE CONFIGURED FROM INSIDE THE CALLED FUNCTIONS THEMSELVES. 
				Thus, allowing for another achieved case of local self-modifiability. 
			
			One can see that this is a full-blown DFA. 
			Also - the user can (optionally) store the '.state' property on these things 
				(they come "pre-added", as the ignored empty object doesn't take any space at all). 
	
		TC1. Create an 'ASTAnalyzer' class [under new 'Node' module]: 
			1. Takes in an `INode`
			2. Traverses the tree
			3. Returns information: 
				1. [Buckets-Categorization] Arrays of '.trivial', '.content' and '.recursive' nodes
				2. [Type-Categorization] Returns a 'HashMap' of arrays of nodes, filtered by their '.type'; 
					NOTE: the 'HashMap' is, TOO, completely configurable by the user [user-defined (if provided), else - default being: when 'string's are used for 'type's - ObjectHashInternal, otherwise - HashMapInternal]; 
						(another function-argument for returning the final ASTAnalyzer class)
	
			4. Provides functionality [ASTAnalyzer]: 
				1. mapTypes((type_name: string) => string): 
					creates a new 'ASTNode', by means of mapping the '.type'-s
						of the current one using the given function. 
					Immensely powerful for changing between different data formats. 
	
				2. mapValues((x: any) => any)
					creates a new 'ASTNode', by means of mapping the '.value's of each 'ContentASTNode', 
						while preserving all else
				
				3. .find(type: ..., pred: (x) => boolean): 
					Seeks an item in the tree with a type '.type', obeying the predicate '.pred';
					Optimizes to look specifically in a '.type'-bucket of the given 'type'. 
					Can be radically faster for large inputs. 
				
				4. .find(pred: (x) => boolean): 
					if the 'type' is not passed, the entire tree is searched; 
					Optimization - uses prior obtained collections for increasing cache locality of large searches. 
				
				5. .iterate(type) - returns a Stream, filled specifically with values of type 'type'; 
					Namely, it returns an 'InputStream', wrapped around the respective '.type'-bucket; 
					The items are listed on a from-beginning-to-end of the 'Stream' used to construct the ASTAnalyzer; 
	
				6. .filter(pred) - returns a new tree, such that it ONLY contains elements that obey the given predicate: 
					HOWEVER, one can also mandate that in order for a sub-tree (RecursiveASTNode) to remain, 
						at least one of its children must be 'true' as well. 
						This is done by means of returning `null`, instead of `true`/`false`.  
				
				7. .search(type: any) - searches, and returns for multi-index information for each of the elemenets of a given type. 
					Optimization: uses ONLY JUST the information from a given '.type'-bucket; 
					This allows one to: 
						1. Iterate a portion of the tree INSTEAD of needing to start "at the beginning
						2. Know exactly when to stop (seeing the *last* item that needs to be iterated over)
	
				8. .map((x: ContentASTNode | TrivialASTNode) => any): 
					Maps all the non-RecursiveASTNode parts of the tree to a different 'Tree'. 
					Useful for creation of generalized non-AST trees, to be used with 'TreeStream': 
						Example, passing a function that returns 'string', then - CONCATENATING all 
							the items inside the obtained Tree, via: 
	
								// [sketch - no types]
								function GenerationFunction(F) {	
									return function generate(ast) {	
										return array(TreeStreamPre(ast.map(F)), new UnfreezableString()).get()
									}
								}
								
								// this is to create either a SUBSET defined by a recursive IndexMap-like function, *or* via working through a '[type]'-defined set of "superior" nodes; 
								// NOTE: 'ast.types[type]' is an *ARRAY*
								function GenerationRecursive(F, type) {	
									return function generate(ast) {	
										return array(InputStream(ast.types.index(type).map(F)), new UnfreezableString()).get()
									}
								}
	
						IDEA: ADD a set of 'samples' to the library - see above...
	
					Also - '.map(F)' OPTIMIZES, so, it expects the given values to be returning FUNCTIONS, for plugging in their children's 'F(x)', 
						so: 
	
							1. A -> B
							2. F(A) -> X(K): X(B) == string
							3. F(B) == string
								F(A)(B); 
							
					The optimization is - SPLITTING the '.types'-buckets via doing '.types[name].map(...)'; 
				
				9. RecursiveASTNode: 
					1. Contains various copying .value-Array-delegate methods: 
						1. .map(f) - creates a new RecursiveASTNode via 'new RecursiveASTNode(this.type, this.value.map(f))'
						2. .filter(f) - creates a new RecursiveASTNode via 'new RecursiveASTNode(this.type, this.value.filter(f))'
	
			5. Optimization information (additional): 
				1. IDEA: add a new 'ASTHierarchy' information object, which: 
					1. Specifies, what '.type'-s CAN be found within which '.type'-s of RecursiveASTNode-s: 
						IMMENSELY powerful for search-optimization inside a given Tree 
							[limits the '.type'-buckets in which one needs to look]; 
					2. Specifies maximum "depth" of a certain particular 'RecursiveASTNode': 
						1. Allows one to replace a continuous check of 'isGoodIndex(.lastChild)' with 'isGoodIndex(.lastChild) && i < MAX_DEPTH_COUNT'; 
							Guarantees a performance bound. 
							When the bound is PRECISE, the 'isGoodIndex' can be dropped
					3. Specifies maximum length for '.children' of a given 'RecursiveASTNode': very useful for iteration! 
						1. Replaces '.children.length' check with a 'counter < FIXED_PRECOMPUTED_LENGTH'; 
					4. Specifies expected structure for a given 'RecursiveASTNode': 
						1. May permit one to be going one-by-one downwards for it, EXPECTING the given '.type's; 
						2. Eliminates checks for '.isSiblingAfter'
	
					'ASTHierarchy' is optional, and its purpose is solely to: 
						1. provide algorithm optimizations
						2. enforce bounds for a given format
	
					NOTE: 'ASTHierarchy' is ONLY good for optimization, when IT IS PRECISE 
						[id est, we can SKIP certain function-calls/checks]. 
	
				NOTE: optimizations work via: 	
					1. ASTStream-s: 
						0. Alternative to 'TreeStream': less generic, provides better optimizations for AST: 
							1. ASTStreamPre - alt. of TreeStreamPre
							2. ASTStreamLevel - alt. of TreeStreamPre
							3. ASTStreamPost - alt. of TreeStreamPost
						1. Accepts an 'ASTNode' instead of a 'Tree'
						2. Optimizations: 
							1. No '.lastChild' presence check in '.isChild()' method - instead, just '.lastChild > -1'	
								This is a good optimization for TreeStream-iteration of large trees ('.isChild()' is called ON EVERY '.next()' call); 
							2. Algorithm Configurable via 'ASTHierarchy':
								1. Optimizations are present ONLY when ASTHierarchy is PRECISE
								2. Expects a VALID abstract syntax tree
	
				2. For '.type's field values, one uses EITHER an 'ObjectHashInternal' (if strings), or a 'MapHashInternal' (if not strings); 
		
		BU1. [Bottom-up Parsing]: 
			1. Add: 'Rule'
				Consists of: 
				
				1. .head: 
					Describes function to "wrap" the matched item with. 
					One of: 
						1. 'TokenType'/'ContentASTNode'-like - uses a single item for '.value'
						2. 'RecursiveASTNode'/'TokenType'-like - uses a multitude of items for '.value' [reads an array from '.body']: 
							NOTE: if a 'TokenType' is used then PRESENCE OF LAST ARRAY TAKES PRECEDENCE, that is - the body 
								*ending* with an array will cause a '.value: any[]', whereas one with a SINGLE value, will cause a 
						3. 'TokenInstance/TrivialASTNode' - has no '.value'
				2. .body: 
					This is a sequence of items: 
						1. 'TokenType'/'RecursiveASTNode'/'ContentASTNode'-like [interface]
						2. 'TokenInstance'/'TrivialASTNode'-like [interface]
						3. ('TokenInstance'/'TrivialASTNode'-like | 'TokenType'/'ContentASTNode'-like | Empty)[]
							This is to describe the ACCEPTABLE TYPES for a '.value: any[]'-TokenType or a 'RecursiveASTNode'. 
							It is basically equivalent to '(TypeA | TypeB | ...)+'

							WHEN the special object 'Empty' is used instead, ANYTHING (including nothing) is matched. 
								Id est, it becomes '(TypeA | TypeB | ...)*', NO ITEMS AT ALL is also acceptable. 

							The array of items that is given CONTINUES the array of items that is wrapped into the given '.head' of 'RecursiveASTNode'-like. 
							Namely, here are the PRECISE TYPES of Rules for each head: 

								1. RecursiveASTNode <- A B ... (or 'TokenType-like <- A B ...') - PRODUCES A '.value: any[]'
								2. TokenType <- a - PRODUCES A '.value: any (single value)'
								3. TokenInstance <- a - PRODUCES a 'TokenInstance' (no .value, matched 'a' is ignored)

							ANY OTHER COMBINATIONS ARE *NOT* suppored: will be ignored. 

						4. A special 'Matchable' interface, which would generalize the 'RegExp' with '.match': 
							This is considered to be the "end" for application of a given Rule. 
							IF the '.body' of a 'Rule' is NOT a 'Matchable', then a SUB-RULE is used for it, one that PRODUCES
								a respective '.type' of token (that is, has a '.head' with a respective type of token)
									is used. 
									
							Note, that OTHER kinds of '.type'-possessing classes CAN be used (user-defined EXTENSIONS of 'TokenInstance', for example...)
					
					The "matched" item is passed as an argument to '.head'

			2. 'Rule's are kept inside a `RuleTable`: 
				A `RuleTable` is the huge table used by the `TableParser`. 

			3. `UpParser` - a shift-reduce parser for the library:
				[properties]
				1. [protected] .stack - the stack of currently unparsed tokens
				2. [protected] .curr - the current item 
				3. [protected] .table - RuleTable, a '.prototype' property: 
					1. During class-creation with 'table', one constructs LOOKAHEAD-'Set's for each of the available Token-s
				4. [protected] .value - a 'Stream' to be bottom-parsed
				5. [public] .result - an 'AST'/'Tree' produced: 
					On the level of the interface - THIS IS A 'readonly' PROPERTY! 

				[methods, internal]
				5. .shift() - moves the Stream forward one position
					0. call '.curr = .value.next()'
					1. push 'this.curr' to the stack
				6. .reducePlain(x) - applies single-lengthed recursive rules to the given 'x'
				7. .reduce() - applies the rules from '.table' to the first rule in the parse-trees currently held in the '.stack'; 
					0. There is a '.currentRules: Rules[]' set of 'Rule's that keeps track of 'Rule's that are *possible* from this particular point. 
						To make it fast, there is a whole tree generated for obtaining these sets from specific items 
							AT THE CREATION STEP of the current 'StackParser' class using the '.table'; 
					1. This applies the currently "chosen" rules to the current item '.curr' in the '.stack'. 
						1. Each of the 'Rule's have their STATE, which consists of: 
							1. '.pos' - position [<= '.body.length']
							2. '.reset()' method (for doing '.pos = 0')
							3. '.inc()' method (for doing '++.pos')

							NOTE: that the rule is being matched IN REVERSE! 
								This way, you have: 
									'A B C .' [.pos = 0] -> 'A B . C' [consumed C, .inc()] -> 'A . B C' [consumed B, .inc()] -> '. A B C' [consumed A; match whole, reduce]

						2. Once '.pos = .body.length', the rule has been fully matched, and it is time that it was '.reduce()'-d 
							Note: the '.reduce()' AUTOMATICALLY finds all the '.reduce()'-d rules in the '.toReduce' .protected member
								WHICH contains the next item to be reduced

				[methods, external]
				7. .inject(value) - injects a new 'Stream' to parse
				8. .get() - NOTE: this is an 'InitializablePattern', with '.value', '.get()' and '.inject(value)'
				9. .parse() - method for actually calling the parser: 
					This implements a proper shift-reduce parser based off a RuleTable. 
					TODO: CHOOSE the algorithm for it. LR/LALR/SLR/other [OR - better, make it GENERIC (somehow), add the respective Prefixes - LRTableParser, LALRTableParser, ...]
				10. constructor(value?): 
					The 'StackParser' is a function-generated class in TERMS OF '.table' (which goes on the prototype). 
					The '.table'-lookups in the '.parse()' method are OPTIMIZED based off the '.table' function. 

		SM1. LATER: important problem - self-modifying parser INTEROP of different self-modifications. 
			Problem with multiple self-modifying parsers is that they rely on SHARED STATE. 
			Which is bad. PARTICULARLY, when code of two independent vendors may be doing the modifications. 

			Solution: 
				1. provide a COLLECTION TYPE [IdCollection] that:
					0. wraps around Array
					1. stores IndexAssignable items
					2. comes with a '.search(id)', by .assignedIndex: 
						1. Defined via a 'MapInternalHash <- HashMap' [FastLookupTable], which STORES values of '.assignedIndex', and returns items from the Array by it
				2. provide 'Stream's classes within the library with DEFAULT 'static .assignedIndex'
				3. for cases, when it's IMPOSSIBLE to have sane defaults (ex: two LimitedStream(), or PredicateStream(), or ...), 
					user can employ the already existing 'assignIndex' util; 

			Benefits of solution: 
				1. Generic
				2. [Relatively] Low-level - wraps around Array more or less directly
				3. Easy to configure for the user (just set the appropriate '.assignedIndex')
				4. '.search()' is fast [Map - based off FastLookupTable]

			Then, a self-modification for parser vendor could: 
				
				this.state.parser.layers.search(...).handler.table = ... // or something...

		TD1. [Top-Down Parsing]: 
			The library (currently) only has the interfaces for aiding in 
				creation of Recursive-Descent Parsers. 
			Create a `DownParser` - similar to the `UpParser` (the Bottom-Up Parser). 
			It would operate based off a RuleTable. 

			[important] NOTE: one will need TWO different types of `RuleTable`-s: 
				1. UpTable - Bottom Up Parsing
				2. DownTable - Top Down Parsing

				Those differ in their construction algorithms. 
				ALSO: important note - the respective tables, themselves, 
					MUST BE SELF-MODIFIABLE! 

		PA1. Add incremental parsing to the library [see Chevrotain for examples]

	2. Secondary [new utils, methods]
		UT1. bufferize(tree, size) - util [Tree/utils.ts]
			Given a 'Tree', returns an array of values of size `size`: 

				{
					childNumber: number,
					value: any
				}

			The `size` is a fixed number >= 0, 'childNumber' is the number of children, ahead of current node in the array.
			To be used for creating a persistent linear data structure for working with the given AST. 
			Good for cases, when the number of items is known, and is large (part of the the "big input" optimizations); 

			Would permit user to store a '.state.size: Pattern<number>' property (Pointer(number)) on the ParserState-s, 
				for counting the size of the input in terms of nodes, then re-use it with 'bufferize', 
					and use the tree for evaluation; 

			If the 'size' is unknown, implements a simple DFS (namely - conversion of a given 'Tree' into an 'Array')
			
		UP1. utility [Parser/utils.ts] - 'nestedInfo'; 
			Returns the object of information regarding the `nested()`-arrays based off the given `inflate`, `deflate`
				functions. 

			Structure of the object (type - NestedInfo): 

			1. .length: number - number of sub-elements (possibly nested themselves)
			2. .whichNested: number[] - array of indexes, of nested elements -- those, which the `inflate()-deflate()` pair detected
			3. .nested: NestedInfo[] - the info for EACH of the `.whichNested` sub-items

		UT2. util - enumerateTree [Tree/utils.ts]
			Given an array of "types" of nodes, it recursively converts their properties (given in the 'shapes' array), 
				to single-array-siblings form (the current ChildrenTree); 	

			This would permit a more "variable" set of trees (those that have properties with fixed names), 
				to be enumerated accordingly...; 

			Example [1]: 

				{
					type: any, 
					a: any
					b: any
					c: any
				}

			Becomes [1]: 

				{
					type: any, 
					value: [
						// ... a, b, c
						// ... or b, a, c
						// ... or any other order
					]
				}

			Example [2]: 

				{ type: any }; remains the same
			
			Example [3]: 

				{ type: any, namedProp: any }; becomes { type: any, value: any }; by changing the property name to 'value'

			For this, define a 'TreeKind'/'TreeShape' - with a list of properties to be used/transformed; 
			More specifically, it creates a tree of `Node`s, from a tree with a given shape. 
	
		IM3. [!!!] Add another 'hash' function for the 'HashMap'? 
			In particular, use the 'extension' for 'set', 'rekey', 'delete' (by-original-key), 
				and another hash (the 'indexHash') for 'index' [this one will be optional and default to 'extension']; 
			This is in parallel with how the 'IndexMap' has it.
			Motivating example: 

				1. Indexes are objects with a field 'T: keyof X'; 
				2. Indexed are the objects with a field 'R: keyof X'; 
				3. One wants to create a HashMap with 'N = HashClass(...)', such that '(new N(...))(T) == smth'; 
					Problem here is that one EXPECTS the same hash to work for both the 'index' and 'delete'. 
					In general, the 'HashClass' does NOT support the categorization (even though in the case of 'SimpleTokenType', it works); 
				
		UT3. Add more complex 'Node' utilities. Based off previous 'one.js' v0.3.1 [the removed methods], add: 

			1. deepSearch - searching inside the given Tree for a value with a given 'prop'
			2. depth - calculating the depth of the given Tree
			3. treeCount - counts the values of a given predicate for all items in the tree

			ALSO: Add new methods to 'RecursiveNode': 

				1. .reverse() - recursively reverses the given tree
							
		PO3. [static method] MultiIndex.fromPosition(stream: TreeStream, position: Position): MultiIndex
			This is a method for converting a 'position' from the given TreeStream to a 'MultiIndex'; 
			Does NOT '.rewind()'; 
					
		ME4. Add "move semantics" to 'Collection's: 
			1. '.move' method - for a 'MoveCollection extends Collection': 
				1. Sets the 'this.value = []', or to a similar "default"/"empty" value. 
				2. '.move()' returns 'this.value'
		
			This permits to achieve the 'PreallocArray' idea WITHOUT the need to copy/change the contents of THE SAME array. 
			CONCLUSION: YES, add the '.move' to SOME collections via 'ArrayMoveCollection' with '[]'; 
			
		CU1. Add a 'binarySearch' utility: 
			1. For IndexBuffer - as it keeps 'number's in an ordered fashion, there is no reason not to employ it...; 
			2. This is generic (meaning, it provides a map 'f', and then compares via 'f(a, b)', with default being '(a, b) => a < b'); 

		SI10. A new `Stream` class: 
			The `DashStream` - a generalization of `JumpStream`. 
			A `StreamParser`, provided with `f(stream): number`, it will call EITHER '.next()' or `.prev()` based off the number returned (positive - '.next(n)', negative - '.prev(-n)'). 
				In other words - it behaves *EXACTLY* like `skip()`

			Let it also permit returning predicates (hello, `skip()`)

		TC3. Add `fromJSON` [JSON deserialization method] onto `INode`-implementing classes; 
			Better still, make it a util - to obtain a given `tree` fully...; 
			It would have: 

			1. A proper class-managing system
			2. As a return value: 
				1. BasicHash(new MapInternal()) of classes [by their `type` - an `IndexMap`]: 
					1. `.copy()` of `typesAutocache.value`, the `Autocache` used 
						within this specific case [easier to do, since we are in need of knowing if something's been already defined or not...]
				2. A tree, composed of `INode`-implementing items

	3. Minor [fixes, optimizations, refactoring, types, naming]
		OE1. [maybe? benchmark first] v8-specific optimizations: 
			Having done some benchmarking on some typical Array methods, one arrived at a...
		
			CONCLUSION: 
				1. Handwritten methods are *often* __MUCH__ faster than the builtins ON LARGE PIECES OF DATA; 
					On the more common-place cases, however, they are inferior in time performance; 
				2. THEREFORE, one should do: 
					Re-organize the library (how?) to permit usage of the current methods on LARGE pieces of data; 
					Prior - do more benchmarking; 

					About re-organization: 
						1. Think - whether to introduce explicit low-level 'size-controls' [possibly costly, when run a lot]; 
							Although, it's likely to be optimized/negligible; 
						2. Or, whether to split the thing onto 2 chunks [where the 'size' is precomputed...]; 
							This is less flexible (and thus, error-prone), plus will be more difficult to re-structure; 
						
					Current vote is for 1.; 

					ALSO, about bounds - those should be CAREFULLY BENCHMARKED; 
			
			MORE GENERALLY - try to adapt the library for work on LARGE datasets; 
			It'll make it useful for working with big projects/pieces-of-text; 

			THINGS OF ISSUE TO THINK ABOUT IN PARTICULAR: 	

				1. What "LARGE" sizes are practical? [meaning - sources of what sizes can occur in the wild?]
					Take them as large as one can. At least 10000000 symbols (~30MB, ~100000 lines of 90-100 chars), maybe try more; 
				2. How much of a performance difference can one get by doing these optimizations?
					In particular - try to sketch out a parser for something (the smtf format of one's own? it's simple to parse), perform 
						profiling on a REALLY big auto-generated file, then: 
							
							1. take overall time measurements (performance.now()); 
							2. profile, break the execution down on several tiny pieces; Then - benchmark and optimize them accordingly; 
								For this, use results from samples from one's benchmarks library;  
				3. Optimize for smaller sizes ONLY when it doesn't impair the memory severely; 
					When it impairs it at all, choose whether or not to do it based off the chosen memory/speed difference measurements...; 
					
					3.1. FOR THIS [maybe] - have specialized "Heuristics" constant-space in 'constants.ts'; 
						3.1.1. IF doing them - create the heuristics for DIFFERENT ENGINES; 
						3.1.2. IF oding them - make the 'Heuristics' only the "default value" for transition of sizes; 
							THE USER must be able to set their own boundries; 
						3.1.3. IF DOING THEM - check how to "bounrdies" for the increase in efficiency when using 
							the custom implementation "shifted" as the versions of Node progressed - TRY IT with different node, v8 
								versions; 
						3.1.4. IF DOING THEM - other engines to test: 
							3.1.4.0. V8 (Chrome)
							3.1.4.1. SpiderMonkey (Firefox)

						[maybe?] Try building the engines from nil, running the benchmarks for heuristics standalone...; 
			
		TS3. TreeStream: flag optimizations; 
			Allow 'TreeStream' to be a Stream-class-generation function also; 
			Permit the TreeStream to have overloads for: 	

				1. navigate (work with number-indexes, when '.buffer' is present); 
				2. next		(same thing - '.buffer' + '.pos')
				3. prev		(same thing - '.buffer' + '.pos')
				... [and so forth]
			
			Will allow to have the benefits of flag-presence via the overloads; 
			
		IM4. [maybe?] Do some amazing TypeScript jiggery-pokery:
			Sometimes, the type can be more complex to predict [but STILL might be possible via templates, type-inferences and special functions like 'Parameters'...]: 

				[LinearIndexMap/methods.ts]
				1. extend
				2. extendKey
						
		SA1. Relocate the `DelimitedStream` to `samples.PredicateStream` [YES, THE `samples` SHOULD HAVE THEIR OWN SUBDIVISION AS WELL!]; 
		
		VA1. validation-related mini-predicates: 
			At least: 
				1. 'input.next() == X'
				2. 'set.has(input.next())'

			Provide as 'trivialCompose'-compositions of elementary methods...
	
		SI14. The `.pos` on `StreamClass` should ALWAYS be added, whenever the user also adds the `.buffer`. 
			Reason - better/simpler implementation, minor memory effect. 
				No reason not to. 

		SI15. StreamParser: `init` signature - fix; 
			The redundant second `buffer` argument is STILL needed, even if there
				IS no `buffer` to be had. Make this more flexible...; 

	4. Uncertain [incomplete; require conceptual work/verification]	
		UP1. [maybe?] Turn the 'TableMap' and 'MapWrap' into 'FlexibleFunction' derivatives? 
			This would [pro]: 

				1. introduce some consistency into the library; 
				2. reduce "dynamic" addition of properties to objects (particularly - functions); 

			However [con]: 

				1. may cause a performance penalty (check); 
					Benchmark this rigorously!
		
			IF using FlexibleFunction is NOT slower (in terms of function calls), then do just that...; 
			The 'FlexibleFunction' is always going to be slower on creation (due to: 1. constructor; 2. runtime-parsing of FlexibleFunction), 
				but this shouldn't be too costly overall (besides, when string is static, it well may be cached to improve performance); 

	5. To Attempt [possible waste, needs investigation/benchmarks]: 
		PM1. [Maybe - add, check for feasibility + redundancy; A Big one] Add support for proper Pattern Matching;
			Serves as an abstraction over the 'RegExp'; 
			Allows one to match 'Token'-s using 'MatchablePattern'-s; 
			Would have: 
	
				[1. matching functions]
				1.1 StreamMatcher - works on Stream-s of items that are '.match(...)'-able [as arguments for 'MatchablePattern.match']; 
					Expects as a '.value'-input a 'MatchableStream', or a similar thing; 
	
				[2. MatchablePattern-s]
				2.1. [StreamClass] MatchableStream 	- walks through a given Stream of items, producing new 'MatchablePattern' on each '.match()' [ex: calls '.next()' on every '.match()']; 
					Depending on the output of the '.match' ('true', 'false', 'null', number, or PredicatePosition), either: 
	
						1. 'true' 					- goes forward 1 position; 
						2. 'false' 					- halts (mismatch)
						3. 'null' 					- goes backward 1 position;
						4. number   				- goes forward/backward specified number of positions
						5. PredicatePosition 		- goes forward/backward until specified condition on a 'MatchablePattern' is met; 
	
				2.2. FlexibleMatchable				- a 'MatchablePattern' defined by a particular user-given function; 
				2.3. [ChildrenTree] MatchableTree	- a 'ChildrenTree', that chooses one of the matching "paths" depending on the result of the '.match' function of its own; 
					Useful in combination with 'TreeStream'+'MatchableStream' - can create "matching trees"; 
	
				[Possibly, add more stuff... LOOK FOR CASES OF APPLICATIONS!]

	6. Unrelated/separate module/grand refactoring: 
		DE2. [Idea?] Create an error-handling API for the 'v0.4';
			Create means of: 
				1. Stacking different levels of exceptions (the 'try-catch' blocks); 
				2. Assigning relevant debug information to them;

			This is (primarily) for the development process of the parsers; 
			
			Define a Catcher [rough sketch]: 

				function Catcher (info: DebugInfo, logger: Function) {
					return function (thing: Function, thisArg: any, args: any) {
						try {
							thing.call(thisArg, ...args)
						} catch (e) {
							logger(info)
							throw e // NOTE: THIS here is to represent DEPTH [as parsers can be VERY recursive indeed, it may be needed to eliminate the recursive errors on a case-by-case basis]; 
						}
					}
				}

			And a ToplevelCatcher [rough sketch, can use to generalize the above, with 'quit = (e) => throw e']: 

				function ToplevelCatcher (info: DebugInfo, logger: Function, quit: Function) {
					return function (thing: Function, thisArg: any, args: any) {
						try {
							thing.call(thisArg, ...args)
						} catch (e) {
							logger(info)
							quit(e, info) // this is here to represent continuation of a program AFTER the exception
						} 
					}
				}

			Although... This is rather general. Perhaps, better implement as a separate package; 
			Likewise, there'd be ways to count levels of recursion, and assign types to them [specialized signature]; 	
		
		DP1. [Unrelated - later, spread to appropriate note-files] Deployment Pipelines for other maintained projects: 
			1. draw-text [install-npm + prod-to GitHub Pages]
				Also - ADD THE NPM DEPENDENCIES THAT IT REALLY REQUIRES - *INCLUDING* 'parsers.js'
			2. selector [to npm]
			3. regex [to npm]
			4. xml [to npm]

	7. Docs: 
		WE1. CREATE A proper website with documentation for the library. 
			Do benchmarks, et cetera...; 
			After doing GitHub Wiki for v0.3, see if it cuts it (spoiler - it likely won't one STILL wants the ravishing types for the library's docs to be present!)
				For this: 

					1. Learn a new CSS-based tool (either for generating/hosting docs/docs-css-styles like TypeDoc/readthedocs , 
						OR a new library to create one's own doc-styles like 'Tailwind CSS'); 
					2. Create a JSON parser in parsers.js and COMPARE it against the [https://chevrotain.io/performance/], with ops/sec. 
						See, WHICH libraries have managed to beat parsers.js v0.4., and WHAT can one do about it (see their optimizations, specifically...); 
						2.1. ALSO: create a benchmark for MEMORY USAGE [more concretely - compare parsers.js with the others...]; 
							The memory usage should be noticeably better than speed...; 

		JD1. JSDoc-s: add references
			Particularly - add the links to documentation website INSIDE the JSDoc (currently, lacks, the user has to look up themselves). 
			Reason (that it lacks currently, as of v0.3): time concerns

		JD2. Use a more complex (high-quality) JSDoc
			Current one is primitive (time concerns). 
			Add tags: 
				1. @returns
				2. @type [JSDoc-level self-reference for types]
				3. ...

		IN2. REMINDER: about the `init` method output type: 
			After it is generalized to a SINGLE interface, make FOR ALL `.init`-having method
			so that `init` returns `this`. 
	
		WI1. Add usage examples: 
			Current API is quite complex considering the need for generic functions (in many cases). 

		WI2. Add proper Guides: 
			Currently, the API code is pretty brutal. 
			Doesn't explain stuff, only contains crucial-to-be-used notes, 
				and general explanations. 
			The API *IS* simple (ultimately), but it is better to be explained,
				because otherwise quite a lot of things may seem cryptical and/or 
					unnecessary

			1. Separate the API documentation from Guides
			2. Guides would include: 
				1. Examples of code for mini-projects: 
					It's going to be one of:

					1. A JSON parser: 
						Use for:
							1. Stream-s [recursive descent]
							2. Bottom Up [tabular]
							3. Top Down  [tabular]

						Reasons: 
							1. simple to implement
							2. simple to test
							3. can unify the code
							4. not really useful (as builtin JSON.parse exists; e.g. no reason to refactor to another repo, besides tidyness...)

					2. Individualized examples of using SPECIFIC APIs [such as DynamicParser]
				2. Terminology pages: 
					Originally intended to be part of v0.3, these have gotten way out of 
						hand, and no longer feasible without some considerable effort (for which there really isn't much time...); 
				3. Samples page: 
					This is a Guide to the `samples` module. 
					Will list the "out-of-the-box" cases for usage 
						and link to respective `samples` module 
							exported definitions. 

		WI3. Remove links to old docs (v0.3 and prior...): 
			It's good, but it's old, and... 
				well, the library wasn't quite formed yet. 
			Constant compatibility breakages, no firm direction of development, 
				constant change of idioms employed, 
					it's basically (almost) like a whole new project now
						(difference being - now it's got a lot more types, and good swe practices: tests, SOLID, etc...)

		WI4. about `fault tolerance`, 
			the user can (themselves) create special "Error" types, 
				which are respectively handled by HashMap/IndexMap-based TableMap-functions; 

			Add a Guide for that.

	8. v0.5+ / future projects / great work / potentially, not worth it: 	
		65. [Much-much later; big maybe] A Stack-less CPS-based version of the library: 
			0. TypeScript
			1. InfiniteStack [from stack.js]
			2. CPSArray [from stack.js]
			3. Implements all the current items within the library using continuation-passing style

			Note: keep this for v0.5 ONLY. *Not* as a part of v0.4

FUTURE: 
	1. idea for a project - a command-line based tool for parser-creation: 
		Works thus, it allows the user to set (sequentially, one-by-one): 
			1. Enum-labels for types (uses a lazy FiniteEnum): 
				1. for acceptable tokens
				2. for acceptable tree-nodes
			2. Regular Expressions (in out Regex-flavour) for parsing different token types
			3. Grammar rules for creation of 'AST' nodes (see 'ASTAnalyzer' for v0.4)

		1. ALSO - allows a non-interactive mode

		2. Output: 
			1. parser (in JavaScript)
			2. source-generator (in JavaScript)

	2. REFACTOR [some] of the `internal` APIs into SEPARATE npm PACKAGES: 
		0. `copiable` package: 
			A TypeScript package. 
			Contains the `ICopiable` trait. 
			Make a dependency of this. 

		1. `callable` package: 
			1. Callable
			2. Composition
			3. ComplexComposition: 
				NOTE [important]: the `DynamicParser` is STILL here (as a case of application of `ComplexComposition`)
				TODO: rename this to something more interesting (like 'ReducableComposition', or something...); 
			4. IndexSet
			5. Signature 
			6. `IBuffer`, `IDynamicBuffer`, `CallbackBuffer` 
		
		2. `test` package [already developed]: 
			1. add the current 'ClassTest', 'MethodTest' 
			2. make `copiable` a dependency of this
			3. [unrelated] add serialization
			4. [unrelated] add 'Snapshot' API (similar to `Jest`)

		3. `autocache` package [make a dependency of parsers.js]: 
			1. Autocache
			2. IIndexable
			3. ISettable
		
		4. `serializable`: 
			1. `ISerializable` interface
			2. `ISerializableObject` interface

	3. Ensure that this works with: 
		1. Deno
		2. Bun

		Rewrite using middleware? 
		Or just check compabitility details? 
			[should be relatively simple, since the package doesn't make almost any use of Node specifically...]

	4. [maybe?] try using JSR for this one [https://jsr.io/]

	5. Standard-dependent [https://github.com/tc39/proposal-grouped-and-auto-accessors]: 
		This specific proposal absolutely rocks. 
		TODO: make all the properties that are currently "public" for both writing and reading, 
			but are intended to be `readonly` into `accessor varName { get; #set }`

			This creates a public getter and a private setter (precisely the thing). 
			Particularly, it touches: 

				1. ILinearIndexMap [.values, .keys, etc]
				2. ILineIndex [.char, .line]
				3. StreamClass [.value, .buffer, etc]
				4. all the `protected value` properties [InitializablePattern, etc]; 
					These are ALSO supposed to only be changeable privately...; 