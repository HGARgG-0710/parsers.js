[v0.3]
	
5. Source code TODO-s: 
	2.73. Do research (V8 performance): https://floitsch.blogspot.com/2012/03/optimizing-for-v8-introduction.html
		SPECIFICALLY - *inlining*. The library assumes its usage HEAVILY to remain efficient. 

	2.74. Do research (Chevrotain - source, capabilities, optimizations): 
		It is *the* fastest JS parsing library out there. See what it can do...:

			1. [basics] http://chevrotain.io/docs/tutorial/step0_introduction.html
			2. [important!] http://chevrotain.io/docs/guide/introduction.html

			[Post parsers.js v0.3; ADD this as parsers.js v0.4 todo...]; 
			3. [yes, that thing also...] https://github.com/Chevrotain/chevrotain/blob/master/packages/utils/src/to-fast-properties.ts#L2
			4. [building v8] d8 - USE IT to profile the optimizations (lack of) of the used library code [reflect upon, and change the library respectively]
			5. [Stream-specific] see if the '.curr/.next/.prev'  method re-binding optimizations were an overkill [elimination of 'this.isEnd = true/false/whatever-for-each-call']; 

	2.76. [parsers.js] Set up a GitHub Actions -> npm pipeline; ALSO - ADD A GitHub Workflows FILE for updating the latest version, and publishing to npm... + publishing the latest docs [separate]
		ALSO - create a list of presently maintained projects, for which to add such a pipeline: 
	
	2.77. Do research [learn LR/LALR-parsing properly]: https://en.wikipedia.org/wiki/LR_parser#
		Compare with the stuff the library currently has, (possibly) implement 
			some of the stuff from there... (specifically - the LALR parsers, if they aren't already present)
		Also, See: https://stackoverflow.com/a/24177215
		Also, See: https://en.wikipedia.org/wiki/Bottom-up_parsing
			ESPECIALLY: https://en.wikipedia.org/wiki/Shift-reduce_parser
				This describes how precisely the Shift-reduce parsers (most popular Bottom-Up parsers out there) work. 
				An LR-parser is a kind of a Shift-reduce parser. 
			And, finally: https://en.wikipedia.org/wiki/GLR_parser
			Note: this is the thing that's currently missing from the library. 
				It's extremely easy and opportune to make Top-Down recursive parsers with it, 
					however the 'Bottom-Up' approach is still missing

		IMPORTANT NOTE [add to docs]: the 'StreamParser' is a kind of mean for creating a PARSER-COMBINATOR 
			(meaning: it allows one to "chain" parsing layers together to form a parser, IMPLICITLY ESTABLISHING 
				the order of rule-application to the given source). 

Order of TODO-elimination: 	
	TODO [any order]: 
		-2. [minor] source code: 
			1. fix the `abstract` classes' names: 
				1. provide a STRICT convention: 
					1. instead of `Base`, use `Pre` [when extending via the `makeDelegate`]
				2. when returning a class instance, use `camelCase`-named classes, WITHOUT the `_`
		-1. fix docs [from tests]: 
			1. IHashMap: 
				1. added `default` property
			2. `IDefaulting`:
			 	1. make the `default` property `readonly`
			3. `ISupered`: 
				1. make the `super` property `readonly`
			4. `ILookupTable`
				1. added the `.copy()` method
			5. ALL CLASSES: 
				1. TO ADD the `.copy()` method
		0. missing permalinks:
			1. `utils.isGoodPointer`
			2. `utils.IndexMap.toPairs`
			3. `utils.IndexMap.listRanges`
			4. `utils.EnumSpace.fromCodePoints`
			5. `utils.EnumSpace.fromItems`
		1. refactor `makeDelegate` into one.js [AND MAKE IT LOOK PRETTY! - refactor the function itself]
		2. add the (minor) `JSDoc` for classes/big-functions
		3. add the jsdoc permalinks for classes to wiki
		7. GitHub Workflows
		8. Special Pages
		9. Tests: 
			2. Add relevant (missing) tests
			3. Modify existing tests: 
				NOTES: 
					1. be ignorant of the internal workings of the thing [behaviour-based]: 
						1. currently, some tests are breaking encapsulation
							FIX, via (instead) providing necessary public methods
						2. some functionality is (currently) not testable due to 1.
							Will need to modify the source code (again)
					2. refactor using one.js v0.5
					4. FOR EACH MODULE, DO: 
						1. List the methods/properties [classes], and functions [utils]
						2. List the methods to be tested
						3. Implement the tests (from scratch)	

				Micro-behaviour plan: 
					TODOs: 
						1. INTRODUCE `ICopiable` on EVERY class inside the library: 
							Reason - otherwise, they are not safely testable...
						2. FIX the ["constructor"]: new (...) => INTERFACE 
							to be ["constructor"]: new (...) => CLASS

							Reason: some classes have methods that are NOT on interfaces 
					2. walk through the tests, INTRODUCING CHANGES [do ONLY for methods that are currently, ALREADY PRESENT (stick to modular test-fixing approach)]: 
						2. Buffer
						3. EnumSpace
						4. IndexMap
						5. HashMap
						6. InternalHash
						7. LinearIndexMap
						8. LookupTable
						9. PersistentIndexMap
						10. Composition
						11. TableMap
						12. MultiIndex
						13. MultiIndexModifier
						14. Stream
						15. InputStream
						16. LimitedStream
						17. NestedStream
						18. PredicateStream
						19. ReversibleStream
						20. StreamClass
						21. StreamParser
						22. TreeStream
						23. Token
						24. TreeWalker
					3. eliminate the old test cases
					4. add new test cases: 
						They bear form:

							suite("ClassName (case #1)", () => {
								ClassTestObject.withInstance(
									new Class(), 
									(instance) => {
										test("ClassName.prototype.methodName1 (case #1), () => 
											instance.methodName1(...)	
										)
										
										test("ClassName.prototype.methodName1 (case #2), () => 
											instance.methodName1(...)	
										)

										...
										
										test("ClassName.prototype.methodName2 (case #1), () => 
											instance.methodName2(...)	
										)

										...
									}
								)
							})

						TODO: create
							1. `TestCounter` class
								for *counting* items within the given 
							2. `TestChain` class, 
								which has an internal `TestCounter`, 
									has a `handler` function, and 
										"inputs: any[][]" arrays of arguments to be 
											supplied to the `handler`

							USE the `TestChain` to express the chains of tests thusly. 
				
				2. FIX testing micro-behaviours: 
					1. add `handlers` to ENSURE that certain specified properties of test-arguments are met: 
						IF a property is NOT satisfied, THROW an error...
							SPECIFICALLY: 
								1. IF `.value` (or other such injectable property) 
									CAN be `null/undefined`, it must be checked that it isn't	

									Particular examples: 
										1. StreamClass (those that are `isPattern` - LimitedStream, ...)
								2. IF one has a "main type", from which "child types" 
									inherit, the "main type" MUST CHECK explicitly for
										properties/methods of a sub-type before calling 
											them.

									Particular examples: 
										1. StreamClass (.prev!(), .pos!, ...); 
									

					2. MAKE COPIES, 
						in other words - DO NOT rely upon a "single state"
							of the given object; 
						
						Do it thus: 
							1. take in the `instance`
							2. make A COPY of the instance
							3. FOR EACH method-test/property-test, 
								CALL them with THE COPY (which then may be altered)

					3. CHANGE the function-tests to CLASS-tests: 
						Reasons: 
							1. STORE the single `instance` property 
								on `this` (this - simplify the calls)
							2. SIMPLIFY the naming conventions for method-tests
								no more needless prefixes, just `this.methodName(...)`; 
						
						'ClassTest' is a class to be EXTENDED thus: 
							0. `ClassTest` COPIES the injected instance, during method/property tests (to permit safe local mutability)
							1. create a `static` symbol `methods`, which defines symbols for methods to be used
								1. Accessible (in general) via the `.constructor.methods` property
							2. methods TESTED DYNAMICALLY, via '.testMethod(methodName, args)'
							3. add simple method-names like 'name(...args) { return this.testMethod("name", args) }'
								Purpose: refactoring, simpler code, better modularization
							4. REMOVE the "big" tests: 
								1. remove the "signature-types": 
									Reason: 
										1. too verbose
										2. unneeded unification of testing logic
										3. bad for modularity (due to 2.)
										4. bad for readabilty (due to 1.)
					
					4. ALLOW injectable method-tests: 
						1. NAMELY: 
							1. Create CLASS-TESTS for INDIVIDUAL methods (THIS IS BASED off 'MethodTest', which calls the provided injected method-test-handler)
							2. INJECT the `this` (`instance`) to use with them
							3. INJECT the `method-test` singleton-instances INTO the class-test singleton instance
						2. They are THEN called via `ClassTest.prototype.testMethod` [which is abstracted over using properly named methods]

					5. REMOVE the "constructor tests": 
						1. those ARE NOT needed
							More specifically, REPLACE them with "interface conformance" tests. 
							These check ONLY for: 
								1. presence of all required-by-interface properties
								2. their types (neglecting the TypeScript-only mutability requirments)
							
							This ensures that ALL the methods are safe to call on the given object. 
				
				3. Add missing methods'/properties tests
					Walk through every class, list the missing methods, 
						implement tests for them, make adjustments to the 
							case-signature. 
							
				4. Add missing utils tests

				5. Remove redundant code

				6. Add the missing classes' tests
					Example: DynamicParser

				7. Add the tests for *THE NEW stuff* [notes moved from v0.4]:
				 	DO TDD with it (
						previous functinoality development 
						took so long due to lack of clear requirements
					)
					
		10. Self-referencing on Wiki
		11. To add Wiki pages [classes]: 
			4. Composition
			5. PreSignature
			6. LayerSignature
			7. StateSignature
			8. DynamicParser
			9. SignatureIndexSet
			10. TableMap
			11. MapWrap
			12. MultiIndex [shares page with `MultiIndexModifier`]
			13. InputStream
			14. LimitedStream
			15. NestedStream
			16. PredicateStream
			17. TreeStream
			18. StreamParser
			19. Token
			20. TokenType
			21. TokenInstance
			22. TreeWalker
			23. ChildrenTree
			24. ParentTree
		13. Add the missing stuff into the library [the "moved from v0.4" todos...]; 

		15. SIMPLIFY TYPES: 
			1. `A extends B, C, D, ... {}` TURN INTO `A = B & C & D & ...`; 

	JSDoc [on the par with wiki]:
		These are short and concise, don't get too deep into the implementation details. 
		[The "big" full GitHub Wiki will be for this...]

		Process: 
			0. add permalinks to: 
				1. minor classes interfaces [DEICIDE WHICH!!!]

			1. Walk the wiki/ tree [enumerate, COME FROM /wiki]: 
				[proper pages, + JSDoc that references them /w brief descriptions]
				2. interfaces
				3. classes

			2. for each, add one of either: 
				1. A full JSDoc
				2. A brief JSDoc + page

				DECIDE WHICH IS WHICH

			4. Special pages to write: 
				1. Usage
				2. Problems
				3. Lacking: 
					Make this a brief list (BASED OFF ONE on 'Home' page): 
						1. detalize the "various" unspecified options [provide sub-lists for those]
						2. do not expand the lists (it is complete on 'Home')
						3. write out a paragraph or two for each of the points "missing" or that are "problematic"
							1. ALSO - note about WHICH of the abstractions ARE to be deleted (like 'TokenizablePattern/ValidatablePattern', for instance...)

			5. VERIFY everthing visually, once the docuementation work is done...; 

	1. Wiki:	
		3. DOCUMENTATION POLICY: 
			1. IF `x` is an alias/util, DO NOT add a new page for it: 
				JSDoc ALONE will suffice. 
				JSDoc contains: 
					1. brief description
					2. @type tags: 	
					3. other tags...
					Read more about `JSDoc` in TypeScript at:
						https://www.typescriptlang.org/docs/handbook/jsdoc-supported-types.html	
			2. IF `x` is a class, it has a page + JSDoc with reference instead
				Page contains: 
					1. method documentation/signatures, output types
					2. public variables documentation/types
					3. *Notes* [those are important...]
				JSDoc contains
			3. IF `x` is an interface, it has a page + JSDoc : 	
				Page contains: 
					0. general info (purpose, usage, dependencies links, etc)
					1. (brief) method/property descriptions
					2. list of places where it's used
					3. provided implementations' list
			4. IF `x` is NOT an export, it is: 
				1. documented, IF it is somehow an important internal detail (ad-hoc decisions)
				2. otherwise, left without documentation (either a JSDoc, or a Wiki page)
		
		4. Wiki preface wording: 
			Work on the wording for: 
				1. `classes` pages: 
					0. low-level: `This module contains implementations for interface ...`
						Or [when multiple]: `This module contains implementations for interfaces ...`
					1. high-level: `This module contains items related to ... [process/idea/interface]`
						Or [when multiple, or contains implementations/utils/interfaces besides]: `... (as before) as well as ...`	
		
		5. Wiki page structure: 
			For structure-pages: 
				0. # *NAME*
				1. preface
				2. ## Exports
				3. [list of exports]
			
			For content pages [classes]: 
				1. # *NAME* [without the '.'-path]
				2. Description
				3. ## Methods
				4. *use the old ```ts...``` thing + descriptions for each argument
				5. [optional] per-method notes
				6. [optional] per-property notes
				7. [optional] per-class notes
		6. IMPORTANT - self-references: 
			EVERY time that a name from the library is mentioned, add: 
				1. `x` - code-stylization
				2. [...](...) - references, point to the abstraction's page
			DO IT SEPARATELY! 
		
		7. remove unused wiki-pages: WALK THROUGH THEM.
			Some capitalized-functions have themselves a separate page, 
				even though they don't really deserve it. 
			Delete those. 

	2. Workflows [GitHub Actions]: 
		1. make-tag [like in one.js]: 
			1. updates 'package.json' version; via `npm version`
			2. makes tag
			3. commits
			4. pushes
		2. npm-publish [like in one.js]: 
			0. runs the run-tests as a Reusable Workflow
			1. publishes to npm
		3. run-tests [not quite like in one.js, but similar]:
			0. builds the project and tests
			1. runs the tests

		1. AFTER you copy/write those, ADD a GitHub Gist for them: 
			1. make-tag
			2. npm-publish

			__NOT__ the `test` command. 
			Reason: too different from project-to-project. 
			These are the same. []

	TODO [naming]:
		1. ADD 'I' to ALL the interfaces within the library, INSTEAD of exporting all as `interfaces.*`: 
			Reasons: 
				1. simpler/faster to type
				2. more apparent usage
				3. [big point] no excessive `interface.` bloat
				4. [minor point] established convention
				5. [primary point] too many interfaces: 
					If one were to unify their naming convention, 
						instead of treating them the same way as 
							value-exports, it would simplify the 
								library's documentation a great deal. 

		2. CHANGE the structure for the `interfaces` documentation:
			1. No more `sub-pages` for `interfaces` - remove those, bloat the wiki only
			2. The interfaces are listed ON A SINGLE page
			3. THEY ARE SEPARATED *BY THEME/MODULE* (within the single `Interfaces.md` file) using the nested `#`-headers

	TESTS [work all anew... too much has changed]: 
		1. enumerate the things to test; 
		2. walk through tests, verify them for being satisfactory; 
		3. re-do where necessary, add new cases; IMPLEMENT MISSING;
		4. re-do the 'import' tests as well; 
		5. log the 'run.ts' thingie into a special temp text file; 
			1. [make into an npm command - 'test-run' and 'test: test-compile + test-run']
			2. add to '.vscode/launch.json' [to simplify the debugging process... we'll need it]; 
			3. WORK on the failed tests from there...; 
		
		6. WRITE a single large 'tests/demo' INTEGRATION TEST, which is a JSON/hybrid-parser. 
			It would utilize: 
				0. JSON: TableMap + HashMap
				1. hybrid: TableMap + IndexMap [local mutability]
				2. StreamParser
				3. DynamicParser
				4. switching between "hybrid" and JSON-modes BASED off 'JSON' contents [global mutability]

			Reasons: 
				1. new DynamicParser is EXTREMELY complex [hence, errorprone]
				2. to test out the overall interface manageability
		
	PROBLEMS: 
		Understood that the v0.3 IS NOT YET PRODUCTION-READY
			from feature perspective - there's just too little stuff. 
		Thus, one TAKES SOME of the features from v0.4 TO IMPLEMENT HERE. 
		BUT, before then - ONE NEEDS TO *DIVIDE* the library into "condemned (dead)
			and *LIVING* pieces. 
		
		Among specific restrictions:
			1. No docs (yet) for: 
				1. `ValidatablePattern` and `TokenizablePattern`
				2. `Eliminable` gets deleted
			2. WRITE DOCS AND TESTS FOR OLD VERSIONS FIRST, then - 
			3. implement the (a) missing feature(s) that go into the v0.3
			4. alter docs 
			5. return to 3., repeat until done
		
		TODOS for v0.3: 		
			Primary: 
				PO1. Introduce a new Position class - Multiline
					Basically, a pair of numbers: 
						(lineNum, charNum)	
					Create a generalization of it (where 'newline' is a context-defined term, like a particular "separation token", or something...): 
					Which correspond to prior number of newlines and prior number of symbols; 
					Used frequently throughout for things like debug information; 
					The '.convert()' would play very nicely with it...; 

					REASON TO IMPLEMENT AS V0.3: 
						1. Every module should ahve at least 2 different classes
							Exceptions:
								1. TokenizablePattern
								2. ValidatablePattern
							Reason:
								1. (as learned per v0.3) Implementing lazy algorithms in a fashion that is clean 
									takes a greater amount of time and effort. 
						2. `Position` is the only that has "semantically", only one unit - MultiIndex (MultiIndex)

					NOTE: it works ONLY with 'Stream's that have a `string` as input...
					
				PO2. [new Position class] Traveller, a point on a Stream [Position/classes.ts]

					REASONS for v0.3: 
						1. TreeStream.navigate - OUT OF FORM (incorrect implementation)
							This is supposed to be `.travel`; 
						2. `Position` is (rather) useless right now, ONLY ever allowing to do `MultiIndex.convert(treeStream)`, and 'treeStream.navigate(new MultiIndex(...))'

					Prior sketches of the v0.3 included a '.navigate' method that implemented an absolute positioning inside a Stream.
					This was scratched in favour of relative positioning, which is more widespread. 
					The absolute positioning, however, has a very powerful use case, provided more things are added: 

						1. StreamClass - a new flag '.size', for remembering the precise size of a Stream, in terms of the last position that 
							is visitable; 
							This is useful for: 

								1. Remembering a Position, at which the portion of the Stream starts; 
								2. Traversing the precise distance until the end; 
									The hard numeric bound gives a speedup due to the lack of need for checking for '.isCurrEnd()'
										(more precisely, one can replace it with a 'this.pos < this.size'). 

									For this, the 'isCurrEnd' has to be __'.writable'__; 

							For efficient implementation of this, one must be able to get to the beginning of the portion in question; 
							Therefore, a custom implementation of '.travel' is necessary. 
							Although, one can define a default that works by an efficient implementation of '.navigate()' and '.rewind()': 

								[sketch - NOTE: requires 'this.pos' to optimize for number-navigation]
								travelDefault = function (pos: Position) {
									if (isNumber(pos))
										return this.navigate((-1) ** (this.pos > pos) * (pos - this.pos)) // final 'this.pos+ = this.pos - (this.pos - pos) == pos'
									else {
										// the default WITHOUT this.pos
										this.rewind()
										return this.navigate(pos)
									}
								}

							Then, one can have: 

								1. A WrapperStream, on a given Stream, with '.size = true'; 
								2. Remember a Position, where WrapperStream starts on a Stream using a 'Traveller'; 
								3. Define an efficient '.rewind()' on the WrapperStream, that would use the 'Traveller' in question, after fully iterating through it; 
									Another reason for formalizing the WrapperStream; 
							
							This is absolutely priceless for lazy parsing (skipping pieces of data in order to gain information about it first 
								- then doing the parsing, on a need-to-know basis). 

							List of Stream-creation classes to make into WrapperStream-s: 

								1. LimitedStream; 
								2. NestedStream; 
								3. PredicateStream; 
							
							Like StreamClass (on which they'd be based), these are all generated classes;  

							ALSO: 
								1. Replace `TreeStream.navigate` with `TreeStream.travel`
									1. LATER: implement the `TreeStream.navigate` DIFFERENTLY - as a RELATIVE *count* for each of the levels: 							
										multind = [(0), (1), (2), ..., (n)]: 
											0 - from current level
											1 - absolute 
											2 - absolute 
											...
											n - absolute

						2. In accordance with '1.', a new default method for StreamClass - '.travel' (does an absolute navigation around the Stream)
						3. To remember particular Positions on a particular Stream, that may need to be recalled later (as per '1.'), one has a new 'Traveller' class: 

							[sketch]
							Traveller extends PositionObject<Position> {
								value: Position
								stream: TravelableStream
								convert() {
									return positionConvert(this.value)
								}
								travel () {
									return this.stream.travel(this.value)
								}
							}
								
				SI1. Add a "whole" Stream-interface, with `Partial` properties, and generics. 
					This is the early version [rough, unfinished, sketchy]: 
						interface Stream<Type = any> extends Summat {
							pos?: number
							buffer?: FreezableBuffer<Type>
							state?: Summat
							prev?: () => Type
							isStart?: boolean

							init?: (...x: any[]) => Stream<Type>
							navigate?: (pos: Position) => Type
							finish?: () => Type
							rewind?: () => Type

							curr: Type
							next: () => Type
							isEnd: boolean
						}

					REASON TO BE A PART OF V0.3: 
						1. library has been divided for too long. 
							It would make SO MUCH more sense, if EVERYTHING dealt in `Stream`, instead of `BasicStream`; 
					
				IN2. Make the `HashMap` NO LONGER a sub-module of `IndexMap`. 
					Make it separate. 
					Reasons:
						1. Doesn't allow for iteration algorithms
						2. Different ontology (no `extends` relation between interfaces, considerably differing purpose)
						3. Will cause less nestedness

				IN1. The `Initializable` interface - make it higher level: 
					1. Make the current `Initializable` into an alias `InitializableStreamClassInstance`
					2. Make the GLOBAL `Initializable` a generalization of this - `<A> { init: (...x: A) => this }`, where `A extends any[]`

					REASON FOR v0.3: 
						1. tired of constantly fixing broken types, want it be done well once and for all
						2. being able to do things like `const x = A.init(...).next()` is FAR too important to be left out of v0.3
				
				ME5. 'InputStream' - provide a '.peek(n)' method: 
					1. looks "forward" n items; 
					2. add a new interface - PeekableStream
						2.1. Add a general 'StreamClass' implementation: uses a 'PreallocBuffer' (empty by default), which 
							keeps the 'next n' items, to be looked up. 
							1. For when '.buffer' is '.isFrozen === true', this IS NOT necessary, one can (instead) just 
								index appropriately [reflect this...]; 
    				
							PROBLEM [PreallocBuffer]: what about the `.next()` calls? 
								In order to *FREE* the buffer for a sufficiently large `.peek(n)` operation after doing `.next(k)`, 
									one will have to call `.unshift()` [which is horribly slow]. 
								THEREFORE, create a class that EXTENDS the `PreallocBuffer`, and use that instead. 
								Call it `RotationBuffer`, and it will work like: 
									1. keep the internal array [pre-allocated, as per the `super` methods]
									2. keep the `.rotation: number = .length - 1 (by default)` index
										It will correspond to the `[i]` index AFTER WHICH the `.peek(1)` [respectively, `.read(0)`] value, starts. 
										It goes up to `.peek(.length - .rotation)`, after which it "wraps around", with internal `.array[0]` being `.peek(.length - .rotation + 1)`, and so forth, 
											up until `.peek(.length)`, which is the last permitted '.peek' for the chosen allocation. 
									3. keep a method for "moving" '.rotation': 
										1. .rotate(direction: boolean)
											if 'true', increases the `.rotation`, 
											otherwise - decreases it. 
    				
					Make 'StreamClassInstance' a `Partial<PeekableStream>` [add the generic `.peek()` definition for case of keeping a small '.peekBuffer' - NEW PARAMETER -- 'peekSize: number = 0']. 
    				
					1. Also - add `.next(n: DirectionalPosition = 1)` and `.prev(n: DirectionalPosition = 1)`, 
						where `n` is the number of items TO SKIP [forwards, or backwards respectively]. 
							The `this.curr` is returned INVARIABLY. 
							Note: this will require some heavy rewriting on behalf of the `StreamClass.prototype.next`; 
							1. ALSO - add the new `JumpStream = (n: number) => (value: Stream) => new StreamParser((input) => input.next/input.prev(n))(value)` [rough]:
							 	The old `parsers.js` had something similar, except not as well-designed...
    				
					2. Also - add the `.has(n)` 'Parser/utils.ts'-utils AS A METHOD, 
						of positive/negative indexes [with respective optimizations]. 
    				
					3. Then - REWRITE the `skip()` util from the `Parser/utils.ts`: 
						Will, from now on, be doing only some fairly simple conversions (PredicatePosition [/w .boolean] -> Function; 
							Then call either '.next(f/n)' or '.prev(f/n)'
						). 
				
				UP2. A new utility function (Parser/utils.ts) - match(word, stream): 
					For a given 'Indexable', it does: 

						// ! PROBLEM: lacks ability to check for MULTIPLE WORDS: 
						// 		* Solution: replace 'matchWord' with a more complex hand-written 'RegExp'-matching engine for the library... [uses 'Stream's instead of JS 'string's]: 
						// 			ALSO - use the *same* engine for the 'RegExpTokenizer'; 
						// rough sketch - requires the '.peek(n)' method; 
						// NOTE: the '.peek(0) === .curr' is ALWAYS a requirement...
						function matchWord(stream, word) {	
							const positions = word.length
							for (let i = 0; i < positions; ++i) 
								if (word[i] !== stream.peek(i)) 
									return 0
							return positions
						}

						// IDIOM: for skipping a word, call it 'function consume(stream, word, wrapper)' - a 'Parser/utils.ts' utility; 
						const matched = matchWord(stream, word)
						skip(stream, matched)
						if (matched) return wrapper(word)
				
				CC1. PreallocArray - a new 'Collection' type: 
					0. new property - 'lastInd: number = this.value.length'
					1. extends ArrayCollection
					2. new method - '.prealloc(n)': 
						1. If 'this.value.length > n', goto 5, else - continue
						2. this.value.length = n
						3. this.lastInd = this.value.length
						4. Finish
					3. change method - '.push()': 
						1. this.value[this.lastInd++] = this.value
					4. change method - '.get()' [as 'readonly number[]']: 
						1. return this.lastInd < this.value.length ? this.value.slice(0, this.lastInd) : this.value
					5. change method - '.move(n)': 
						1. this.value.length = this.lastIndex
						2. const lastValue = this.value
						3. this.value = Array(this.n)
						4. return lastValue

					Purpose: 
						1. Providing an interface for user-level preallocation of an array
						2. Providing 1. together with an integration with the 'Collection' interface

				CC2. RotationBuffer - See `.peek(n)` for more details
				
				TO2. Remove (replace):
					REASON FOR V0.3: 
						1. PatternTokenizer/PatternValidator is TOO SLOW IN PRACTICE ALGORITHMICALLY (not good - multiple passes instead of a single one)	
						2. They are unbearable. Seriously. Everything else in the library is SHINY. But one *is* missing that bloody RegExp engine...

					1. PatternTokenizer
					2. PatternValidator

					[NOTE: important - only these implementations/algorithms! Keep the interfaces, they're quite good...]
					3. ValidatablePattern
					4. TokenizablePattern

					With a NEW approach - a: 
						1. 'RegExpTokenizer'
						2. 'RegExpValidator'
						3. 'TokenStream' - output from 'RegExpTokenizer' [or, alternatively - an interface for the 'StreamParser']; 

						IMPORTANT NOTE: THESE *DO NOT* use a RegExpMap, instead a `Pairs<RegExp, ParserFunction (or whatever)>` is passed directly to them...
					
					These are function-creation functions that is based off *THE SAME SIGNATURE* as 
						the 'RegExpMap', with the difference that these get to be used with 'StreamParser/LocatorStream/PositionalValidator/...'; 
					
					The RegExpTokenizer is A FULL-FLEDGED RegExp ENGINE. 
					ALSO: 
						1. 'Regex' constructor for making the Regular Expression IS THE ENGINE: 
							Methods: 
								1. .matchAt(x: Indexed<string>, i: number)
									Checks that AT THE GIVEN `i` IN `x`, 
										there is AN IMMIDIATE MATCH. 
									This is important, as it allows the "tabular" 
										work that the RegexTokenizer and `RegexValidator` do...
								2. .sub(from: number, to: number) - returns a new Regex that is the result of LIMITING the current one
									between `from` and `to` indexes for VALID TOKENS. 

									This way, for instance: 
										(a(bc)+) has 4 tokens, ordered as: 
											1. (...) - outer bracket
											2. a 
											3. (...) - inner bracket
											4. + [relative to the outer bracket]
											5. b
											6. c

									Thus, recursion is handled BY THE FIRST OCCURENCE. 

								ALSO, add the JavaScript string-compatibility methods: 
									1. [Symbol.match]
									2. [Symbol.matchAll]
									3. [Symbol.replace]
									4. [Symbol.search]
									5. [Symbol.split]

								ALSO [imporant]: have ALGEBRAIC methods for it. 
									Meaning: ability to join them via a disjunction '|', 
										applying the quantifiers '*', '+', '{a, b}', '?', 
										applying negation '^'. 
									Each of those CREATES A NEW 'Regex'! 
									Purpose is to (basically) replace the `regex` module. 
								
								ALSO: the `Regex` implements an NFA
						2. `RegexTokenizer` is BASED OFF IT
					NOT based upon JS's one. 
					It: 
						1. Takes the signature that is MULTIPLE expressions
						2. Combines it into a `Stream`, that produces tokens according to the set rules: 
							1. consists of a PARSER for the original regular expression string 
							2. consists of a COMPILER for the regular expression AST (obtained from parsing)
							3. consists of methods: 
								1. amongst which there is '.match', from which 'Stream' is produced by doing something like: 
									new RegExpEngine(GENERICS)("REGEXP_STRING").match("TARGET_STRING")
						3. When tokens CANNOT be produced, STRINGS are produced instead
							Input (.value - it's a 'Pattern') *IS* a string...
					
					Reasons: 
						1. PatternTokenizer (AND TokenizablePattern), PatternValidator (AND ValidatablePattern) use THE SAME code
						2. PatternTokenizer, PatternValidator are (both) FAR too generic
							Used only once. 
							It's a waste of generality. 

					NOTE: the 'RegExpMap' REMAINS: 
						Reason: it can still be useful to someone (as a boilerplate saver...)

					ALSO: about `RegExpTokenizer`: 
						[maybe] - create your own flavour for 'RegExp's
						Then: 
							1. rewrite the `regex` module to INSTEAD create YOUR flavour [more modular]
								1. BUT - *do not* delete the current stuff
									Instead, add a submodule FOR YOUR flavour, and for the "builtin" JS flavour
									Together add a "convert" submodule to switch between them
							2. create a submodule for it to CONVERT between the two [JS regex <-> our regex]; 
							3. write proper docs for it
								3.1. [possibly] make a separate module; This could be too huge conceptually for the library's ontology

						It will improve upon the "simple" JS regex in the following ways: 
							0. replace the first and last items "/^$/" with /:#/
							1. add a general `^x` for NEGATING a pattern. Thus, elementary [^...] becomes composite ^[...]
								This way, for instance /^ab+/ matches "bbb", but not "ab". 
							2. add a general `(){n, k}` - limits input from 'n' symbols to 'k' inclusively
								to NOT grab it, use '(:...){n, k}' [basically, the same as the *ordinary* '{n, k}']; 
							3. remove the `\B`, `\D`, `\S`, (?!...), (?<!...), ..., other negative-classes/assertions in favour of the simpler `^x` form
							4. replace the (?:...) with (:...) [syntax simplification]
							5. provide different '\\x' characters (obviously, due to changed syntax); make the escaping redundant for the rest
							6. provide A SINGLE '\u{}' instruction instead of that horse**** with \X, \u and \c that JS has
								Will span the max possible values of the Unicode range
							7. get rid of the flags: 
								1. 'g': 
									replace the different behaviour with METHODS; make it non-exclusive

								2. 'v':
									remove, provide its functionality by default: 
										1. `P` not working - it is removed and replaced by `^p{...}`:
											By default, consumes the max possible string (remaining, if needed). 
											Limited by (...){n, k}

								3. 'i': 
									make it a new group type - that matches A GROUP regardless of the case: 
										Ideas for syntax: 
											
											1. /(#i...)/; 
											2. /(?i...)/; # more traditional...
									
									ALSO: make it possible to *inject* custom functionality 
										(utilizing the fact that `RegExpTokenizer` CAN be generic); 

								4. 's': 
									make it default
								
								5. 'u': 
									remove, included by default with `v`

								6. 'y' (????): 
									find out what this even is (and what it's used for)
									seems very much like a hack for 'regex.match/exec/search/whatever(string.slice(x))'...
							
								7. 'm': 
									remove - unnecessary
									possible to achieve the same thing using a slight modification of:
										/(?<\n)...(?=\n)/
								
								8. 'd': 
									too specific, ugly
									make these kinds of behaviours GENERIC instead
									[note: the 'RegExpTokenizer' WILL be generic...
							8. get rid of form-feed (`\f`): 
								Reason: not useful for parsing
							9. ALLOW the space in '{a,b}' (becomes '{a, b}'); 
							10. Add a possesive quantifier "*+", "?+", "++": 
								https://www.regular-expressions.info/possessive.html
							11. Add atomic groups: 
								https://www.regular-expressions.info/atomic.html
							12. Double negation inside a class: 
								^[^\w] is the same as (in JS flavour) [^\W]

								That is, it is COMLEMENT OF A CLASS of items that are NOT A CHARACTER.
								That is, it's \w. 
							13. Add relative backreferences: 
								https://www.regular-expressions.info/backrefrel.html
							14. Fix the ECMAScript behaviour of empty-matching backreferences: 
								https://www.regular-expressions.info/backref2.html
							15. Add the conditionals: 
								https://www.regular-expressions.info/conditional.html
							16. Regarding the /(#)/ - modes: 
								https://www.regular-expressions.info/modifiers.html
							17. Add subroutines (recursion): 
								https://www.regular-expressions.info/subroutine.html

						IDEA: use this regex-flavour for the HammockLang implementation later...

					ALSO - retain the `TokenizablePattern` and `ValidatablePattern` interfaces, 
						KEEP (and use) THEM for your `RegExpTokenizer`. 
						
						They are very good at (in general) at expressing the idea of a 
							"carrier object" and a its "handler function" for tokenization/validation information 
								(here - regular expressions). 

						Don't use them as-is, and INSTEAD of plain JS RegExp-s, use your flavour (+ the "iterable tables" for handles instead individual [string, handler] pairs,
							so the interfaces will still have to be changed somewhat...	
						), BUT *DO NOT* throw them out...

							IDEA: for usage with `RegExpTokenizer`, create an 'IterableMap' function [inside 'Parser/TableMap']: 
								This is a function that:
									1. takes in a LinearIndexMap [or other `IndexMap`]
									2. produces a function that: 
										0. assigns the map to its own `.table` property
										1. iterates the given map, CALLING each function-value inside of it with arguments: 
											0. 'this' [yes, it retains the `this` value...]
											1. the original value `x`
											2. the function itself (for access to `.table` property)
											n. all the other (spread) arguments
										2. returns the result of the last one

				SI5. IDEA: simplifying the list of available Stream-s further: 
					Replace the 'ReversibleStream' with a 'StreamClass.reverse()' method 
						returning a Stream with precisely this same definition; 

					Then, you can do: 
						const reversed = stream.reverse()
					instead of: 
						const reversed = ReversedStream(stream)

					Nicer semantics. 

				EL1. Eliminable - get rid of it: 
					More specifically, create a `RegExpEliminator` interface, which would replace it: 

					1. Based off the same RegExp engine as `RegExpTokenizer`/`RegExpValidator`
					2. More efficient [algorithmically], as it requires a single pass over the string, instead of 
						several with multiple `.split()` calls;
					3. Uses *the same* table-interfaces to represent multiple "ordered" removals as before

					ALSO: 
						1. remove the `EliminableCounter`
						2. remove the `Eliminable` module (as well as Validatable and Tokenizable); 
						3. remove the `Eliminable` interfaces

				SI2. Add the `WrapperStream<Type, S extends BasicStream<Type> = BasicStream<Type>>` interface: 
					This is a `BasicStream<Type> & Pattern<S>`. 
					Let appropriate special-cases of `StreamClass` be marked to implement it: 
						1. StreamParser
						2. NestedStream
						3. LimitedStream
						4. PredicateStream
						
				SI4. GenericCollection - a handler for different "combinations" of 'StreamClass'-initializers: 
					Sometimes (ex: PositionalValidator), one MAY need to create A SINGLE instance of the 'StreamClass', 
						which *happens* to be the exact same one that the user is already using. 

					Instead of creating A NEW class and (potentially): 

						1. wasting memory (insignificant, though noticeable - prototype objects)
						2. wasting optimizations (significant), by confusing the Engine
					
					The user COULD just receive a handle to the already created Class object, handled within a 'StreamClassCollection', 
						which would be: 

						1. A Global object
						2. That is connected to 'StreamClass'
						3. And FOR WHICH 'StreamClass' DOES provide a handle, 
							via *checking* whether or not the given class configuration already exists. 

							Checks: 

							1. properties/flags - by value/reference (x, y) => x === y
							2. by categorizing by FLAGS by 'Map' (so as to simplify initial search)

					The library actually DOES export various GenericCollection-s, namely one for each of: 
						1. StreamClass
						2. PredicateStream
						3. LimitedStream
						4. StreamParser
						5. ProlongedStream
						6. NestedStream
						[find any other classes that use a 'StreamClass'-like generated "base model" classes, that have to ALSO BE INHERITED]

				SI7. Expose the `StreamClass`
				Currently, it is not yet ready to be "released to the user", 
					however, once the `GenericCollection` [and .copy, and other optimizations...] get implemented, 
						one may as well do so, as: 

					1. It would now become far too high-level to still be an implementation detail
					2. [GenericCollection] There's an interface detail attached to it, that ISN'T AT ALL used by the library - it's PURPOSEFULLY external

			Secondary: 	
				ME2. Add a fast implementation of '.finish' to the 'TreeStream'; 
					When called for the first time - it SAVES the "last index", THEN - re-uses the computed one on any subsequent '.finish()' calls; 
					Calls the '.index' on the saved '.lastIndex'; 
					
				ME3. [StreamClass] Iterator Helper functions - '.filter', '.take', ...: 
					Provide them. 
					Use them in the library definitions that actually EMPLOY them [example: PositionalValidator would benefit greatly from a single '.filter()' call]; 
					
				RE1. 'regex' - allow for methods for creation of regular expressions that use the extended regex-syntax (enabled by the 'v' flag); 
					Namely: 
						1. intersection: 	/[[...]&&[...]]/v
						2. subtraction: 	/[[...]--[...]]/v
						3. string literals: /[q{...}]/v
			
			Minor: 	
				SI14. Add the ability to bind the custom `.rewind`, `.navigate`, etc implementations ONTO `StreamClass`. 
					Currently, missing. 
					Example: TreeStream. 
						It has to create a SEPARATE class that inherits from the "base" class that is, itself, the `StreamClass({...})`; 
						After this change, NO MORE will the "binding" happen outside the "base" definition. 
						Instead, THE ONLY method that is "bound" will be THE `.init` METHOD. 

				DE1. Increase the Library's natural debuggability: 
					ADD names to all the nameless functions. 
					This is due to the fact that in error logs, they would appear A LOT more clearly if one did do it. 	
					
				SI9. Make `Position` a submodule of `Stream`
					Reason: ontology. 
					`Position` is ONLY ever used with `Stream`-s. 
					Even when applied to `Tree`s (MultiIndex), it bears most use when combined with `TreeStream`. 
					Also - it's originally intended to be a "convertable" type for representing fixed (`number`) positions. 
					The predicates, too, work best when considered in directional context (id est: iteration...)

				SI11. The `.prod()` method - repurpose it. 
					Replace it with a simple, general `.peek(1)` [PeekableStream]
					Thus, it will allow a better unification of the library's concepts under the 
						same terminology. 

	5.2.73.
	5.2.74.	
	5.2.76.	
	5.2.77.

	[fix test compilation errors]
	[run the tests]

	B. 
		
	C.
	
B. documentation (wiki - change/fix it...);
	Hosted on the project's GitHub Wiki. 
	WHEN WRITING DOCUMENTATION, 
		
		0. make a note that the '.hash' and '.extension' functions SHOULD BE ABLE TO HANDLE null-values!	
		1. Note in the 'UnfreezableArray' docs that AFTER '.unfreeze()' is called, 
			the underlying Stream behaviour *DOES NOT CHANGE* (meaning - one has to create a NEW Stream...); 

		2. SPECIFY: that (at the moment) parser's best support is for TOP-DOWN RECURSIVE DESCENT PARSERS! 
		
		3.	Ways to do self-modification for DynamicParser [!!! - the Holy Grail...]: 				
			1. 'this.state.parser.layers = ..." [self-modification, GLOBAL]
				1. Also: 'this.state.parser.layers[i] = ...'; 
			2. 'parser.table = ...' [self-modification, LOCAL]

			TODO[1]: add examples for using it to the docs with 'StreamParser' + 'InputStream' [by far - *the* most common case...]; 
			TODO[2]: use the code for doing so WITHIN THE 'demo' integration test...

		4. ABOUT the '.typesTable' in `NestedStream`: 
			IT HAS A CONTRACT, of returning `null/undefined` instead of a valid index in `.getIndex`, IF a given item in the present NestedStream is NOT 
				supposed to be granted an index (and, thus, be made into a SEPARATE NestedStream), 
					THEN THE RETURNED INDEX is to be `null` or `undefined`. 

	EXAMPLES: 
		1. Currently, this is the code to take a certain `input` Stream, 
			then - take its portion and transform it: 

				// TODO: add this code as example for THE DOCS later...! 
				parserStream = new StreamParser(...)()
				limStream = new LimitedStream(...)()
				convertPortion = (input) => {
					limStream.init(input)
					parserStream.init(limStream)
					parserStream.finish()
					return parserStream.buffer.get()
				}

		2. Faster/more-elegant version of 1.: 
		
			parser = transform(...)
			limStream = new LimitedStream(...)()
			convertPortion = (input) => {
				limStream.init(input)
				return parser(limStream, new CollectionClass()).get()
			}
	
C. write the CHANGELOG [too much has been altered since the "original" v0.3]; 
	C.1. Walk through the last commit's state of v0.2.1, noting changes and writing them down [destructive]; 
	C.2. Walk through the v0.3 changes, and write them down [constructive];

[v0.4]

TODO: 

	0. General (see these before anything else): 
		DE1. Development order: 
			1. Start from `Primary` and `Secondary` todo-s
			2. Finish (or try to, at least) the `Secondary` problems quickly (1-3 days)
			3. Try not to add excessive amounts of new stuff to the library: 
				1. it is already quite powerful/loaded
				2. more things will just make it more obscure

			4. [IMPORTANT] Do TDD: 
				previous versino of the library was a HELL to write, for the following reasons:
					1. originally, there was a fairly moderate list of requirements
					2. it was quickly impleemented (~ 2 weeks)
					3. then, one foudn out that it was a BAD idea to be using arrays for everything 
					4. decided to implement lazy Streams
					5. goodbye, 8 months...: 
						1. more new classes
						2. changing utils/existing classes
						3. adding new methods/functionality (constantly)
						4. ...

					Key issues were: 
						1. lack of proper list of requirements (no vision for the library)
						2. lack of good tests (development was NOT test-driven, NO intermediate testing between changes/additions)

					This, way - DO TDD: 
						1. pick an item
						2. add skeleton code [passes TypeScript compiler only]
						3. implement (many) tests
						4. fail them
						5. write/fix (a comprehensive) implementation
						6. rinse and repeat, until it works desireably, and no tests (seem) to be missing

					Accompany all this with intermediate notes (as sometimes important information for later, that CANNOT, 
						go into tests may be forgotten otherwise)

			5. [IMPORTANT] Expand JSDoc: 
				1. use it more thoroughly (give higher priority for classes and so forth - not just a brief description + Wiki link)
				2. expand its contents (argument types, @type, self-reference, etc)
				3. [IMPORTANT] document early - another one of the banes of parsers.js: 
					Due to the fact that development was (mostly) content- and feature- oriented, 
						one very much forogt about importance of spending appropriate amount of 
							time on documentation. Hence, one (slowly) started to forget what one 
								was doing, and whether it even (sometimes) made any sense. 
					Had there been documentation, certain specific poor choices may have been more avertable, and 
						some little time could have been saved. 

		FI1. Eradicate small interface files: 
			1. Let the *small* interfaces BE LOCATED inside the `class.ts` files (as with `abstract` classes); 
				Reason: fewer pointless microscopic files with < 20 loc
					We have long abandoned the "total-modularity" philosphy on this project, so why not this too...

	1. Primary [new ideas, modules, interfaces/classes, algorithms]	
		TS1. TreeStream: create a new versions; 

			The 'TreeStream' (currently) is the 'LL' pre-order tree traversal algorithm Stream-implementation; 
			There should be ANOTHER - the 'LR' post-order tree traversal; 

			1. Rename the 'TreeStream' to 'TreeStreamPre', and create an implemnetation for the 'TreeStreamPost'; 
				This is (particularly) useful for some types of interpreters; 

				The Pre visits the node, then its children one-by-one starting from the beginning, then the siblings; 
				The Post visits first the children, then the siblings, only THEN the parent [when there are NO MORE children - then the parent is visited]; 

				Basically, Pre (parent-to-children) returns the nodes 'as they go' in the left-to-right traversal in the tree,
					whereas Post returns them in 'reversed' order: for the parent to return, children must all return first; 

				[Example usage: transform the binary '+' -- a + (b + c * (d + (k + f * r))) -> a + b + c * (d + k + f * r)]; 
			
		TS2. Implement a 'level-by-level TreeStream' ('TreeStreamLevel'): 
			It would get an initial given 'level' (tree root), walk through all of its '.children', 
				then walk each of the '.children' themselves as levels (one after another...), THEN descending lower...; 
			Would do it via calling '.getJointLayer' function (then - caching for later '.prev'); 
			This (basically) takes the nodes from the previous 'joint layer', gets their respective children,
				CONCATENATES THEM into a new layer, then iterates it; 
				
		NM1. [DEFINITELY!] Create a 'samples' directory; 
			This is for 'common-case' parsing - generally, walk through the various syntaxes that were 
				PRESENT within the previous parsers, use them?
			
			Or, better, make this into a separate mini-project called 'parsing-samples'; 
			[EXAMPLE: handling the 'escaped' strings/sequences all the time];
	
			Think about it [this particular refactoring is VERY fine and case-specific, though, it can come in useful...]; 
	
			No, if one is to do it, the thing should: 
	
				1. Work with general parsing patterns, that would be CONFIGURABLE for every case; 
				2. Provide low-level Token-s with specialized given names; 
	
			1. Tokens: 
	
				1. Take them from various parsing projects of yours already existing: 
	
					1. xml
					2. selector
					3. regex
	
					Unite, choose best string-names; 
					Create appropriate names for sample-token classes (via TokenInstance) [
						example: 
						
						Opbrack - (, 
						Clbrack - ), 
						OpSqBrack - [, 
						ClSqBrack - ], 
						OpBrace - {, 
						ClBrace - }, 
	
					...]; 
	
			2. arrays with common items (USED throughout...): 
	
				'binary = [0, 1]'
				'decimal = [0, 1, ..., 9]'
				'hex = [0, 1, ..., F]' - TAKE OUT of the global '/utils.ts'
				'ascii = [0, 1, ...]'
				'alphanumeric = [0, 1, ..., 9, a, ..., Z]'
	
				Also - the sets in question should be provided AS-ARE (without the 'inSet'...); 
			
			3. ALSO - join the 'samples' with the 'constants.ts' file 
				[they are pretty similar in that they BOTH provide frequently recurring ambigious constant entities with meaningful "static" names and uses]; 
			
			4. MOST IMPORTANT - generation functions, stuff for working with AST-s: 
				1. Based off ASTAnalyzer, ASTStream-s, ASTHierarchy, etc... Generally, the 'Tree.AST' module
			
			5. Stuff for Stream-transforming: 
				Namely, nested-stuff via the 'StreamParser' [WITH '.buffer']: 
	
					// A VERY common pattern...
					// Generalize to an arbitrary 'X <- LimitedStream() [here]', with 'StreamParser.init(X())'
					const l = new LimitedStream()()
					const t = new StreamParser()()
					const parse = (input) => {
						l.init(input)
						t.init(l)
						t.finish()
						return t.buffer.move()
					}
	
			6. module-related stuff IS GIVEN ITS OWN MODULE 
				Example: DelimitedStream goes into the `samples.PredicateStream.*` module
		
		IM1. Create a new 'IndexMap' implementation: 'SetMap'; 
			This would be a: 
				1. IndexMap interface implementation; 
				2. generalized wrapper around a Set (in a way that HashMap is a wrapper around Object and Map); 
					2.1. Would have its own 'extension'; 
					2.2. For 'values' would contain true/false; 

			This is useful for working with cases that require the 'or'-predicate; 
			Example: 'nested' utility; 
			How it is done [pseudo-code]: 

				const SpecialCaseSetMap = SetMap(...)
				const processNested = (x) => nested(TableMap(new SpecialCaseSetMap([...])), TableMap(new SpecialCaseSetMap([...])))(new WhateverStream(x))
								
		IM2. [new type] BitArrayHash [IndexMap/classes.ts -> *]
			Accepts a BitArray, returns a value from it; 
				A hash based off bit-shifts (<<, >>); 
			Use for defining the '.init', '.curr' and other choice functions in the StreamClass; 
			BitArray - a UInt8Array (or other) array. Look into the '2.35' (pattern-tokenizer speedup) for details. 
			[Namely - to properly replace the `chooseMethod` stuff]

		IM5. [IndexMap/classes.ts -> *] MultiMap - [IndexMap-like] this returns ALL the matched items, and while running multiple results 
			Can contain functions (called to obtain a value), or values themselves.
			Reason: 
				1. greater modularity
				2. modular representation of constructions like: 
					if (A(x)) B(x)
					if (C(x)) D(x)
					if (R(x)) return M(x)
					return K(x)
			
			More specifically, this is a MUCH MORE DEEPLY CONFIGURED IndexMap, one 
				that generalizes the linear search on the `LinearIndexMap`, and can persist 
					FOR LONGER [hence, needed NOT ONLY for looking for the first item]. 

			Namely, it can be configured to take MULTIPLE SEARCH PATHS, that can include: 
				1. moving to the next/previous item/call
				2. performing a call/returning item
				3. performing a check over the next key
				4. quitting, while returning the last stored value
				5. storing the item returned from the last call (includes non-function items)
			
			NOTE: the *FUNCTION* used to determine the next step CAN BE CONFIGURED FROM INSIDE THE CALLED FUNCTIONS THEMSELVES. 
				Thus, allowing for another achieved case of local self-modifiability. 
			
			One can see that this is a full-blown DFA. 
			Also - the user can (optionally) store the '.state' property on these things 
				(they come "pre-added", as the ignored empty object doesn't take any space at all). 
	
		TC1. Create an 'ASTAnalyzer' class [under new 'Tree.AST' module]: 
			1. gets an 'AST', defined by: 
				1. Trivial nodes - 'TokenInstance'-like
				2. Content nodes - 'Token'-like or 'TokenType'-like
				3. Recursive nodes - 'TokenType'-like, with arrays for values
			2. Traverses the tree: 
				1. New abstraction: ASTNode - class, with '.from' method: 
					1. The '.from' method takes a JavaScript object (JSON), 
						with values as described above, and transforms it INTO A PROPER TREE, 
							using: 
								1. ChildlessTree
								2. SingleTree
								3. MultTree
					2. The ASTNode is a 'ChildrenTree' [see Tree/classes.ts]: 
						1. Namely, it is based off a 'ChildrenTree' descendant (it's a class-generating function), already provided ones are: 
							1. ParentTree [for quick iteration via 'TreeStream' - better for subsequent reads]
							2. TrivialWalkableTree [for better memory footprint - better for initial writes]
					3. ASTNode can be created via a constructor, like any other tree [uses the 'constructor' of the given parent class]: 
						1. Namely, it employs a specific 'converter' function [one that converts the elements "simply"]
						2. Keeps a '.type' property on itself (it's a TokenInstance)
						3. There are *3* types of ASTNode-s [3 distinct classes - for optimization purposes]: 
							1. RecursiveASTNode - without '.value' and '.children: ASTNode[]'; NOTE: here 'ASTNode' is an interface
							2. ContentASTNode - with '.value: any', without '.children'; This is just a 'TokenType' with additional prototype-methods
							3. TrivialASTNode - with '.value', without '.children'; This is a 'TokenInstance', with additional prototype-methods
					4. THERE IS a `.toJSON` method that: 
						1. uses THE CONSTRUCTOR FOR THE APPROPRIATE `Token`/`TokenInstance`/
						2. THE `Token`/`TokenInstance`, then, will ALSO have a `toJSON` method!
							1. Defined so that the DEFAULT `.toJSON` for the "non-Token/TokenInstance" items in the tree 
								WILL BE a "special" 'stripMethods()' util (possibly move to one.js), for 
									CREATING AN OBJECT WHICH IS **THE SAME** as the current one, EXCEPT 
							2. IMPORTANT NOTE: 
								1. IF the implementation of `AST` ends up being simplistic/trivial/Token-like, 
									REMOVE 'Token' and 'Tree' as a module! 
									Instead, just add the `AST`! 
									Then, there have to be the ability to create the 'AST' on a per-layer recursive-descent basis...
										[like with `Token`]
									AND to use it for from-tree-source-generation purposes [like with `Tree`]. 
					5. Likewise, there is the '.fromToken' method for creating an 'AST' object from nested 'Token's
					6. ALSO - there is a 'fromJSON' util which CREATES A NESTED TREE of 'Token/TokenInstance' ONLY (trying to "guess" via "type" and "value" fields). 
				2. By doing 'ASTStream(TreeWalker(ASTNode.from(...)))', one can iterate a given generated JavaScript AST object! 
					2.1. This is what the ASTAnalyzer (actually) is doing; 
					2.2. The 'Stream' is, then, applied a 'StreamParser' upon; 
					2.3. NOTE: that the 'TreeStream' used is CONFIGURABLE [namely - 'ASTAnalyzer' is, too a function, taking in and returning A CLASS!]
					2.4. About 'ASTStream' - see below...
	
			3. Returns information: 
				1. [Buckets-Categorization] Arrays of '.trivial', '.content' and '.recursive' nodes
				2. [Type-Categorization] Returns a 'HashMap' of arrays of nodes, filtered by their '.type'; 
					NOTE: the 'HashMap' is, TOO, completely configurable by the user [user-defined (if provided), else - default being: when 'string's are used for 'type's - ObjectHashInternal, otherwise - HashMapInternal]; 
						(another function-argument for returning the final ASTAnalyzer class)
	
			4. Provides functionality [ASTAnalyzer]: 
				1. mapTypes((type_name: string) => string): 
					creates a new 'ASTNode', by means of mapping the '.type'-s
						of the current one using the given function. 
					Immensely powerful for changing between different data formats. 
	
				2. mapValues((x: any) => any)
					creates a new 'ASTNode', by means of mapping the '.value's of each 'ContentASTNode', 
						while preserving all else
				
				3. .find(type: ..., pred: (x) => boolean): 
					Seeks an item in the tree with a type '.type', obeying the predicate '.pred';
					Optimizes to look specifically in a '.type'-bucket of the given 'type'. 
					Can be radically faster for large inputs. 
				
				4. .find(pred: (x) => boolean): 
					if the 'type' is not passed, the entire tree is searched; 
					Optimization - uses prior obtained collections for increasing cache locality of large searches. 
				
				5. .iterate(type) - returns a Stream, filled specifically with values of type 'type'; 
					Namely, it returns an 'InputStream', wrapped around the respective '.type'-bucket; 
					The items are listed on a from-beginning-to-end of the 'Stream' used to construct the ASTAnalyzer; 
	
				6. .filter(pred) - returns a new tree, such that it ONLY contains elements that obey the given predicate: 
					HOWEVER, one can also mandate that in order for a sub-tree (RecursiveASTNode) to remain, 
						at least one of its children must be 'true' as well. 
						This is done by means of returning `null`, instead of `true`/`false`.  
				
				7. .search(type: any) - searches, and returns for multi-index information for each of the elemenets of a given type. 
					Optimization: uses ONLY JUST the information from a given '.type'-bucket; 
					This allows one to: 
						1. Iterate a portion of the tree INSTEAD of needing to start "at the beginning
						2. Know exactly when to stop (seeing the *last* item that needs to be iterated over)
	
				8. .map((x: ContentASTNode | TrivialASTNode) => any): 
					Maps all the non-RecursiveASTNode parts of the tree to a different 'Tree'. 
					Useful for creation of generalized non-AST trees, to be used with 'TreeStream': 
						Example, passing a function that returns 'string', then - CONCATENATING all 
							the items inside the obtained Tree, via: 
	
								// [sketch - no types]
								function GenerationFunction(F) {	
									return function generate(ast) {	
										return array(TreeStreamPre(ast.map(F)), new UnfreezableString()).get()
									}
								}
								
								// this is to create either a SUBSET defined by a recursive IndexMap-like function, *or* via working through a '[type]'-defined set of "superior" nodes; 
								// NOTE: 'ast.types[type]' is an *ARRAY*
								function GenerationRecursive(F, type) {	
									return function generate(ast) {	
										return array(InputStream(ast.types.index(type).map(F)), new UnfreezableString()).get()
									}
								}
	
						IDEA: ADD a set of 'samples' to the library - see above...
	
					Also - '.map(F)' OPTIMIZES, so, it expects the given values to be returning FUNCTIONS, for plugging in their children's 'F(x)', 
						so: 
	
							1. A -> B
							2. F(A) -> X(K): X(B) == string
							3. F(B) == string
								F(A)(B); 
							
					The optimization is - SPLITTING the '.types'-buckets via doing '.types[name].map(...)'; 
				
				9. RecursiveASTNode: 
					1. Contains various copying .value-Array-delegate methods: 
						1. .map(f) - creates a new RecursiveASTNode via 'new RecursiveASTNode(this.type, this.value.map(f))'
						2. .filter(f) - creates a new RecursiveASTNode via 'new RecursiveASTNode(this.type, this.value.filter(f))'
	
			5. Optimization information (additional): 
				1. IDEA: add a new 'ASTHierarchy' information object, which: 
					1. Specifies, what '.type'-s CAN be found within which '.type'-s of RecursiveASTNode-s: 
						IMMENSELY powerful for search-optimization inside a given Tree 
							[limits the '.type'-buckets in which one needs to look]; 
					2. Specifies maximum "depth" of a certain particular 'RecursiveASTNode': 
						1. Allows one to replace a continuous check of 'isGoodIndex(.lastChild)' with 'isGoodIndex(.lastChild) && i < MAX_DEPTH_COUNT'; 
							Guarantees a performance bound. 
							When the bound is PRECISE, the 'isGoodIndex' can be dropped
					3. Specifies maximum length for '.children' of a given 'RecursiveASTNode': very useful for iteration! 
						1. Replaces '.children.length' check with a 'counter < FIXED_PRECOMPUTED_LENGTH'; 
					4. Specifies expected structure for a given 'RecursiveASTNode': 
						1. May permit one to be going one-by-one downwards for it, EXPECTING the given '.type's; 
						2. Eliminates checks for '.isSiblingAfter'
	
					'ASTHierarchy' is optional, and its purpose is solely to: 
						1. provide algorithm optimizations
						2. enforce bounds for a given format
	
					NOTE: 'ASTHierarchy' is ONLY good for optimization, when IT IS PRECISE 
						[id est, we can SKIP certain function-calls/checks]. 
	
				NOTE: optimizations work via: 	
					1. ASTStream-s: 
						0. Alternative to 'TreeStream': less generic, provides better optimizations for AST: 
							1. ASTStreamPre - alt. of TreeStreamPre
							2. ASTStreamLevel - alt. of TreeStreamPre
							3. ASTStreamPost - alt. of TreeStreamPost
						1. Accepts an 'ASTNode' instead of a 'Tree'
						2. Optimizations: 
							1. No '.lastChild' presence check in '.isChild()' method - instead, just '.lastChild > -1'	
								This is a good optimization for TreeStream-iteration of large trees ('.isChild()' is called ON EVERY '.next()' call); 
							2. Algorithm Configurable via 'ASTHierarchy':
								1. Optimizations are present ONLY when ASTHierarchy is PRECISE
								2. Expects a VALID abstract syntax tree
	
				2. For '.type's field values, one uses EITHER an 'ObjectHashInternal' (if strings), or a 'MapHashInternal' (if not strings); 
	
		DE3. *Add* ability to store indexes of various AST nodes within the original 'FreezableBuffer'
			object passed to the InputStream: 
				Rather a curious problem here, as the Stream that issues the error HAS TO KNOW what index 
					the original input 'String' has that the parsing starts at. 

			NOTE: this is ONLY for the TOP-DOWN parsers BASED OFF 'Stream'-s! 
				Index-finding for Error-recovery in a Bottom-up parsing is a whole other beast entirely...; 
			
			SOLUTION [idea/attempt]: an 'IndexBuffer' (a FreezableBuffer) wrapper: 
				1. This accepts a 'number[]': 
					Basically 'export const IndexBuffer = FreezableBuffer<number>' [rough, BUT that is it's ultimate definition...]; 
				2. Requires making the given 'Token' an 'IndexAssignable': 
					1. Create a new class 'IndexedToken', one that would also be having an '.assignedIndex' property; 
					2. '.assignedIndex' stores the INDEX inside the '.indexBuffer', that the user can store on (say) .state of the given StreamParser;
						1. IndexBuffer has THE SAME length as an actual '.buffer' would, EXCEPT that it *doesn't* get to store the values, ONLY just the 'indexes': 
							1. For this, USE '.pos'; 
							2. [big maybe...] Enable storing the .indexBuffer as an optional 'stream.indexes' property? 
								1. Would automatically enable '.pos' property
								2. Would keep the '.pos' associated with each object
								3. Would assign '.assignedIndex' to the thing...;
									3.1. Would expect 'IndexAssignable's as its items; 
								
								4. Then - create an 'RecoverableStream' [from StreamParser], which would do that sort of thing...: 
									1. keeping positions for future error-reference
				3. The 'IndexBuffer' stores FINAL indexes (in the original 'InputStream')

			Then, one can do a setup: 
				0. All the Stream-s have a running '.pos' that is being '.push'ed to the respective '.state.indexBuffer'

				1. InputStream - contains the original '.buffer' (no '.state')
				2. [first] StreamParser - tokens contain indexes inside the initial parsed string - 'InputStream.buffer'
				3. [higher-level] StreamParser - tokens contain indexes within the '.value.state.indexBuffer': 
					This way, the chain goes: 

						// rough sketch: TODO - add this full-on as a 'util'; 
						function findErrorOrigin (errToken, stream) {
							let index = errToken.assignedIndex
							do index = stream.value.state.indexBuffer[index]
							while ((stream = stream.value) && stream.value)
							return stream.buffer.get()[index]
						}
					
					Purpose of keeping the '.assignedIndex' on EVERY level, is that IN CASE that this is a parsing error 
						that can be discovered early, IT WOULD [alternatively - during later validation, one may want to provide a more precise error information]
	
		BU1. [Bottom-up Parsing]: 
			1. Add: 'Rule'
				Consists of: 
				
				1. .head: 
					Describes function to "wrap" the matched item with. 
					One of: 
						1. 'TokenType'/'ContentASTNode'-like - uses a single item for '.value'
						2. 'RecursiveASTNode'/'TokenType'-like - uses a multitude of items for '.value' [reads an array from '.body']: 
							NOTE: if a 'TokenType' is used then PRESENCE OF LAST ARRAY TAKES PRECEDENCE, that is - the body 
								*ending* with an array will cause a '.value: any[]', whereas one with a SINGLE value, will cause a 
						3. 'TokenInstance/TrivialASTNode' - has no '.value'
				2. .body: 
					This is a sequence of items: 
						1. 'TokenType'/'RecursiveASTNode'/'ContentASTNode'-like [interface]
						2. 'TokenInstance'/'TrivialASTNode'-like [interface]
						3. ('TokenInstance'/'TrivialASTNode'-like | 'TokenType'/'ContentASTNode'-like | Empty)[]
							This is to describe the ACCEPTABLE TYPES for a '.value: any[]'-TokenType or a 'RecursiveASTNode'. 
							It is basically equivalent to '(TypeA | TypeB | ...)+'

							WHEN the special object 'Empty' is used instead, ANYTHING (including nothing) is matched. 
								Id est, it becomes '(TypeA | TypeB | ...)*', NO ITEMS AT ALL is also acceptable. 

							The array of items that is given CONTINUES the array of items that is wrapped into the given '.head' of 'RecursiveASTNode'-like. 
							Namely, here are the PRECISE TYPES of Rules for each head: 

								1. RecursiveASTNode <- A B ... (or 'TokenType-like <- A B ...') - PRODUCES A '.value: any[]'
								2. TokenType <- a - PRODUCES A '.value: any (single value)'
								3. TokenInstance <- a - PRODUCES a 'TokenInstance' (no .value, matched 'a' is ignored)

							ANY OTHER COMBINATIONS ARE *NOT* suppored: will be ignored. 

						4. A special 'Matchable' interface, which would generalize the 'RegExp' with '.match': 
							This is considered to be the "end" for application of a given Rule. 
							IF the '.body' of a 'Rule' is NOT a 'Matchable', then a SUB-RULE is used for it, one that PRODUCES
								a respective '.type' of token (that is, has a '.head' with a respective type of token)
									is used. 
									
							Note, that OTHER kinds of '.type'-possessing classes CAN be used (user-defined EXTENSIONS of 'TokenInstance', for example...)
					
					The "matched" item is passed as an argument to '.head'

			2. 'Rule's are kept inside a `RuleTable`: 
				A `RuleTable` is the huge table used by the `TableParser`. 

			3. `UpParser` - a shift-reduce parser for the library:
				[properties]
				1. [protected] .stack - the stack of currently unparsed tokens
				2. [protected] .curr - the current item 
				3. [protected] .table - RuleTable, a '.prototype' property: 
					1. During class-creation with 'table', one constructs LOOKAHEAD-'Set's for each of the available Token-s
				4. [protected] .value - a 'Stream' to be bottom-parsed
				5. [public] .result - an 'AST'/'Tree' produced: 
					On the level of the interface - THIS IS A 'readonly' PROPERTY! 

				[methods, internal]
				5. .shift() - moves the Stream forward one position
					0. call '.curr = .value.next()'
					1. push 'this.curr' to the stack
				6. .reducePlain(x) - applies single-lengthed recursive rules to the given 'x'
				7. .reduce() - applies the rules from '.table' to the first rule in the parse-trees currently held in the '.stack'; 
					0. There is a '.currentRules: Rules[]' set of 'Rule's that keeps track of 'Rule's that are *possible* from this particular point. 
						To make it fast, there is a whole tree generated for obtaining these sets from specific items 
							AT THE CREATION STEP of the current 'StackParser' class using the '.table'; 
					1. This applies the currently "chosen" rules to the current item '.curr' in the '.stack'. 
						1. Each of the 'Rule's have their STATE, which consists of: 
							1. '.pos' - position [<= '.body.length']
							2. '.reset()' method (for doing '.pos = 0')
							3. '.inc()' method (for doing '++.pos')

							NOTE: that the rule is being matched IN REVERSE! 
								This way, you have: 
									'A B C .' [.pos = 0] -> 'A B . C' [consumed C, .inc()] -> 'A . B C' [consumed B, .inc()] -> '. A B C' [consumed A; match whole, reduce]

						2. Once '.pos = .body.length', the rule has been fully matched, and it is time that it was '.reduce()'-d 
							Note: the '.reduce()' AUTOMATICALLY finds all the '.reduce()'-d rules in the '.toReduce' .protected member
								WHICH contains the next item to be reduced

				[methods, external]
				7. .inject(value) - injects a new 'Stream' to parse
				8. .get() - NOTE: this is an 'InitializablePattern', with '.value', '.get()' and '.inject(value)'
				9. .parse() - method for actually calling the parser: 
					This implements a proper shift-reduce parser based off a RuleTable. 
					TODO: CHOOSE the algorithm for it. LR/LALR/SLR/other [OR - better, make it GENERIC (somehow), add the respective Prefixes - LRTableParser, LALRTableParser, ...]
				10. constructor(value?): 
					The 'StackParser' is a function-generated class in TERMS OF '.table' (which goes on the prototype). 
					The '.table'-lookups in the '.parse()' method are OPTIMIZED based off the '.table' function. 

		SM1. LATER: important problem - self-modifying parser INTEROP of different self-modifications. 
			Problem with multiple self-modifying parsers is that they rely on SHARED STATE. 
			Which is bad. PARTICULARLY, when code of two independent vendors may be doing the modifications. 

			Solution: 
				1. provide a COLLECTION TYPE [IdCollection] that:
					0. wraps around Array
					1. stores IndexAssignable items
					2. comes with a '.search(id)', by .assignedIndex: 
						1. Defined via a 'MapInternalHash <- HashMap' [FastLookupTable], which STORES values of '.assignedIndex', and returns items from the Array by it
				2. provide 'Stream's classes within the library with DEFAULT 'static .assignedIndex'
				3. for cases, when it's IMPOSSIBLE to have sane defaults (ex: two LimitedStream(), or PredicateStream(), or ...), 
					user can employ the already existing 'assignIndex' util; 

			Benefits of solution: 
				1. Generic
				2. [Relatively] Low-level - wraps around Array more or less directly
				3. Easy to configure for the user (just set the appropriate '.assignedIndex')
				4. '.search()' is fast [Map - based off FastLookupTable]

			Then, a self-modification for parser vendor could: 
				
				this.state.parser.layers.search(...).handler.table = ... // or something...

		TD1. [Top-Down Parsing]: 
			The library (currently) only has the interfaces for aiding in 
				creation of Recursive-Descent Parsers. 
			Create a `DownParser` - similar to the `UpParser` (the Bottom-Up Parser). 
			It would operate based off a RuleTable. 

			[important] NOTE: one will need TWO different types of `RuleTable`-s: 
				1. UpTable - Bottom Up Parsing
				2. DownTable - Top Down Parsing

				Those differ in their construction algorithms. 
				ALSO: important note - the respective tables, themselves, 
					MUST BE SELF-MODIFIABLE! 

		IM6. ArrayInternalHash [InternalHash]: 
			A wrapper around `ValueType[]` for keeping the number-keys. 
			Store together with MapInternalHash and ObjectHashInternal; 

	2. Secondary [new utils, methods]
		UT1. bufferize(tree, size) - util [Tree/utils.ts]
			Given a 'Tree', returns an array of values of size `size`: 

				{
					childNumber: number,
					value: any
				}

			The `size` is a fixed number >= 0, 'childNumber' is the number of children, ahead of current node in the array.
			To be used for creating a persistent linear data structure for working with the given AST. 
			Good for cases, when the number of items is known, and is large (part of the the "big input" optimizations); 

			Would permit user to store a '.state.size: Pattern<number>' property (Pointer(number)) on the ParserState-s, 
				for counting the size of the input in terms of nodes, then re-use it with 'bufferize', 
					and use the tree for evaluation; 

			If the 'size' is unknown, implements a simple DFS (namely - conversion of a given 'Tree' into an 'Array')
			
		UP1. utility [Parser/utils.ts] - 'nestedInfo'; 
			Returns the object of information regarding the `nested()`-arrays based off the given `inflate`, `deflate`
				functions. 

			Structure of the object (type - NestedInfo): 

			1. .length: number - number of sub-elements (possibly nested themselves)
			2. .whichNested: number[] - array of indexes, of nested elements -- those, which the `inflate()-deflate()` pair detected
			3. .nested: NestedInfo[] - the info for EACH of the `.whichNested` sub-items
		
		ME1. RE-INTRODUCE the '.copy()' method and 'Copiable' interface; 
			Comes inside the 'StreamClass' (optionally) via the '.state' property;
			Will additionally be supported by: 

				1. 'StreamClass(...).prototype.copy()' method; 
					Solution for '.copy': 
						1.1. By default, copy ONLY: 
							1. the user-defined variables under '.state'; 
							2. the '.pos', '.buffer', and so forth - predefined "inherited" properties;
								Any CUSTOM copying/initialization logic can be defined by the user in DERIVED CLASSES; 
					
					Definition [loose, has to have overloads, depending on presence/absence of '.pos']: 

						function uniCopy (stream: ...) {	
							const copy = new (Object.getPrototypeOf(stream).constructor)()
							copy.uniInit(initSignature(this, this.bits))	
							return copy
						}

					NOTE: here, 'this.bits' is the '.prototype'-level BitArray with different properties that ARE PRESENT; 
						Look below for pseudocode explanation

			ALSO [regarding '.init']: 
				[Predefined] 
					To simplify the process, store (on each StreamClass), a 'BitArray' - to indicate, which are present; 
						Depending on this - the user will be able to call the '.init' with "universal" signature, like so: 

							// note: stored on the '.prototype', no memory waste
							stream.uniInit(initSignature(this, this.bits))
						
						The 'initSignature' is a util, returning the "maximum" '.init'-signature, that is possible; 
						The '.uniInit' is a method that initializes the given Stream BASED OFF THE "maximum" signature; 
						This version of 'initSignature' is "from-bits", one that isn't is done via an object like: 

							stream.uniInit({
								state: this.state, 
								value: this.value, 
								...
							})
	
			ALSO: 
				1. the copy is ALWAYS DEEP [otherwise, one would create some serious synchronization-issues for internal state, like '.pos']
					This INCLUDES the `.value` property
				2. the copy is SYNCHRONIZED ON STATE [the '.pos' is copied as well, for instance...]; 

		UT2. util - enumerateTree [Tree/utils.ts]
			Given an array of "types" of nodes, it recursively converts their properties (given in the 'shapes' array), 
				to single-array-siblings form (the current ChildrenTree); 	

			This would permit a more "variable" set of trees (those that have properties with fixed names), 
				to be enumerated accordingly...; 

			Example [1]: 

				{
					type: any, 
					a: any
					b: any
					c: any
				}

			Becomes [1]: 

				{
					type: any, 
					value: [
						// ... a, b, c
						// ... or b, a, c
						// ... or any other order
					]
				}

			Example [2]: 

				{ type: any }; remains the same
			
			Example [3]: 

				{ type: any, namedProp: any }; becomes { type: any, value: any }; by changing the property name to 'value'

			For this, define a 'TreeKind'/'TreeShape' - with a list of properties to be used/transformed; 
			MORE SPECIFICALLY, it creates a 'ASTAnalyzer'- and 'ASTNode'-fit tree, from one that is 
				too complex (not homogenous enough nodes, more complex node categorization). 	
	
		IM3. [!!!] Add another 'hash' function for the 'HashMap'? 
			In particular, use the 'extension' for 'set', 'rekey', 'delete' (by-original-key), 
				and another hash (the 'indexHash') for 'index' [this one will be optional and default to 'extension']; 
			This is in parallel with how the 'IndexMap' has it.
			Motivating example: 

				1. Indexes are objects with a field 'T: keyof X'; 
				2. Indexed are the objects with a field 'R: keyof X'; 
				3. One wants to create a HashMap with 'N = HashClass(...)', such that '(new N(...))(T) == smth'; 
					Problem here is that one EXPECTS the same hash to work for both the 'index' and 'delete'. 
					In general, the 'HashClass' does NOT support the categorization (even though in the case of 'SimpleTokenType', it works); 
				
		UT3. Add more complex 'Tree' utilities. Based off previous 'one.js' v0.3.1 [the removed methods], add: 

			1. deepSearch - searching inside the given Tree for a value with a given 'prop'
			2. depth - calculating the depth of the given Tree
			3. treeCount - counts the values of a given predicate for all items in the tree

			ALSO: Add new methods to 'ChildrenTree': 

				1. .reverse() - recursively reverses the given tree
							
		PO3. [static method] MultiIndex.fromPosition(stream: TreeStream, position: Position): MultiIndex
			This is a method for converting a 'position' from the given TreeStream to a 'MultiIndex'; 
			Does NOT '.rewind()'; 
					
		ME4. Add "move semantics" to 'Collection's: 
			1. '.move' method - for a 'MoveCollection extends Collection': 
				1. Sets the 'this.value = []', or to a similar "default"/"empty" value. 
				2. '.move()' returns 'this.value'
		
			This permits to achieve the 'PreallocArray' idea WITHOUT the need to copy/change the contents of THE SAME array. 
			CONCLUSION: YES, add the '.move' to SOME collections via 'ArrayMoveCollection' with '[]'; 
			
		CU1. Add a 'binarySearch' utility: 
			1. For IndexBuffer - as it keeps 'number's in an ordered fashion, there is no reason not to employ it...; 
			2. This is generic (meaning, it provides a map 'f', and then compares via 'f(a, b)', with default being '(a, b) => a < b'); 

		SI10. A new `Stream` class: 
			The `DashStream` - a generalization of `JumpStream`. 
			A `StreamParser`, provided with `f(stream): number`, it will call EITHER '.next()' or `.prev()` based off the number returned (positive - '.next(n)', negative - '.prev(-n)'). 
				In other words - it behaves *EXACTLY* like `skip()`

			Let it also permit returning predicates (hello, `skip()`)

	3. Minor [fixes, optimizations, refactoring, types, naming]
		OE1. v8-specific optimizations: 
			Having done some benchmarking on some typical Array methods, one arrived at a...
		
			CONCLUSION: 
				1. Handwritten methods are *often* __MUCH__ faster than the builtins ON LARGE PIECES OF DATA; 
					On the more common-place cases, however, they are inferior in time performance; 
				2. THEREFORE, one should do: 
					Re-organize the library (how?) to permit usage of the current methods on LARGE pieces of data; 
					Prior - do more benchmarking; 

					About re-organization: 
						1. Think - whether to introduce explicit low-level 'size-controls' [possibly costly, when run a lot]; 
							Although, it's likely to be optimized/negligible; 
						2. Or, whether to split the thing onto 2 chunks [where the 'size' is precomputed...]; 
							This is less flexible (and thus, error-prone), plus will be more difficult to re-structure; 
						
					Current vote is for 1.; 

					ALSO, about bounds - those should be CAREFULLY BENCHMARKED; 
			
			MORE GENERALLY - try to adapt the library for work on LARGE datasets; 
			It'll make it useful for working with big projects/pieces-of-text; 

			THINGS OF ISSUE TO THINK ABOUT IN PARTICULAR: 	

				1. What "LARGE" sizes are practical? [meaning - sources of what sizes can occur in the wild?]
					Take them as large as one can. At least 10000000 symbols (~30MB, ~100000 lines of 90-100 chars), maybe try more; 
				2. How much of a performance difference can one get by doing these optimizations?
					In particular - try to sketch out a parser for something (the smtf format of one's own? it's simple to parse), perform 
						profiling on a REALLY big auto-generated file, then: 
							
							1. take overall time measurements (performance.now()); 
							2. profile, break the execution down on several tiny pieces; Then - benchmark and optimize them accordingly; 
								For this, use results from samples from one's benchmarks library;  
				3. Optimize for smaller sizes ONLY when it doesn't impair the memory severely; 
					When it impairs it at all, choose whether or not to do it based off the chosen memory/speed difference measurements...; 
					
					3.1. FOR THIS [maybe] - have specialized "Heuristics" constant-space in 'constants.ts'; 
						3.1.1. IF doing them - create the heuristics for DIFFERENT ENGINES; 
						3.1.2. IF oding them - make the 'Heuristics' only the "default value" for transition of sizes; 
							THE USER must be able to set their own boundries; 
						3.1.3. IF DOING THEM - check how to "bounrdies" for the increase in efficiency when using 
							the custom implementation "shifted" as the versions of Node progressed - TRY IT with different node, v8 
								versions; 
						3.1.4. IF DOING THEM - other engines to test: 
							3.1.4.0. V8 (Chrome)
							3.1.4.1. SpiderMonkey (Firefox)

						[maybe?] Try building the engines from nil, running the benchmarks for heuristics standalone...; 
	
		OT1. Create a "fast" 'TreeIterator' class [Tree/TreeIterator/classes.ts]
				This is to replace the WalkableTree+TreeWalker+TreeStream combination due to: 

					1. They are [algorithm-wise] slow (require FIRST converting the source to a 'Tree'); 
					2. They are [algorithm-wise] memory-costly (creating a new Tree-structure is somewhat demanding); 
				
			This would (instead) approach iteration directly - it would use PARTICULAR properties for a given Tree;
			Also, (as it's not a PERSISTENT type), it'd accept a 'converter' (so as to be able to "iterate" AND "convert" a given Tree); 
				Note: the `convert` function is called on each Node. 

			The interface works like: 

				TreeIterator(TreeKind(...))(childIterator, converter)
			
			Requires NO copying for long-term storage (it's "inlined"), hence - no time needed for unnecessary allocations; 

			The 'childIterator' is (basically) an IndexMap/function that returns for any given item its' sub-iterated items
				(an array of children); 
			The 'childIterator' is used for walking through the items INSIDE the 'TreeIterator'; 
			
			BENCHMARK THIS [compared to the 'WalkableTree'+'TreeWalker'+'TreeStream' approach to code-generation]!
			
		PR1. [prototypes, generality] make all the prototype 'value's (in particular - methods) OVERRIDABLE! 
				via doing: 

					Object.defineProperty(_class.prototype, {	
						methodName: { value: ..., writable: true }
					})
				
				Currently, they are ALL non-overridable; 
				This will enable the user to more liberally use the library classes [they are, presently, highly single-purposed (intentionally)]; 

		TO1. [refactoring] Add a new util [Token/utils.ts]: 
			tokenCompare = (tt: TokenType) => (x: any, y: any) => tt.is(x) && tt.is(y) && value(x) === value(y); 
			
		TS3. TreeStream: flag optimizations; 
			Allow 'TreeStream' to be a Stream-class-generation function also; 
			Permit the TreeStream to have overloads for: 	

				1. navigate (work with number-indexes, when '.buffer' is present); 
				2. next		(same thing - '.buffer' + '.pos')
				3. prev		(same thing - '.buffer' + '.pos')
				... [and so forth]
			
			Will allow to have the benefits of flag-presence via the overloads; 
			
		IM4. [maybe?] Do some amazing TypeScript jiggery-pokery:
			Sometimes, the type can be more complex to predict [but STILL might be possible via templates, type-inferences and special functions like 'Parameters'...]: 

				[LinearIndexMap/methods.ts]
				1. extend
				2. extendKey
				
		SI3. [codebase consistency] The '.init' methods for StreamClass DO NOT return 'this': 
			Fix this - let them. All the other '.init' methods do it; 
			(This, in particular, generally permits usage like: 

					const t = new InitializedClass().init(...)
				
			In cases, when the constructor is NOT the same as the '.init(...)';
			Likewise, a different semantics [copying] for '.init', when a partial copy is necessary: 

					const t = new InitializedClass(...)
					const r = t.init(...)
					t === r // false
			)

			Then, one'll be able to finally add the missing ': this' output type to the 'Initializable' mini-interface
			
		TY1. [types] Maybe - split the "specific" composed 'X extends A, B, C, ... {}' types from the elementary 'A, B, C, ...': 
			Currently, they are being exported within the same 'type'-module as the first (original/closest) usage. 
			However, one could also (instead) export them at the special 'interfaces.mixin' level [which, basically, contains the "small" interfaces]. 
			
			One COULD otherwise put them into a single 'refactor.ts' file, HOWEVER, then becomes the problem of needing to actually export some of them 
				[ex: Indexable]. 
			
			Thus, divide them on: 
				1. "unused" (src/refactor.ts) - those that DON'T appear in any function definitions
				2. "used" - export them IN THE SAME module as their ontology best demands
	
		NO1. Adopt a new naming convention - the 'I' for interfaces is TERRIBLY useful. 
			Yes, one *did* prior consider it a semantic bloat, however, it allows one 
				to see dependency injections/generalized patterns much clearer due to 
					greater explicitness of using an interface and *not* a class with identical name. 

		SA1. Relocate the `DelimitedStream` to `samples.PredicateStream` [YES, THE `samples` SHOULD HAVE THEIR OWN SUBDIVISION AS WELL!]; 
		
		VA1. validation-related mini-predicates: 
			At least: 
				1. 'input.next() == X'
				2. 'set.has(input.next())'

			Provide as 'trivialCompose'-compositions of elementary methods...

		SI6. Add constructor types for SPECIFIC cases of `StreamClass` [see ex: PredicateStreamConstructor - used by DelimitedStream]
			1. ALWAYS, whenever a composite type is RETURNED from a function, and then USED by *another* function, ONE GIVES IT AN ALIAS. 		
	
		SI8. Move the current contents of the `utils.Stream.StreamClass` into the `Stream.utils`: 
			They DO NOT contain anything specific to the implementation/class-creation (which is the purpose of the `StreamClass` as an interface). 
			Instead, they are concerned with checking existence of properties on given specific objects implementing the general `Stream` interface. 
			It would make a whole lot more sense, and less confusion for the user. 

		S12. Remove the `.isStart` requirement from the `BasicStreamClassInstance`. 

		SI13. DOCUMENTATION: add the `StreamClassSignature` documentation. 
			It is currently omitted. Reason: implementation detail, the `StreamClass` is not yet properly exposed...

	4. Breaking [removals]	
		UT4. REMOVE the `utility` types: 
			Put them ALL into `samples`. 
			THE ONLY 'utils' to keep are: 

				1. Parser/utils.ts
				2. IndexMap/utils.ts (without the `table`)
				3. Stream/utils.ts (only the 'byStreamBufferPos' and 'isEmptyStream', namely)
				4. Pattern/utils.ts (only the `dig`)
				5. Position/utils.ts (everything, except for `is` functions)
				6. Tree/utils.ts (only: `hasChildren`, `sequentialIndex`, `treeEndPath`)
				7. DynamicParser/utils.ts

			Those - MERGE into a single export-file called `utils.ts` (new module -- `utils`); 
			Split them in a (similar) fashion to how it was before, if you want to, except: 
				1. `dig` - this one's standalone now
				2. `Parser/utils.ts` - this combines two types (AT LEAST), hence, it cannot be well-categorized

			The `is` functions are ALL kept inside the special new `types` module.

	5. Uncertain [incomplete; require conceptual work/verification]	
		UC1. [maybe?] BreakableStream - a major generalization of the library's patterns: 
			Returns a class for producing Stream-s: 

			1. Have an '.input' (another Stream); 
			2. Has an 'Indexable' of 'BreakPattern's (they are objects) pre-given, which define how the Stream instances behave: 
				2.1. type: string - one of: 
					2.1.1. limit - works like LimitedStream; 
					2.1.2. nest - works like NestedStream; 
					2.1.3. halt - stops the stream [no more items after that]; 
					2.1.4. navigate - finds: 
						2.1.4.1. Either the next item [or the first item AFTER THE BEGINNING] that obeys a given property (PredicatePosition)
						2.1.4.2. Goes to a static position [relative or absolute]; 
						
						ALSO, has a boolean arg 'isRelative' (default: true, basically - whether the values for 'navigate' are to be handled absolutely or not...); 
					2.1.5. transform - applies a given transformation on the '.input' (calls a function); Returns the result;
				2.2. args: object - the way that the 'type' is resolved (equivalent of constructor-arguments/.init-arguments); 
				2.3. buffer: boolean | () => boolean - this one corresponds to whether the current value should be bufferized; 
					When a function is passed, it is called on the 'BufferizedStream' in question to get the value; 
			
			The result of matching 2. to the '.input.curr' is (effectively) evaluated to be the Stream's '.curr'; 

			This todo is a 'maybe?' because all the said things can (pretty much) already be done by the user using StreamTokenizer and the rest
				- this really only just provides a minor abstraction over the whole thing (frees the user from needing to use the exports of the library); 
			This kind of thing is (somewhat) against the library's "all-minimialism" approach; 

			On the other hand, it would permit one to use a very domain-specific language to describe operations on streams conventionally, 
				thus - simplifying semantics + unifying a very large number of different abstractions that are creatable via the library using just this one; 	
			Conclusion: think about it, more of a YES, tha a NO currently;

		UM1. [maybe?] Add a module SPECIFICALLY for working with strings/identifiers/Sequence-s (currently: the 'Indexed' interface); 
			In particular, functions/abstractions: 

				1. shorten(names); 
					Given a sequence of "names" (strings/sequences),
						and a 'comparison' predicate for each of its elements,
							it would implement a name-shortening algorithm, 
								that would preserve the name uniqueness;
					The return value is an IndexMap; 

				2. renameable(names);
					Given an IndexMap of names (which could come from, for instance, 'shorten'), 
						the method: 

						1. Re-orders;
						2. Creates new name-map entries;

					In a fashion that would allow straightforward implementation of a sequential "exact replacement algorithm" based on the output: 
						1. Loop through a list of name-maps; 
							2. [In the loop] Rename current encounters of a name with its mapped value;	
						
					There are 2 operations that may be necessary: 
						1. creation of temp-names (when collisions in present names are far too high to re-order);
						2. re-order; 
					
					The algorithm for the 'renameable' function: 
						0. Keep the cached (met) names in a 'Set'; 
						1. Loop through the 'names' given: 
							1.0. If the name is in the cached Set, continue; 
							1.1. If the current 'value' of the 'name' is already present amongst 'keys' (another loop): 
								1.1.1. put the keys in question BEFORE the current value;
								1.1.2. '.swap' the two indexes;
								1.1.3. go one position back (because now one needs to check the "others" now); 
								1.1.4. cache the current amongst the checked names in a 'Set', (so that one doesn't get into an infinite loop);

					This is useful when implementing things like mass renamings/name shortenings in code mangling software; 

			Put identifier, and Indexed-related stuff there (organization, overall tidyness); 
			Problem with this: 
				1. short
				2. (potentially) generics-mania strikes again...

		UP1. [maybe?] Turn the 'TableMap' and 'MapWrap' into 'FlexibleFunction' derivatives? 
			This would [pro]: 

				1. introduce some consistency into the library; 
				2. reduce "dynamic" addition of properties to objects (particularly - functions); 

			However [con]: 

				1. may cause a performance penalty (check); 
		
			IF using FlexibleFunction is NOT slower (in terms of function calls), then do just that...; 
			The 'FlexibleFunction' is always going to be slower on creation (due to: 1. constructor; 2. runtime-parsing of FlexibleFunction), 
				but this shouldn't be too costly overall (besides, when string is static, it well may be cached to improve performance); 

	6. To Attempt [possible waste, needs investigation/benchmarks]: 
		TO1. OLD - likely redundant, see below...
			The result of this is O(nk^2 log n). 
			The RegExp engine [RegExpTokenizer -> TokenStream] would likely give us a "lazy" O(nk). 
			*If* it performs better than the current TokenizablePattern and ValidatablePattern:
				1. Remove them
				2. Remove this note
			
			Otherwise, explore the possibility of making this AND THEN benchmarking against the `ValidatablePattern`
				and `TokenizablePattern`. IF the `O(nk^2log n)`-version *is* faster, 
					then remove the `ValidatablePattern`/`TokenizablePattern`. Else, keep them, and remove it instead. 

			FULL OLD NOTE: 
				IDEA: how to speed up the tokenization algorithm [attempt]:
				Way: 
				1. Split the "tokenized" and "untokenized" parts with 2 arrays
				2. Keep the 'indexes' of the tokenized (on a per-item basis) and untokenized (on a "linear" - read below - basis) parts
				3. Split the final result (tokenized) into BATCHES	
					3.1. The underlying is a NEW structure called 'BatchArray'; 
						It keeps its values as a list of lists, which HAVE THEIR INDEXES; 
						The second-level lists are all ordered as the original array is; 

						Operations: 
							
							0. bind(array) - binds the given array
							1. newBatch() - creates a new last batch, to have values written into
							2. merge() - combines the batches into a single array, removing the indexes; 
								For this - use Merge-sort (it's a natural choice here - the structure is precisely what the algorithm requires); 
								Due to quality of data, the complexity (in this case) is O(nlog k) [considerably better than 'O(nlog n)'];
							3. replace(i, subarr, subinds) - replaces a given index with a value of 'subarr'
								This has to be fast, therefore: 

									1. Store the '.bound' NOT as 'any[]', but as a special 'ContinuedElement[]'; 
										It keeps elements as "referenced" (ContinuedElement), and 
											items that are referenced can have: 
												1. a '.value' (when no array is used); 
												2. an '.array' (when one is used, otherwise - null); 
											
										Thus, the replacement-with-array operation becomes O(1);

									2. When this happens, after changing the 'ContinuedElement' one: 

										2.1. Finds the batches that have the affected elements; 
											To make it fast (for general case), store batches NOT just as 'any[]', 
												but also have '.minIndex' and '.maxIndex' properties; 
											Thus, it's possible to determine (in a single check) whether 
												they are affected ('.maxIndex < .changedIndex')
												in O(1) time; 
										2.2. Finds the elements inside the batches; 
											Done via binary search over the indexes; 
											Then - as they are linearly ordered, walks the remainder of the batch-list, 
												doing 2.3.; 
										2.3. Changes the elements; 
											Add the 'length-of-the-replaced-subarray - 1'; 
											O(1) already, no need to change anything; 

										IMPORTANT: for this to be FAST (algorithm becomes O(k^2 logn) and NOT O(nlogn)), 
											one needs to do this step is done in a SEPARATE 
									
									3. When 'replacement' occurrs recursively (inside an existing ContinuedElement): 
										3.1. It's the same, only difference is - one has to calculate the index manually (which can be mildly cumbersome, but is still O(n)); 
									
									4. The '.indexes' list, then, is ALSO altered; 
										The old 'untokenized' index is "deleted", new UNTOKENIZED items' indexes (subinds) are put in its stead; 
										It is ALSO kept as a 'ContinuedElement[]'; 
										When an index is JUST deleted (that is, it's tokenized wholy and singl-y), 
											one replaces the value with 'null' (no '.splice' necessary); 

										To find it, one has to: 

											1. Take the first 'ContinuedElement' x: 
												1.1. Increase the value by (if array has no) 'x.array.length', or (if no '.array' is used, just '.value') 1
													To make checking for presence of a 'ContinuedElement' inside 'ContinuedElement' faster - add a flag-property ('isRecursive: boolean'); 
													BETTER IDEA: 
														Use a 'BitArray' (a new data structure - a UInt8Array/UInt32Array, whatever, that keeps flags as bits); 
														ANOTHER IDEA: 
															Use a 'BitArray' to store a number [together with a boolean]; 
															This way, it can have less bits, and more appropriate for a given usage ("cram it" with the flags - these go first); 
															Then, one can also KEEP TRACK of the first 'ContinuedElement' [helps, when it's > 0]; 

															ANOTHER number stored - count of recursive 'ContinuedElement'-s [allows to sometimes finsh early]; 
															IDEA: 
																Keep not the FIRST 'ContinuedElement', but the one that is THE FARTHEST! 
																Then, what we have is - the largest possible speed increase; 
																This also requires an ADDITIONAL number getting stored: separate BitArray (for the index that is skipped); 
																Remainder are linearly walked-through; 

																BETTER IDEA STILL: 	
																	Store it upon the 'CountedElement'-s that are using '.value'; 
																	THEN - there is no need for the flag, as one ONLY works with 'the next' value; 

												1.2. Proceed until the happenstance that it becomes equal to the sought-after index, OR 
													the difference between the current one, and the next becomes "too far" to be used as an intermediate-point; 
													From there - one does a literal jump (as there are no more "nested" indexes left); 
												
											Therefore, our '.indexes' is capable of getting the current index at about O(k), where 'k' is the table size (number of tokenizations); 

											IMPORTANT: the '.indexes' is actually A 'ContinuedElement' ITSELF; 
												Thus, all techniques can be used on it also...; 
								4. indexSync() - see '3.->2.->IMPORTANT'; 
								5. getIndex(prevIndex: number): number - for iteration, see 'Fields->2.'; 

						Fields: 

							1. .bound: ContinuedElement[] - the array of items; 
							2. .indexes: number[] - the list of indexes that are NON-REMOVED [used for iteration]
								IDEA: instead of keeping it as a list of numbers, use 'BitArray'-s, with a changing memory outfit; 
								This permits to (greatly) economy the memory, because to see if an index is "present" one just 
									marks it as 1/0; 
								The "nested" ones will require more memory; 

								(
									Possibly... One could save *some* memory with a complex bit-layout?
									Thus, for instance, having, 01 mean 'non-nested, present', 00 - 'non-nested/nested, missing', 
									'1' - 'nested' [only a single (first) bit used]; 

									Also - there'd be another - 'GrowingNumber' (new library data structure; this time - a single-element UInt8Array/UInt16Array/UInt32Array as a base, not UInt8Array); 
									This one'll keep a single number; NOTE: the thing is needed at all even because of the '2^53 - 1' number-bound (namely, that one doesn't like it...)
									This is (basically) a wrapper around a number to allow for quick manipulations that are ALSO memory-efficient

									'Nested' ones will USE THE SEPARATE INDEXES inside the '.indexes: (...)[]' array; 

									Then - one'll be able to have ANOTHER method on this, called 'getIndex'; 
									It'll return the next non-missing iteration index, based off the current one; 
								)

								Also - regarding the 'automatic' empty return from 'tokenization' function - this won't work here (too much per-element overhead); 
								Conclusion - one has to check the return-array length FROM THE START; 

								PROBLEM: consider this - '.indexes' becomes PARTITIONED! 
									More importantly - a given portion now becomes more about "missing" than "present" 
										non-nested bits; 
									SOLUTION: reverse the bits inside the given 'nested' bit; 
									To determine WHEN to do it - count the "missing" and "present" parts; 

							3. .batches: [any, number][] - the list of indexed elements inside the array
							4. .syncInfo: [number, number, number] - the list of REPLACEMENTS that has been done; 
								Used for increasing speed (namely - DELAYING the slow index-sync operation); 
								Has form of '[batchHappen, replacementLength, replacementIndex]'
								
								1. Because one KNOWS that for all things that are AFTER the 'batchHappen', the index is "fixed", we also know
									that only 'batchHappen' batches need be checked at all - O(k^2), where k^2 is the size of the table; 
								2. Because one KNOWS that there are '.minIndex' and '.maxIndex' for a given batch (and it's ordered), 
									one can find out whether one has the desired changed indexes - O(1)
								3. Because one has the indexes as NUMBERS, one can use binary search - O(log n)

								IDEA: for an even BETTER optimization - to have better memory consumption, a more complicated structure: 

									1. STORE THE INDEX-ADDITION INFO on a per-batch-basis' (buckets)
									2. Store them in a linearly ordered fashion (id est, so as to PRESERVE the values...); 
								
									QUESTION: how much will this book-keeping piece cost? (the 'number' triple only requires O(1))
										To do this, however, one is in need of: 

											1. get the triple; 
											2. put it into one of the 'batch-baskets' of the '.syncInfo' (based off 'batchHappen'); 
											3. in the chosen basket - '.push([index, length])'; 

											[post-creation, when calling '.sync']
											4. later - get the value from the appropriate bucket (they are ITERATED from 0 to 'curr-batch-index' for '.batches'); 
											5. apply all the transformations to the given batch; 
										
									ANSWER: not much in terms of performance; 
										ALSO - it's actually better in terms of cache-locality (because there's less jumping around); 

									ALSO: the two numbers are kept as GrowingNumber-s! 
				Result [theory]: 
				1. O(n) additional memory usage (where 'n' - number of final tokens)
				2. O(1)-O(n) performance gain (depends on number of tokenized items, least waste in cases when there's a lot)
					The current "speedy" O(n^2) (O(n) - walk through the items; O(n) - the '.splice' in replace due to index-keeping) 
						can be replaced with a "slowish" (meaning - one with a possibly significant factor) O(n).
					This ability is important, and in certain applications can be considerable; 
					Only questions are: 

						1. When can this be used? (use-cases)
						2. [Most important] Can use-cases be identified prior? 
					
					Generally, for: 

						1. Large inputs; 
						2. Large (> 15 items) tokenization-tables; 
					
					This could make some sense. 

						1. The '.splice' requires one to go SEVERAL TIMES through the 
							ALREADY TOKENIZED elements (the reason for inner O(n))
						2. The 'isType' requires performing redundant checks; 

					In expense of memory and: 

						1. Book-keeping cost (a noticeably more complex algorithm that runs in O(n) linear time + lots of index-keeping); 
					
					this implementation would permit to remove these; 	

					Final performance is: 

						O(nk^2log n), where 'n' - number of tokenized elements, k - table size

				CONCLUSION [post micro-benchmarking]: 
					1. After some little fiddling with benchmarks, one found out that (actually), 
						'.splice' is optimized to be extremely fast; 
					2. One'll need to check the optimization difference betwee this and '.splice'
						(very well may be that this turns out to be slower regardless)
					3. (Theoretically), the '.splice' STILL copies the array, even though it happens on a 
						MUCH lower-level (C++, Asm); IF it's faster (which is quite likely, indeed), 
							that is only due to the purely numeric part; 
					
					4. Important: 
						(From what is known insofar) The only "really" slow part of the algorithm, 
							that genuinely would ruin the relative performance to repeated '.splice' (which, for a large enough input would take literally forever...), 
								is the "setting up". 

						Namely, the process of "guessing" the number of 'nested' indexes [their tree]; 
						One would be required to "grow" it accordingly, with a relatively few size-increases for it...; 

					5. Before adding ANY memory-saving data structures 
						1. Benchmark for memory; 
						2. Benchmark for speed; 

						Because if either is absent, the whole thing could simply collapse due to speed/memory assumptions going horribly wrong; 

					6. FINAL: if it's slower, REMOVE; 
						First - implement and benchmark performance for some large inputs (see if faster at all, likely not); 
						Second - see the 'time-difference/memory-difference' ratio, see if it's worth it (very likely not); 
						Third - either delete, or keep; 
	
		PM1. [Maybe - add, check for feasibility + redundancy; A Big one] Add support for proper Pattern Matching;
			Serves as an abstraction over the 'RegExp'; 
			Allows one to match 'Token'-s using 'MatchablePattern'-s; 
			Would have: 
	
				[1. matching functions]
				1.1 StreamMatcher - works on Stream-s of items that are '.match(...)'-able [as arguments for 'MatchablePattern.match']; 
					Expects as a '.value'-input a 'MatchableStream', or a similar thing; 
	
				[2. MatchablePattern-s]
				2.1. [StreamClass] MatchableStream 	- walks through a given Stream of items, producing new 'MatchablePattern' on each '.match()' [ex: calls '.next()' on every '.match()']; 
					Depending on the output of the '.match' ('true', 'false', 'null', number, or PredicatePosition), either: 
	
						1. 'true' 					- goes forward 1 position; 
						2. 'false' 					- halts (mismatch)
						3. 'null' 					- goes backward 1 position;
						4. number   				- goes forward/backward specified number of positions
						5. PredicatePosition 		- goes forward/backward until specified condition on a 'MatchablePattern' is met; 
	
				2.2. FlexibleMatchable				- a 'MatchablePattern' defined by a particular user-given function; 
				2.3. [ChildrenTree] MatchableTree	- a 'ChildrenTree', that chooses one of the matching "paths" depending on the result of the '.match' function of its own; 
					Useful in combination with 'TreeStream'+'MatchableStream' - can create "matching trees"; 
	
				[Possibly, add more stuff... LOOK FOR CASES OF APPLICATIONS!]

	7. Short/Dirty notes [early - develop]: 
		PV1. Parsing and validation: 
			Need a fast approach that would combine BOTH `parsing` and 
				validation, together with the ability to recover from errors 
					that arise from parsing. 

			ORIGINAL NOTE [dirty - *clean up*, do not throw out, important]: 		
				This is a somewhat worrysome part of the library, currently, as:

					1. Validation happens separate from parsing, despite using the same abstraction (ex: ValidatablePattern and TokenizablePattern); 
					2. '1.' causes the entire process of 'validate + parse' to take 2x the time (need another loop); 
				
				[SOLUTION 1] Still, for more complex cases (in particular - those that require a per-Stream-element identification - this will not suffice); 
					For this reason, a TODO: 

						Create a new abstraction that would encompass BOTH parsing and validation; 
						Granted, it would take more time than a simple parsing procedure, 
							but (and this is CRUCIAL) it must take LESS time than split parsing + validation (in exchange for increased memory usage); 
						
					This is already doable (manually) using 'StreamTokenizer', but one would love to make it a part of the library (general abstractions + interface + algorithm); 
					From the noted above, 'PatternTokenizer' and 'PatternValidator' are a no-go here 
						(potentially, an infinite set of possible valid/invalid expressions with convoluted structure); 
					It should be a case of 'StreamTokenizer'; 

					Ideas: 
						0. It is a StreamClass (a configurable class of StreamTokenizer-s, more precisely); 
						1. Let stream elements be '<OutType>[valid, token]: [boolean, OutType]'; 
						2. Contains a field of '.validator', which returns '[boolean, OutType]', where 'OutType' is the UPDATED token (based off current one and 'input'); 
							entire return value is, then: 
								
								return this.validator(this.handler(this.input), input)
							
				[SOLUTION 2] Alternative solution [most natural approach in practice, although forces using more general grammars]: 
					1. StreamParser; 
					2. Use a table (HashMap) to see whether a given token can be (in-order) parsed to anything; 
					3. If it cannot be matched - an error/exception, one needs a specialized ErrorHandler; 
					4. Otherwise - part of the Stream; 

					Benefits: 
						1. Least possible overhead
						2. Trivial to implement
						3. Natural 
					
					Cons [problems]: 
						1. Little-to-no custom validation logic: 
							SOLUTION: add it; 
								the 'ErrorHandler' be passed the given Stream; 
						2. Forces stopping of the parsing:	
							SOLUTION: do not; 
								Add the ability to return an additional value from the 'ErrorHandler': 
									either '[false, true]' - ContinueParsing (error), or '[false, false]' - HaltParsing (error). 

					
				[SOLUTION 3] ASTValidator - for 'ASTAnalyzer/ASTNode' APIs: 
					1. Due to homogeniety, it is extremely convinient/fast to check for the "right" '.lastChild' value 
					2. Due to homogeniety, it is 
					3. [Summary] The 'ASTHierarchy' idea could be used to verify the tree post-parse

					YES, DO THIS

		CH1. To "steal" from Chevrotain: 
				1. Fault tolerance/error-recovery: 
					parsers.js has excellent error-reporting and error-representation means. 
					It has no means for error-recovery (the user has to write them ON THEIR OWN...)
				2. Incremental parsing: 
					Consider a language-server for an IDE. 
					parsers.js would NOT be the ideal choice due to: 
						1. lack of error recovery mechanisms
						2. lack of *incremental parsing* - starting parsing from ANY given point in the file: 
							You can, however, already implement it pretty briskly [pseudo]: 
								1. StreamParser/other
								2. x = StreamParser.init(...y = InputStream.init(SOURCE_STRING)...) 
								3. x.next() ...
								4. x.isEnd && x.state.error == ...
								5. [someplace else - error-type matching] if (x.state.error === ...) { ... } else if (...) { ... } ...
								6. each case - one has a separate 'StreamParser', which alters the original 'y'
								7. once, the error has been handled [id est - it's a valid symbol to continue parsing from ONCE AGAIN], goto 8.
								8. we continue with 2. via some kind of 'x.renew()': 
									To define that, one FIRST, has to work on the 'x.state.error' thing [namely - the no-validation error-discovery]. 
									There was a todo for this somewhere... [replacing 'PatternValidator' with 'parsing + validation']

			This is not an actual note, just some food for thought...
			Take these points and TRY TO IMPLEMENT THEM SOMEHOW [desirably - generic, with modularity, efficient, handling common cases]...	

		LA1. Consider an application case [missing]: large file with a NEED to quickly read/parse a PORTION of its contents: 
			Provide: 
				1. LazyFileReader - to lazily read from a file, provides a 'Stream' of symbols
					ONLY reads up to a given number of characters. 
					CAN: 
						1. jump inside the file
						2. alter the read-length (when increased - proceeds with reading)
						3. "reset" the read-length (reads next X requested characters, as if current read '.i == 0')
				2. PROBLEM: suppose that the Stream ENDS abruptly: 
					1. 'input.next()'
					2. LACKS DEFINITION - return 'input.curr' [.isEnd]
					3. error...

					SOLUTION: provide an error-handler, more specifically: 
						1. provide a Parse + validate thing (find note)
						2. provide an error-handler that CHECKS for '.isEnd', and SIMPLY returns the already parsed 
							'chunk' instead. Call this whole construction the 'ChunkParser'. 
		
	8. Unrelated/separate module/grand refactoring: 
		DE2. [Idea?] Create an error-handling API for the 'v0.4';
			Create means of: 
				1. Stacking different levels of exceptions (the 'try-catch' blocks); 
				2. Assigning relevant debug information to them;

			This is (primarily) for the development process of the parsers; 
			
			Define a Catcher [rough sketch]: 

				function Catcher (info: DebugInfo, logger: Function) {
					return function (thing: Function, thisArg: any, args: any) {
						try {
							thing.call(thisArg, ...args)
						} catch (e) {
							logger(info)
							throw e // NOTE: THIS here is to represent DEPTH [as parsers can be VERY recursive indeed, it may be needed to eliminate the recursive errors on a case-by-case basis]; 
						}
					}
				}

			And a ToplevelCatcher [rough sketch, can use to generalize the above, with 'quit = (e) => throw e']: 

				function ToplevelCatcher (info: DebugInfo, logger: Function, quit: Function) {
					return function (thing: Function, thisArg: any, args: any) {
						try {
							thing.call(thisArg, ...args)
						} catch (e) {
							logger(info)
							quit(e, info) // this is here to represent continuation of a program AFTER the exception
						} 
					}
				}

			Although... This is rather general. Perhaps, better implement as a separate package; 
			Likewise, there'd be ways to count levels of recursion, and assign types to them [specialized signature]; 	
		
		PA1. Callable [in Parser/Composition/abstract.ts] - make a dependendency, a "mini-package" named "callable"; 
			Export the function there as 'Callable'

		DP1. [Unrelated - later, spread to appropriate note-files] Deployment Pipelines for other maintained projects: 
			1. draw-text [install-npm + prod-to GitHub Pages]
				Also - ADD THE NPM DEPENDENCIES THAT IT REALLY REQUIRES - *INCLUDING* 'parsers.js'
			2. selector [to npm]
			3. regex [to npm]
			4. xml [to npm]

	9. Docs: 
	
		WE1. CREATE A proper website with documentation for the library. 
			Do benchmarks, et cetera...; 
			After doing GitHub Wiki for v0.3, see if it cuts it (spoiler - it likely won't one STILL wants the ravishing types for the library's docs to be present!)
				For this: 

					1. Learn a new CSS-based tool (either for generating/hosting docs/docs-css-styles like TypeDoc/readthedocs , 
						OR a new library to create one's own doc-styles like 'Tailwind CSS'); 
					2. Create a JSON parser in parsers.js and COMPARE it against the [https://chevrotain.io/performance/], with ops/sec. 
						See, WHICH libraries have managed to beat parsers.js v0.4., and WHAT can one do about it (see their optimizations, specifically...); 
						2.1. ALSO: create a benchmark for MEMORY USAGE [more concretely - compare parsers.js with the others...]; 
							The memory usage should be noticeably better than speed...; 

		JD1. JSDoc-s: add references
			Particularly - add the links to documentation website INSIDE the JSDoc (currently, lacks, the user has to look up themselves). 
			Reason (that it lacks currently, as of v0.3): time concerns

		JD2. Use a more complex (high-quality) JSDoc
			Current one is primitive (time concerns). 
			Add tags: 
				1. @returns
				2. @type [JSDoc-level self-reference for types]
				3. ...

		IN2. REMINDER: about the `init` method output type: 
			After it is generalized to a SINGLE interface, make FOR ALL `.init`-having method
			so that `init` returns `this`. 
	
		WI1. Add usage examples: 
			Current API is quite complex considering the need for generic functions (in many cases). 

	10. v0.5+ / future projects / great work / potentially, not worth it: 	
		65. [Much-much later; big maybe] A Stack-less CPS-based version of the library: 
			0. TypeScript
			1. InfiniteStack [from stack.js]
			2. CPSArray [from stack.js]
			3. Implements all the current items within the library using continuation-passing style

			Note: keep this for v0.5 ONLY. *Not* as a part of v0.4

FUTURE: 
	1. idea for a project - a command-line based tool for parser-creation: 
		Works thus, it allows the user to set (sequentially, one-by-one): 
			1. Enum-labels for types (uses a lazy FiniteEnum): 
				1. for acceptable tokens
				2. for acceptable tree-nodes
			2. Regular Expressions (in out Regex-flavour) for parsing different token types
			3. Grammar rules for creation of 'AST' nodes (see 'ASTAnalyzer' for v0.4)

		1. ALSO - allows a non-interactive mode

		2. Output: 
			1. parser (in JavaScript)
			2. source-generator (in JavaScript)