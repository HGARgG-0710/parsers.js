[GENERAL]
1. Check if this works with Deno; 	

[v0.3]

CURR TODO LIST: 

		[todo - fix the utils to match the interfaces]; 
		1. IndexMap [various classes, FINISH FIRST]; 
		
		[todo - fix the utils to match the interfaces]; 
		2. Collection [various implementations]; 
		3. EliminablePattern; 
		4. EnumSpace [various implementations]; 
		5. TokenizablePattern; 
		6. ValidatablePattern; 
		7. SimpleTokenType [after changing the 'TokenType' into it]; 
		8. TokenInstance (GET RID of the 'cached' bit, unneeded now); 
		9. StreamClass;	
		10. InputStream; 
		11. LimitedStream; 
		12. NestedStream; 
		13. PositionalStream; 
		14. PredicateStream; 
		15. ProlongedStream; 	
		16. ReversibleStream; 
		17. TransformedStream; 
		18. TreeStream; 
		19. MultiIndex; 
		20. TreeWalker; 
		21. MultiIndexModifier; 

	0. Find all the redundant generics + function definitions - remove them [scour the source]; 
	1. StreamTokenizer - ADD THE PROTOTYPE!	[unfixed...]; 

	2. DUE TO CHANGES OF INTERFACES [once again], fix the utils...; 
		Namely: 
			1. Initializable-related; 

	3. PROBLEM: with the NestedStream: 	

		FOR THE COUPLED-NestedStream and NestedlyTransformedStream: 

			Create a special 'FastLookupTable' wrapper-interface that 
				could be used to redirect to HashMap and/or PersistentIndexMap (generally - '.index'-having + some other methods): 

					The idea is - being capable of having O(1) operations after creating the NestedStream
						[that is - indexes are remembered for Stream-s, and their elements are checked/transformed using those; 
							This (for linear case) comes in a cost of average O(log n) [n - elements total] number of additional checks overall in a NestedStream structure to determine 
								the types of Stream-s [in linear case - O(s), where s - size of table, so it comes to O(s * log n) ONCE instead of O(ns) EVERY single layer]; 
						]; 
					
					Fundamentally, it works via methods: 

						1. '.index' - initial indexation ['inflation: [Pointer(number)?, deflation, transform | null]'] - null when no bound NestedTransformed was uninitialized, Pointer(number) only in 'PersistentIndexMap'; 
							This is potentially slow; 
						2. '.own' - "owns" the given object; 
							To be applied on 'Stream'-s; 
							In case of 'PersistentIndexMap' - stores the GIVEN Pointer(number) on them as a property for later reference by '.byOwned'; 
							In case of 'HashMap' - does nothing, it's already O(1) [assuming SimpleTokenType-s]; 
						3. '.byOwned' - secondary indexation (used by 'NestedlyTransformedStream'); Guaranteedly fast, O(1); 
							In case of 'PersistentIndexMap', this one calls the '.byIndex' based of the stored '.tableIndex' property on the NestedStream that was '.own'-ed; 
							Those are pointers, to permit changes in the table [WHICH IS SHARED BY THE __CLASSES__ CREATED - NestedStream and NestedStreamTransform VIA PROTOTYPES]; 

							Comparison "whether something is a 'NestedStream" (one of sought types)" is done via a 'hidden' prototype-level constant on NestedStream; 

						4. 'set' - this one is for changing the values of the table [adding new/replacing old]; 
						5. 'delete' - this one is for deleting items from the table by key; 
						6. 'replaceKeys' - this one is for deleting the old key, while transfering its value to the new one; 

					THE TABLE FOR the pair of classes is initialized FROM a FastLookupTable of 'inflation: [deflation, null]', where "null" is the future slot for 'transformations'; 
						Those are initialized upon calling the '.init' method OF THE NestedlyTransformedStream CLASS!: 
							VIA ANOTHER METHOD of FastLookupTable: 

								7. mutate(mutation): 

									In 'HashMap' case does: 
										this.keys.forEach((key, i) => this.set(key, mutation(this.get(key), i, this)))

									In 'PersistentIndexMap' case does the same via the GOOD OLD FOR-LOOP (reverse-order); 
										
							
							WITH 'mutation = function (x, i) {
								x[1] = transformations[i]
								return x
							}'; 

							EVERY TIME that a new 'ResettingNestedlyTransformedStream' is created (extends NestedlyTransformedStream), 
								the 'transformations' used for it are ADDED DIRECTLY INTO THE NestedlyTransformedStream CLASS'S PROTOTYPE!

		1 IT CANNOT HANDLE MULTIPLE DIFFERENT "KINDS" OF NESTED-NESS; 
			Here's an example: 

				0. A grammar consisting of 2 nested constructs - () and [], together with a primitive symbol X; 
				1. String: [([X])]

				Current NestedStream CANNOT match several different brackets; 
				CONCLUSION: need to reform to permit this
					ALSO: need the ability to extend the list of the brackets [dynamic parsing]; 
						CONCLUSION: 

							1. Pass the 'IndexMap' [interface, can actually also be a HashMap]; 
								1.1. NOTE: it's actually the 'StreamMap'; 
							2. Save it; 
							3. For each such 'type' of the given 'NestedStream', have an 'end-predicate' to end THIS PARTICULAR 
								nested structure; Then, each nested structure carries a KEY (index) using which it can 
									get the value of the reverse-predicate (for which it checks ON EVERY turn to see if 'NestedStream' is indeed over); 
									
							Problem [memory]: what about the passed table? 
								IT IS the same one; So, it would make sense to
									CACHE IT for every single part of the NestedStream; 
							
							ERGO, use a single Table for AN ENTIRE CLASS [keep inside the prototype]; 

							Form: 
								[???]: [index, inflate, deflate] // note: the 'index' is NEEDED in order to work as a key for '.byIndex' (quick access); 

							IDEA: implement this as an 'PersistentIndexMap'; 
								Basically, every time something new is added/deleted,
									the 'index' either increases, decreases or remains the same; 

		2. Once different kinds of Nested-ness ARE USED, 
			they cannot be recursively transformed; 

			The problem with TransformedStream is that it ONLY just permits 
				working with the CURRENT STREAM; 
			The user can transform the thing recursively themselves [by means of checking the 'key' + having a HashMap/IndexMap], 
				but it's somewhat convoluted [due to the fact that it's NOT POSSIBLE to express BOTH the 'NestedStream', 
					and its 'Transformed' counterpart in terms of THE SAME table (duplication reduction)];
			
			Ergo, one desires to (somehow) fix this as well... [how?]
			There must be a single agreed-upon [between two interfaces] shape for the 'IndexMap': 

				HOW to handle dynamic keys? 
				Suppose that the user changes them mid-stream...; This would (very much) ruin the synchronization between prior and current data...; 

				IDEA: Pointer [that's where it comes in]: 

					1. Have a 'PersistentIndexMap' data structure for adding/deleting items from a given 'IndexMap'; 
						1.1. Serves as a wrapper for: 
							1.1.1. calling the '.add'/'.delete' of the underlying object; 
							1.1.2. increasing/decreasing appropriate indexes; 
							1.1.3. Having its own 'byIndex', and other methods [delegates]; 
					2. Then, accept a `PersistentIndexMap`, with shape: 

						[???(indexes the curr-arg)]: [Pointer(index), inflate, deflate]

						The pointer is needed to ENSURE that the thing WORKS ALWAYS DESPITE CHANGES IN THE TABLE STRUCTURE!

						Works like: 
							2.1. When first encountering an item: 
								2.1.1. runs through the 'PersistentIndexMap', to see if it has a match (if it's nested); Otherwise - do nothing; 
								2.1.2. When it does have a match, it (automatically) categorizes it by its index, WHICH IS STORED ON THE STREAM CREATED [note: the Pointer is stored, and then used]; 
									2.1.2.1. And then, it proceeds to evaluate the Stream in question likewise (again); 
							2.2. When processing an item ('NestedlyTransformedStream'): 
								// ! This is the difficulty - HOW to ensure that the item in question is GUARANTEEDLY a Stream,
									and NOT a something that the user has implemented themselves???
								
								IDEA: Create those two IN PAIR: 
									Meaning, that the construction of the NestedlyTransformedStream is ALSO 
										dependent on THE SAME table; 

									Thus, the NestedlyTransformedStream class created would have a SPECIAL CHECK for the 
										'NestedStream.prototype' that is used;

									CONCLUSION: do 

										1. Create a dynamic semi-hidden constant on the .prototype of 'NestedStream' in question;
										2. Check for this constant in the 'NestedlyTransformedStream'; 
									
									This should give a very good speedup (as it only requires a single '===' instead of a whole 'instanceof'); 
									
									The only way that the user could 'break' it is by extending the particular NestedStream
										[or, manually adding the property's value to wherever...]; 
								
								2.2.1. Check if the given item HAS a [valid] key; 
								2.2.2. Check if it is a 'NestedStream' via hidden-constant-comparison; 
									2.2.2.1. If it is, APPLY RECURSIVELY [return a new NestedlyTransformed, based off it]; 
									2.2.2.2. Otherwise, apply the '.transform' that is passed; 

2.16. MINOR ISSUE: the 'PredicateStream' - that one DOESN'T have a very well defined 'defaultIsEnd' predicate; 
	In other words, IT REQUIRES for the given thing to be able to tell WHETHER OR NOT in the given Stream, there is
		an item with given predicate; 

	Which is something of a problem, as it requires iterating over the Stream (and is, thus, useless); 
	
	Practical solution for this: 

		{
			// ...
			const hasValue = has(pred)(stream) // changes the 'Stream' position
			if (hasValue) {	
				const S = PredicateStream(stream, pred) // the '.curr' guaranteedly becomes the '.curr' in the 'S'
				// ...
			} else {
				// ... handle the alterior case...
			}
		}

	Think whether an elegant solution may be found (or whether this is to be simply left to the docs...); 

	The issue with this is: IT RETURNS THE '.curr' NEVERTHELESS; 
		So, this means that there's always at least one element - trouble is, in this case it becomes the '.isEnd'-element [last one]; 

	Nope, no good solution for this... Just do the '.defaultIsEnd, then...'; 

2.31. Do profiling - do not stop at simple benchmarks, ANALYZE THEIR TIME AND HEAP OCCUPATION!
	Likewise, profile the previous projects - see if one's guesses (albeit, simple) 
		regarding the natures of performance issues were, indeed, correct; 

	ALSO - replace the '.filter((x) => x.length)' with new 'filter' from 'one.js'

A. testing: 
	A.1. Create list of modules to be tested; 

		Testing choice [per-method/per-class] and implementation are defined by: 

			1. Stream Classes are tested in the following fashion [they branch]:
				1.1. They branch: 
					1.1.0. Empty Stream instance
					1.1.1. Single-element Stream instance
					1.1.2. Two-element Stream instance
					1.1.3. Long Stream instance (>= 20 elements)
				1.2. There are benchmarks; 	
					2.2.1. In particular, IDENTIFY THE BOTTELNECKS IN DEFINITIONS [if there are any]; 
			2. Pattern Classes are tested in the following fashion [they branch]: 
				2.1. Empty; 
				2.2. One-element; 
				2.3. Two-element; 
				2.4. Long; 
			3. Parsers are tested as follows (with according Stream-s): 
				3.1. There is a set of Stream-s, for which they're tested; 
				3.2. With appropriate Collection-s; 
				3.3. They branch [by 4.1. and 4.2.]: 
					3.3.1. The Empty Stream; 
					3.3.2. The One-element Stream; 
					3.3.3. The Two-element Stream; 
					3.3.4. The many-element Stream; 
			4. Generally, provided an instance of a given class, its methods are tested: 
				4.1. All methods are tested; 
				4.2. If a method is mutating, there is a new instance created (a temp copy), then the operation performed on it; 
				4.3. If a method is non-mutating, it is performed without copying; 
			5. There are benchmarks: 
				5.1. Stack benchmarks: 
					5.1.1 NestedStream - find out how deep a given NestedStream must be in order for the "default" Stack to break; 
						Use as metric later
				5.2. Parse really long strings: 
					5.2.1. Try to parse some incredibly long strings;
						See the speed limits for different string sizes
				5.3. "Cached" TokenInstance vs "Non-Cached" TokenInstance: 
					5.3.1. Auto-generate a GIGANTIC tree in a simple syntax made entirely of TokenInstance-s, then do a version for BOTH cached and non-cached 
					5.3.2. Iterate through them [TreeStream]; Measure time; 
						NOTE: it is expected that the "cached" 'TokenInstance' will be A LOT faster to build/do; 
			6. Rewrite/restructure the 'regex' tests; 
			
		Left to make tests for: 

			1. IndexMap/ [specifics, RE-DO]
			2. Pattern/  [specifics - underlying]
				2.1. Token/ [specifics]
				2.2. EnumSpace/ [specifics]
				2.3. TokenizablePattern/ [specifics]
				2.4. ValidatablePattern/ [specifics]
				2.5. EliminablePattern/ [specifics]
				2.6. Collection/ [specifics]
			3. Tree/ [specifics]
			4. Stream/

				To test:

					1. Position [re-design]; 
					2. InputStream; 
					3. PositionalStream; 
					4. TreeStream; 
						4.1. TreeWalker; 
						4.2. MultiIndex; 
							4.2.1. MultiIndexModifier; 
							4.2.2. Slicer; 
					5. TransformedStream; 
					6. PredicateStream; 
					7. ProlongedStream; 
					8. LimitedStream; 
					9. ReversedStream; 
					10. NestedStream [RE-DEFINE]; 
						10.1. NestedlyTransformedStream;
						11.1. NestedStream; 

				4.0. One tests on a class-basis; 
					4.0.1. Classes are tested per-method; 
					4.0.2. A single class is a collection of methods, ergo, there MUST BE A WELL-DEFINED SET OF 
						general wrapper-tests: 
						4.0.2.1. These would be based OFF passed "checks"; These would (fundamentally) 
							compare the result of the given Stream having a given method called with given arguments, 
								with the GIVEN EXPECTED RESULTS. 
							
							The 'compare' would be class-based; 

							THERE WOULD BE SPECIAL-CASE PER-CLASS DEFINITIONS FOR IT! 
							They would be used with the actual Interfaces; 

				4.1. The tests have TO BE ORDERED [because various interfaces are intertwined]; 

					In particular: 

						1. All PositionalStream-s must come AFTER Position; 
						2. All the sub-interfaces of TreeStream must be checked BEFORE the 'TreeStream'; 
						3. "WrapperStream"-s (such as 'NestedStream', 'TransformedStream', 'LimitedStream' and others)
							must ALL be tested on different combinations/variations of Stream-classes; 

							Thus, for instance, one has to test ALL of: 

								LimitedStream-NestedStream-InputStream;
								TransformedStream-LimitedStream-TreeStream;
								NestedStream-TransformedStream-InputStream;

							And more...

				4.2. There are SEVERAL TYPES of tests for the same method:
					4.2.1. Basic functionality (comparisons, and so forth...); 
						This (typically), just calls the method once, + does some comparison work on the result; 
						These are separated by predicates used for comparison.

					4.2.2. Utils; 
						These are tested regularly via the 'utilTest'; 
						4.2.2.1. ORGANIZE THESE! 
							Let each 'util' go to its appropriate interface directory...;

							NOTE: the 'is'- utils ARE NOT to be tested ; 
								Only things like 'toInputStream', 'uniNavigate', and so forth...

							To test [only]: 

								1. InputStream/toInputStream
								2. IterableStream/isIterable
								3. BasicStream/isStream
								4. RewindableStream/rewind

					4.2.3. Constructors: 
						Make them "typical" (in the sense they uses the 'is'-functions); 

						1. For each of the 'implementations' of the 'StreamClass'; 
							These would be 'and'-s with SPECIFIC properties; 
							LIST: 

								1. StreamTokenizer (Parser/) [need both 'is' and the constructor]; 


					4.2.5. Methods:
						4.3.5.1. StreamClassInstance-specific (general): 
							[predefined]
							4.3.5.1.1. generalIsEndTest; 
							4.3.5.1.2. generalIsStartTest; 
							4.3.5.1.3. generalRewindTest; 
							4.3.5.1.4. generaFinishTest; 

							[per-case-defined]
							4.3.5.1.5. navigateTest (based off 'streamNavigateTest' + comparisons); 
							4.3.5.1.6. copyTest (based off 'streamCopyTest'; for InputStream and TreeStream); 

					4.2.6. Refactoring: 
						4.2.6.1. In tests, rely HEAVILY on 'navigate' (once tested); 
						4.2.6.2. Don't test optimizations (like 'uniNavigate') INSIDE the test-code; 

					NEXT ITEM-s: 

						1. Renew the 'ClassConstructor' function to include: 
							1.1. Ability to ensure that a given 'Stream' is (after all) BOTH creatable using 'new' AND regular function call; 
							1.2. That the properties on it are NOT OWN [that being, that the memory saving is real]; 
						2. per-case defined (.copy, .navigate); 
							2.1. .copy is for InputStream and TreeStream only (can be generalized?); 
							2.2. .navigate is for: 
								2.2.1. InputStream; 
								2.2.2. TreeStream; 
								2.2.3. LimitedStream; 
						3. Boilerplate-implementations (generalIsEndTest, generalIsStartTest, generalRewindTest, generalFinishTest); 
						4. [mini, side] Re-design the 'Position' [re-check what could have been missing - been adding some new definitions...]; 
						5. [Finally] combining the testing code together FOR EACH one of the classes, via signatures; 

						6. ALSO : add a 'streamIterator' TEST [look for other such "minor" utils, if any remains...]; 

			5. Parser/
			6. Stateful/
			7. regex/
			8. benchmarks/
			9. imports/ 
				NOTE: there has to be a WHOLE SEPARATE testing procedure for the 'imports'; 
		
	A.2. Write out dependencies to be tested for each one of them; 

		1. Classes (classes.ts) - ALWAYS
		2. Utilities (utils.ts) - OCCASIONALLY
		
	A.3. Write the tests (module-by-module); 

B. documentation (change/fix it...);
	B.1. Create new pages:
		B.1.1. Home: 
			B1.1.1. Motivation (write that the library was created/conceived primarily as a mean of refactoring of frequent parsing tasks/data-structures); 
			B1.1.2. "One good approach" principle (write that the library was made to have a single "intended" best approach for all the things that are possible to do using it); 
			B1.1.3. Examples [projects done using it]; 
			B1.1.4. Structure - explain how the library structure works (interfaces., methods., classes., utils.); 
			B1.1.5. Known issues (this lists known problems that the library currently has);
C. rewrite the CHANGELOG [too much has been altered since the "initial" v0.3]; 
	C.1. Walk through the last commit's state of v0.2.1, noting changes and writing them down [destructive]; 
	C.2. Walk through the v0.3 changes, and write them down [constructive];

Note: the new object-oriented approach will enable better speed and lesser structure-copying (better memory use); 
	PLUS: one can have the 'Streams' in question working AS TREES without the necessity for creation of the actual TreeStream-s
		(as the recursive Stream-s more or less serve the same function - only problem is populating them with the appropriate kinds of objects); 

[v0.4]

1. STACK! 
	The library is not too stack heavy, BUT it does have an (ultimate) stack-limitation
		when working with NestableStream-s and the 'nested' util (generally, anything that is NON-FLAT);
	This is very much impossible to solve on its own; 

	SOLUTION: use the CPS style. 
		Re-implement library (whilst keeping the non-CPS version) in CPS, 
		with an infinite 'stack'; 

	! THIS WILL REQUIRE FOR ONE TO FIRST IMPLEMENT THE INFINITE CPS-BASED STACK LIBRARY!
	That one should: 
		0. Be written in TypeScript; 
		Include Types: 
		1. InfiniteStack; 
		2. CPSArray (this is, ultimately, a CPS version of JS array);

	As, CPS is (quite) time-costly to actually run (due to amount of stack usage [primary] + the stack unwinding + additional checks...),
		this sort of thing would be useful for applications where speed of little importance
			(that is: NOT things like apps/websites/interpreters, and so forth; Stuff like AOT-compilers, perhaps);
	
2. Add various optimization data structures + utils + parser properties: 

	2.1. bufferize(process, size)(tree) - util

		Given a 'Tree', returns an array of values: 

			{
				childNumber: number,
				value: any
			}

		This (ultimately) does THE SAME thing
			as the 'TreeStream', BUT without losing the children's Tree-information. 
		Unlike the 'TreeStream', this is intended as
			a more persistent data structure with faster, 
			and NON-LINEAR (important) access than TreeStream.
		
		Would return a 'NodeBuffer' (which is just a kind of an array); 
		Indexing an array could be (potentially) noticeably faster than walking through a Tree 
			(particularly so with the 'TreeStream'); 

		The 'index' is the index FROM THE BEGINNING of the buffer to the current Node. 
		The 'childNumber' is the number of children that come AFTER the current node. 
			Children are read between positions from 'index' to 'index + childNumber', [where 'index' is the NODE'S POSITION! Keep it or keep it not...]; 
				with each 'child' being checked for its own children
					(which, if nonzero, are excluded from the current children, and are instead counted as grandchildren). 
		
		The 'value' is the actual value at the index (note: being 'process'ed - this is intended for removing/keeping children, primarily); 

		If size is used, it will ONLY use this many items from the tree (up to the maximum amount - the rest of positions will be assigned to 'undefined'); 

		BASICALLY, this only works for WHENEVER the 'size' is given; 

			Insted of doing: 

				const final = []
				while (!stream.isEnd && size--) 
					final.push(stream.next())
				return final
				
			It does [except, with no stream - it just walks the tree recursively]: 

				const final = Array(size).fill(null)
				const staticSize = size - 1
				while (size--) 
					final[staticSize - size] = stream.next()
				return final
			
			Which is faster, as it (basically) doesn't require running the '.push' repeatedly, 
				AND it doesn't risk having 'undefined' values; 

			Except here, the 'stream' [TreeStream] is replaced by the tree:
				so it recursively traverses it, preserving the SAME buffer and index across different recursive calls;

	2.2. TreeStream: create two versions; 

		The 'TreeStream' (currently) is the 'LL' pre-order tree traversal algorithm Stream-implementation; 
		There should be ANOTHER - the 'LR' post-order tree traversal; 

		Rename the 'TreeStream' to 'TreeStreamPre', and create an implemnetation for the 'TreeStreamPost'; 
		This is (particularly) useful for some types of interpreters; 

		The Pre  visits the node, then its children one-by-one starting from the beginning. 
		The Post visits first the children, only THEN the parent.

	2.3. Implement a 'PartialValidator': 
		This, ultimatly, works in THE SAME WAY as the 'PatternValidator' and 'PatternTokenizer' (GENERALIZE THEM TO A SINGLE FUNCTION ALREADY!)
		Gets the recursive structure with all the 'split' and 'inserted' bits; 
		The 'PartialValidator' would RETURN THE INFORMATION FOR INFERENCE OF CORRECT/INCORRECT PARTS! 
		[Which it does by 'numbering' particular portions of a given Pattern]; 
		This would allow for a greater ability to analyze/point-out the errors in the original input;

		Unlike plain PatternValidator, it would return ALL the possibly available error information, not just the stuff about the index of the "failed" check, or coverage; 

	2.4. Implement a 'level-by-level TreeStream': 

		It would get an initial given 'level' (tree root), walk through all of its '.children', 
			then walk each of the '.children' themselves as levels (one after another...), THEN descending lower...; 

	2.5. Add a 'HashClass'-implementation of 'MapClass'; 
		
		NOTE [important]: one needs to implement A DETAILED and GENERAL collision-resolution algorithm for this thing; 
			In particular, must be available: 

				1. Open addressing: 
					1.1. Via a special interface, permitting usage of different step functions `hash_addr(key, probe_index)`,
						ALSO, add implementations of frequent cases - linear, quadratic, and so forth...; 
				2. Separate chaining: 
					2.1. Via a special interface, permitting usage of different according data structures: 
						2.1.1. Self-balancing search trees (height-balanced/weight-balanced); 
						2.1.2. Arrays; 
				3. Proper resizing; 
			
			These three features are CRUCIAL for implementing '.add', '.delete' and other dynamic-parsing related operations; 
				Due to this, a simple wrapper around a primitive-based lookup table will simply not do...; 

		Basically, takes the 'hash' function (rough sketch) and returns a hash-table based off it...: 

			function HashTable (hash) {
				// The HashClass
				return function (values, keys = new Set(), default = null) {
					// The '.index' function; 
					return function (x) {
						return !keys.size || keys.has(x) ? values[hash(x)] : default
					}
				}
			}

		Great for generalizing optimizations of tables/hashes/maps...

		Implement special  cases of this ALSO (PARTICULAR hash-functions...): 

			1. LengthHash: 

				This is excellent for string-identifiers; 
				Given a set of strings (THAT CAN BE UPDATED!),
					one has a hash function: 

						(string) => string.length

				The thing in question is (supposed) to map a given string to an array/collection 
					of other strings of equal length, then map in this collection (which has a different interface...);

				NOTE: ANYTHING with a '.length' property is usable with this;

		NOTE [on optimizations]: this is especially useful when
			static-lengthed and long-term parsing tables are concerned; 
				The HashMaps are IDEAL for this because
					(on large inputs, with DISTINCT, and/or integer-keys), 
					there is A LOT of choosing between different values; 
				
			IDEA: generalize the 'TokenType'-s, by EXPOSING their 'type'-s (whenever they're 'number'-s, or 'string'-s); 
				WHENEVER they are guaranteedly disjoint, it allows for an immidiate
					one-to-one match; 

				Example of hash-table implementation based off objects [rough sketch]: 

					function ObjectHash (hash, default) {
						return {
							hash, 
							default, 
							index: function (key) {
								return this.hash[key] || default
							}
						}
					}
				
				Thus, one'd want some sort of data structure to (assuming disjoint-ness) 
					convert the given '.type'-s into HashMap-s (equivalent of 'TypeMap', but for the newly exposed '.type' not '.is'); 

					ALSO:
						one may want to implement: 

							1. hashFromKeys: 

								Basically, creates a 'hash' function from a given 'Map' (inverses it); 
									Basically does: 

										function hashFromKeys(keys) {
											const map = new Map()
											for (let i = 0 ; i < keys.length ; ++i) map.add([x, i])
											return (x) => map.get(x)
										}

								The resulting function can be used in conjunction with the 'keys' (provided they're static); 

							2. hashFromObject: 

								A way to implement the 'ObjectHash' ORGANICALLY [without the need to change the '.hash' type]: [crude sketch]

									function hashFromObject(object) {
										return (key) => object[key]
									}

									THEN, 

										ObjectHash := (object) => HashClass(hashFromObject(object))

								This is (particularly) for cases, when the given map has integers/strings for keys; 
								Useful in conjuction with the 'SimpleTokenType'; 

									ONE HAS TO BE ABLE to (somehow) get use the 'SimpleTokenType' with this; 
									Possibly?: 	

										// Parsing table; 
										const basPairs = mutate([
											[TokenTypeA, ...], 
											...
											[TokenTypeN, ...]
										], ([Type, Handler]) => [Token.type(Type), Handler])

										const baseObj = toObject(new Map(basePairs)) // known that THE KEYS are all strings/numbers + disjoint; 
										const hashMap = ObjectHash(baseObj)	

										
										TODO: implement the: 

											3. disjointKeysParseTable(tokenPairs)

												This (basically) accepts pairs of '[SimpleTokenType, Function]';
												And spits out a 'HashMap', like: 

													disjointKeysParseTable := (pairs, default) =>
														ObjectHash(
															hashFromObject(	
																dekv(fromPairsList(pairs.map(
																				([Type, Handler]) => [Token.type(Type), Handler])))), 
															default)
												
												? perhaps, make a new version for one.js that would have a new method for optimizing transformation from pairs-list to an Object? 

												That would be desired for two reasons: 

													1. Performance (when the table consists of SimpleTokenType-s and is used A LOT); 
													2. Common-ness of the pattern: 
														This thing can be VERY frequently used in actual parsers;	

												THIS SHOULD WORK IN REVERSE: instead of mapping to match the underlying hash, 
													one USES THE 'extended' function of the Hash to THEN get the value 
													from the underlying Hash [note: this would permit a greater variability]; 

				Also, there'd need to be a way to get the '.type' of the 'TokenType'-s directly; 

	2.6. Add a module SPECIFICALLY for working with strings/identifiers/Sequence-s (currently: the 'Indexed' interface); 

		In particular, functions/abstractions: 

			1. shorten(names); 
				Given a sequence of "names" (strings/sequences),
					and a 'comparison' predicate for each of its elements,
						it would implement a name-shortening algorithm, 
							that would preserve the name uniqueness;
				
				The return value is an IndexMap; 

			2. renameable(names);
				Given an IndexMap of names (which could come from, for instance, 'shorten'), 
					the method: 

					1. Re-orders;
					2. Creates new name-map entries;

				In a fashion that would allow straightforward implementation of a sequential "exact replacement algorithm" based on the output: 

					1. Loop through a list of name-maps; 
						2. [In the loop] Rename current encounters of a name with its mapped value;	
					
				There are 2 operations that may be necessary: 

					1. creation of temp-names (when collisions in present names are far too high to re-order);
					2. re-order; 
				
				The algorithm for the 'renameable' function: 

					0. Keep the cached (met) names in a 'Set'; 
					1. Loop through the 'names' given: 
						1.0. If the name is in the cached Set, continue; 
						1.1. If the current 'value' of the 'name' is already present amongst 'keys' (another loop): 
							1.1.1. put the keys in question BEFORE the current value;
							1.1.2. '.swap' the two indexes;
							1.1.3. go one position back (because now one needs to check the "others" now); 
							1.1.4. cache the current amongst the checked names in a 'Set', (so that one doesn't get into an infinite loop);
		
	2.7. Expand on the 'Position' type further;

		Allow for creation of more "exotic" types;
		Example: 

			"go up to nearest point in the past defined by predicate 'P', then another 5 positions", 

		should be something like: 

			[P, 5] // with 'P.direction = false'; 
		
		Allow "nesting" positions like so, create a proper PositionEvaluator (current thing is VERY limited); 

		The 'RelativePositionEvaluatorStream' would (ultimately) create a 'Stream' of values, defined by this "sequence"
			of Position-s (call it 'PositionSequence = Position[]'), and which would be entirely relative to one another; 
			Example: given [3, 6] it would get the 3'rd, then the (3 + 6) = 9'th positions of the Stream; 

		The 'AbsolutePositionEvaluatorStream' would (similarly) interpret a sequence of 'Position's,
			but relative to the initial Stream (not one another).
			Example: given [3, 6] it would get positions 3 and 6 relative to the initial Stream; 
		
		Then, GENERALIZE THIS to a 'PortionPosition' [which is the pair '[Position, Position]' used in 'LimitedStream'],
			to also use this with 'LimitedStream';

		So, 'PositionSequence = (Position | [Position, Position])[]' will define a new Stream based of a given one
			via the 'AbsolutePositionEvaluatorStream' and 'RelativePositionEvaluatorStream'; 

	2.8. RELAX overly demanding interface and types definitions/requirements;
		Example: inputStreamCopy, inputStreamNavigate; 

			These do not NEED to have all the parts of the 'InputStream' definition (even though they are originally implemented to work with it);
			RE-DEFINE them...;

	2.9. [Idea?] Create an error-handling API for the 'v0.4';
		Create means of: 

			1. Stacking different levels of exceptions (the 'try-catch' blocks); 
			2. Assigning relevant debug information to them;

		This is (primarily) for the development process of the parsers; 
		
		Define a Catcher [rough sketch]: 

			function Catcher (info: DebugInfo, logger: Function) {
				return function (thing: Function, thisArg: any, args: any) {
					try {
						thing.call(thisArg, ...args)
					} catch (e) {
						logger(info)
						throw e // NOTE: THIS here is to represent DEPTH [as parsers can be VERY recursive indeed, it may be needed to eliminate the recursive errors on a case-by-case basis]; 
					}
				}
			}

		Although... This is rather general. Perhaps, better implement as a separate package; 
		
	2.10. Minor refactoring (StreamClass): 

		Create a special case for the 'StreamClass' - the 'WrapperStreamClass'; 
			Basically, this works in a fashion similar to the 'InputStream', 'TransformedStream', and 'NestedStream', and others such (generally, UnderStream-s and Inputted): 

				1. There is 'iter' function (for iteration); 
				2. There is 'curr' function (for 'initGetter' and the definition of 'baseNextIter'); 
				3. Then, the 'baseNextIter' is defined as: 

					{
						iter.call(this)
						return curr()
					}

	2.11. [mayyyybee??] Try re-doing the project's type-system? [again...]

		The direct use TypeScript's 'interface's cause huge issues surrounding the 'conditional' properties
			(in particular, the need to duplicate to make everything work). 

		See, if not (mayhaps), it is feasible to re-write everything via 'extends BasicStream'-sort of thing
			(meaning, the needed StreamType-s would be "inserted" where desired...); 

		Examples [interfaces]: 

			1. TransformedStream; 
			2. NestedStream; 

	2.12. BreakableStream - a stream-class that produces instances, which : 

		1. Are capable of being "broken" via different positions; 
			1.1. can eliminate parts of the given stream, IF it's a 'number' or a StaticPosition,
			1.2. Gets a chunk of the given 'Stream' if it's: 
				1.2.1. DualPosition [a LimitedStream: NOTE: this literally builds a sub-LimitedStream]; 
				1.2.2. PositionPredicate [a property to be followed by the next chunk]; 
		2. Have a sub-stream of '.input', on which they operate; 
		3. Take in a sequence of (Position | DualPosition)[]
		
		NOTE: this is, basically, same stuff as the 'AbsolutePositionEvaluatorStream', BUT, 
			it's "not-picky" in the sense that the FIRST of the allowed occurences will be used, 
			AND, it will be used without preference; 

			Also - this works UNTIL THE END OF THE GIVEN Stream, NOT until the end of THE EVALUATED ARRAY; 

		SO, what is given is an Array, each of possibilities in which are "checked" for getting picked for 
			the Stream, then the first one getting its "go" as the '.curr'; 

		BUT, the given 'Array' will actually be several, which are ORDERED BY ATTEMPT in the following fashion: 

			1. limits[] - the array of 'DualPosition's; These define 'LimitedStream'-s [NOTE: these can contain pairs of predicates also!]; 
				1.1. In particular, this defines the predicate-limiting on the given Stream (in the sense, that, the piece matched by the first)
			2. nest[] - the array of dual 'StreamPredicate'-s used to define the nesting; 
			3. preserve[] - a set of 'Position'-s for IMMIDIATE matches; 
				3.1. These are added to the Stream ONE-BY-ONE and ONLY if none of the above [.predicates, .limits] match; 
			4. skip[] - a Position-s array; Will be used FOR SKIPPING items sequentially; 
				5.0. THIS is only matched if NOTHING from 'preserve' is matched; 
				5.1. StaticPosition: This will take the next one that is 'x: StaticPosition' '.next'/'.prev' (depending on direction) calls away
				5.2. PredicatePosition: This will skip the items FOR AS LONG as the given predicate holds...;
			5. halt[] - this is a set of Positions, that, if met/reached will TERMINATE the given Stream's consumption; 
			6. default - this is a 'special' action, if NOTHING amongst these matches, it'll do one of 'preserve', 'halt' or 'skip' [or other...]; 

		Whether the 'predicates', 'limits', 'preserve', or 'skip' are checked for first, the user can choose; 

		IDEA: 
			No, don't do that; 
			INSTEAD: categorize; That is, a list of things is given (BreakPattern): 

				1. BreakPattern = ((Position | DualPosition) | BreakPattern)[] // tree-like structure
				2. Each of the elements of 'BreakPattern' is one of 'type: "..."' (one of the above)
				3. WHEN ARRAY IS PASSED, the given thing is being 'sub-broken'; Thus, one can work through trees; 

				ABOUT ITERATION: 

					Once having achieved the '.break'-ing of a BreakableStream, one does the following: 
						
						1. keep the 'currType' property on the 'BreakableStream', which IS 
							one of the allowed types for a given thing; 
						2. Based off them, RESPOND; 
							2.1. The response can (for instance) be based off a 'TransformedStream', or 'NestedlyTransformed' stream; 
		
		ALL the 'BreakableStream'-s are RELATIVE [in the sense that the current position is used to grab the future '.curr'-s]; 
		
		These can serve as a wonderful alternative to PatternTokeniz-ing via Regular Expressions; 
		Moreover, 'BreakableStream'-s are (very much) capable of replacing sequences of operations inside of 'IndexMap'-s for StreamTokenizer-s; 
			In that, it's entirely feasible to define any operation currently present within the parser.js library using them; 

			Example: parse string: "..."

				(string) => BreakableStream(string, {})
	
	2.13. CREATE a new file 'constants': this will keep track of in-library constants; 
		Put the 'PRE_CURR_INIT', and other such things THERE; 

	2.14. [reminder] Optimize minitua (that is, just go through the code, removing unnecessary repetitions of things, useless work and so forth...); 
		When the same (even tiny) operations get repeated a large number of times, the speed decay accumulates; 
		Try to make the library code that avoids this stuff fundamentally; 

		2.17.1. DO ACTIVE MICRO-BENCHMARKING!

			Having general/particular implementations in mind, MICRO-BENCHMARK, 
				and on the basis of those, optimize particular functions/approaches; 
	
	2.15. utility - nestedDepth; 
		Finds a depth of a given Stream, based off 'inflate' and 'deflate'; 
		
	2.16. utility - findNestedEnd: 

		Given a 'ReversibleStream'/'NavigableStream', it does: 

			1. Walk through the Stream (similarly to 'nest' - counting 'depth');
			2. Finds the location, at which the end of the nested block lies (note: A NUMBER); 
			3. Returns that location, whilst also altering the location of the 'Stream' given; 

		This, basically, allows one to "know" where to end the parsing of a given thing (WHEN it's needed to be parsed LATER)!

	2.17. BufferizedStream - a wrapper around a given Stream, to enable the storage of a buffer
		for its operations; 

		Once '.next' is called, it stores the '.curr' inside the inner 'buffer', ONLY IF the user-given '.predicate'
			HOLDS!	 
	
	2.18. TypeScript - use 'enum's; 

		Learned about 'enum's being present in TypeScript; 
		Use them to designate "special constants" (ALSO - take those out into separate files); 

		Examples of applications (already present): 

			1. StreamClass - PRE_CURR_INIT, POST_CURR_INIT, POST_START...; 
			2. ValidationOutput: 
				1. null - NoCoverage; 
				2. true - Coverage; 
				3. [false, n] - ValidationError(n); // GENERALIZE THE 'TokenType' to an 'Isable', with '.is(x: any): boolean'; 
															Make 'NoCoverage', 'Coverage' and 'ValidationError' into individual 'Isable'-s, 
															identifying outputs from a 'PatternValidator'; 

															GENERALLY, make this sort of distinction BETWEEN DIFFERENT KINDS OF 'Validator'-s!

		ALSO - adapt either the SCREAMING_TRAIN_CASE or PascalCase for the constants; 
		[Current preference is to the latter...]

	2.19. Idea for a method: ProlongedStream.prolong; 	

		This (utilmately) modifies the 'this.streams'; 
		Also, there'd be '.shorten(n: number)' - it'd delete a certain number of elements from '.stream'-s, 
			optionally changing '.curr', and set '.isEnd = true' (if the .curr is in one of the deleted 'Stream'-s); 
	
	2.20. MAKE the 'utils' TYPE-SAFE! 

		Currently, what lacks is the ability to dynamically generate the appropriate type-checking functions that may require 
			additional predicates, BUT still preserve the form; 

		Just needs more boilerplate code like: 

			interface C<Type = any> {
				X: Type
			}

			const isC = structCheck<C>(["X"])
			// this 
			const isCType = (isType: (x: any) => x is Type) => structCheck<C<Type>>({ X: isType })

		Currently, the checks can make SOME mistakes still; 
			[Noticed this while doing 'isNumericPositional' - should be refactored...]; 

	2.21. [Maaayybe?] Create a 'samples' directory; 
		This is for 'common-case' parsing - generally, walk through the various syntaxes that were 
			PRESENT within the previous parsers, use them?
		
		Or, better, make this into a separate mini-project called 'parsing-samples'; 
		[EXAMPLE: handling the 'escaped' strings/sequences all the time];

		Think about it [this particular refactoring is VERY fine and case-specific, though, it can come in useful...]; 

		No, if one is to do it, the thing should: 

			1. Work with general parsing patterns, that would be CONFIGURABLE for every case; 
			2. Provide low-level Token-s with specialized given names; 

	2.22. Generics for the 'isType'-functions; 	
		Learned about TypeScript generics for nameless functions (namely, how they can be used with 'as'): 

			const isType = ... as <Type = any>(x: any) => x is Type
		
		Variation on this - add throughout the library [particularly, when 'Type' is itself a generics parameter]; 

	2.23. IDEA: generalize the type of 'T<K> = CollectionLike<K | T<K>>' properly; 
		It appears at least in 2 places inside the library: 

			1. Tree (InTreeType)
			2. NestedStream;
		
		This might (also) resolve some 'interface'-related issues for the 'NestedStream'; 

	2.24. Generalize the streams even further for general copying implementation: 

		Have to have: 
			
			1. 'props' field (StatefulStream); 
				1.1. This one '.preventsExtension'! [that is, it's a FIXED object, with a fixed number of properties];
			2. the '.copy' is then defined: 

				function uniCopy (stream: ...) {	
					const copy = new (Object.getPrototypeOf(stream).constructor)()
					copy.init(stream.props)
					return copy
				}

		The method would go on the 'StreamClass'; 

	2.25. QUESTION [minimalism]: are 'SkipParser' and 'BasicParser' really useful? 
		The former can easily be replaced with an appropriate table on BasicStream, 
			while 'BasicParser' is equivalent to a 'TransformedStream' (or NestedlyTransformedStream); 
		Consider removal...

3. Dependency management: 
	3.1. [maybe?] Create a 'proxy-op' library SPECIFICALLY for various optimizations based on Proxy-functions on different types/kinds of objects;	
		3.1.1. These (sometimes) can help GREATELY reduce memory-consumption; Example: the Slicer; Include those there...; 
