[v0.3]

Due to TODO swamp, one has decided to SPLIT all the tasks into:
	1. actually relevant (current), which are BEING DONE at the moment
	2. minor ones that WILL get done, but slightly later
	3. the "general", which may get removed/deleted/never-get-done/changed, or (maybe) done
		At worst - they get done later due to being less significant. 
	4. the "leftovers" for v0.3 - this is definitely getting done, and definitely done last - before the publishing
	5. the "v0.4"
	6. the (possibly, not so distant) "future"
	
REMINDER: Final Task Order
	2. tests [possibly - with finalization, if new redundancies are found]
		1. also - some minor todo completion [those that cause bugs, more specifically...]; 
	3. wiki
	4. FINALE [Regex, TokenizerStream, etc - FINAL NOTES]

IMMIDIATE NOTES: 
	0. reminder: the tables inside the parser [maybe some] DO NOT WORK: 
		1. need the `HashMap.currMap`, or something like this...
			The one that takes `input.curr`, anyway...
		2. refactor: 
			1. the 'if (input.curr !== X) bail(ErrorCode.MissingCharacter); input.next()': 
				Turn this into a `consumeExpected` function. 
				IT IS INTERNAL, and SPECIFIC TO `RegexParser`. 
				Reason: 
					1. improves clarity [introduction]
					2. too specific [isolation]

	1. Need a NEW PATTERN [not export/abstraction, etc]: 
		1. A user-defined `ILinkedStream`-class that is capable of: 
			1. outliving its under-source *MULITIPLE TIMES* [i.e. - a class of MULTIPLE different inputs]

	2. PROBLEM: the new pattern is impossible [but still needed]. 
		The library has a CRITICAL design flaw. 
		It is as such: 

			Provided an underlying `IStream<A>` and an upper `IStream<B>`, 
				a single iteration of `IStream<B>` must finish BEFORE `IStream<A>`
					is finished at least once. 

		This means that cross-ending operations that "combine" several 
		finished 'IStream's are FORBIDDEN! [which means that an entire 
		class of parsers (quite a lot of them immensely popular) just 
		flies out the window]. 

		The root of the problem is highly non-trivial. 
		In order for one to update the status of the underlying stream's, 
		the INVOCATION of the "checking method" of the RecursiveList 
		[or StreamList, or CompositeStream, or whatever...] must be made 
		FROM WITHIN the 'IStream<B>' in question.  

		(Proposed) Solution: 
			1. make the 'IStream' in question 'IStateful' 
				(i.e. - add `readonly state` property 
					and a `setState` method). 

					This way, we'll be able to access the 
					'this.state.parser' inside of the "chooser"s 
					with `this` being the `CompositeStream` already 
					"in the know" to the parser's Global State. 

			2. (If inside the chooser, and not standalone), 
				Provide the state to the underlying `IStream`
				inside the chooser via the respective `.setState` call. 
			
			3. Provide a new (public) method for `RecursiveList/StreamList/CompositeStream/whatever`, 
				for renewing a SPECIFIC `IStream`: 
					0. name it `renewItem(item: T, ...)`
					1. used as `this.state.parser.renewItem(this)`, 
						from inside [say] `NamedGroupStream`, with `this`
						being a `NamedGroupStream`. This'll allow one to 
						actually change 'this.resource' CORRECTLY, while 
						preserving encapsulation, AND without violating any 
						of the previously outlined design. 
					2. Algorithm: 
						1. Start at `item`
						2. Walk downwards [WITHIN THE BOUNDRIES OF THE SAME `Switch`-contained COLLECTION!]
							until an (updatable) `x: Recursive` which `.isOld(x) === true`; 
						2.1. If such an item CANNOT BE FOUND, then one THROWS AN EXCEPTION of misuse 
							of `renewItem` (id est - item CANNOT be explicitly renewed)
							
						3. If such an item CAN be found [i.e. BOTH `x: Recursive`, AND `.isOld(x) == true`], 
							then - we trace to THE LAST non-`.isOld` item [STILL inside the current `Switch`-collection], 
							and update all the remaining ones ACCORDINGLY, up to the current item [`this`, the thing 
							we called the `.renewItem(this)` on]. 

						3.1. IF we cannot find a single non-`isOld` item within the current group, then it is deemed 
							FINISHED, and we throw the error - because the item in question *cannot* be renewed, and 
							is, therefore, since it's a part of its definition, malformed as a Stream. 

			4. Provide a respecitve shortcut (a delegate method)
				for `DynamicParser`, like, say, `DynamicParser.renewItem(item: ILinkedStream)`.

	3. REMINDER: about inheritance. 
		Some of the derived IStream-classes employed in the `RegexParser`
		violate LSP. Specifically, the ones that OUTLIVE their sub-streams. 
		They mustn't inherit from `DyssyncOwningStream`. 

		Instead:

			1. Let they inherit from ALL of the stuff that `DyssyncOwningStream` 
				does, EXCEPT for `DelegateStream` [from `PipeStream`]. 
			2. MAKE this *new* combination into a new `mixin`-export 
				inside the `Stream` submodule...

notes on `RegexParser` development: 
	-2. About empty expressions: they aren't allowed (always false). 
		This is why "|a+b?" doesn't match empty string. 

		Reason: 
			1. makes no sense, this is an ad-hoc-match regex-engine, not a fulltext one
				Allowing matches for an empty string can only produce infinite loops 
				in the usage context in question. Forbidding it makes for simpler parser code. 

	-1. About Pooling: 
		1. Add 'ObjectPool's to this. ESPECIALLY for `IStream` objects created. 
			1. NOT THE tree-objects, though. Those are intended to be persistent and distinct

	0. About Error-Handling: 
		1. Non-error-resistant
		2. Throws upon encounter of unacceptable characters
		3. The error Stream is *very* "low" - right above the 'InputStream'; 

		4. CURRENTLY, it "skips" (as in - ignores) erronous segments. 
			LET IT REACT TO THEM! 
			Particularly - it must *EXPECT* a character [vital, since doing otherwise would completely change the meaning of ' ' symbol in some contexts
				Example: 's {name}' - VALID SIMPLE MULTI-CHAR REGEX; whereas - 's{name}' - a single type-match]; 

		5. VITAL: 
			1. Add a `PosStream` at the beginning [reason: for a better error-message]
			2. See `RegexParser.ts` file regarding the need for a whole separate
	1. Based off `IStream`-s
	2. Input: 'InputStream' (since it's supposed to be used by the user in-program, and the strings are (supposed to be) relatively small)
	3. To replace the `TokenizerStream` itself [since we cannot use `Regex`] here, one uses `HandlerStream` + `StreamHandler`

	4. Syntax: 			
		1. Disjunction [a | b]
		2. Root Node [.type: "regex-tree"]
		3. DISCLAIMER: [important] `Regex` assumes matching the given `IStream` 
			FROM ITS CURRENT POSITION. Id est, it really doesn't make sense to have, say, 
			#, or :, because it matches AS MUCH AS IS NEEDED, and FROM THE POINT THAT IT STARTS. 	

		DECISIONS [hard]: 
			0. IMPORTANT: for performance reasons - USE THOMPSON'S REGEX ALGORITHM!
				Read this: 
					1. https://swtch.com/~rsc/regexp/regexp1.html
					2. https://cs.stackexchange.com/questions/138275/worst-case-complexity-of-quantifiers-in-thompsons-construction
					3. https://en.wikipedia.org/wiki/Thompson%27s_construction#Use_in_string_pattern_matching

				Don't worry, it's an NFA, there won't be memory reprecussions...

				Also [reason for stable performance] - this thing has NO BACKTRACKING [at all]. 
				Bad news are: features are somewhat more limited, 
					as for those that ARE still present, but aren't supported 
						by the construction, one will have to extend it to support: 

						1. Lookahead/Lookbehinds ((=...) and (<...))

				To make sure that what you have is implementable with the Thompson's NFS, 
				use a modification of 'awk' Regexp syntax: 
					https://www.gnu.org/software/gawk/manual/html_node/Regexp.html
			1. no backreferences
				Reason: https://swtch.com/~rsc/regexp/regexp1.html
				That means NO NAMED GROUPS!
			2. no capture groups
				Reason: pointless. 
				Currently, let there be JUST the no-capture groups.
				[REMAKE the '(:...)' into just '(...)' then...]
			3. LET 'linefeed' '\n' match be a NEWLINE-MATCH, that is - WITH CR (carriage return - \r) on Windows!
				Reason: 
					1. simplicity
					2. 99% of all applications will benefit from this [easier crossplatform support, etc...]: 
			4. No ':#' (or, in JS flavour, '^$')
			5. Negation ONLY exists for character classes: 
				Replacing the '[^...]' with '^[...]' [more readable]
				This means, among other things - NO negative lookahead/lookbehind! 
				The other things are just: 
					1. too much work
					2. poor support from Thompson's Construction - would have to add one's own extension
					3. [key reason] Limited practical usefulness

			6. About Lookahead/Lookbehind-s: 
				1. Those aren't supported by the Thompson's Construction by default
				2. They enable one to do some (fairly) useful (*high-level*, on the 's/i{type}'-level) things 
					with regular expressions. Most notably - emulate partial capture based off `LimitStream`s. 
				3. Implement them as an extension of sorts: 
					1. They USE `PeekableStream`s to work [without `.next()/.prev()`-based backtracking]; 

			7. no unicode properties (\p, \P) from JavaScript [unneeded]
				[\u{....}-\u{....}] is sufficient

			8. no 'v' functionality [unneeded, too much complexity]

		KIND-OF DONE [see notes]: 	
			23. [sort of - 'Range' incomplete, sketch] *, +, ?, {n}, {n, k} - operate the same as JavaScript
		
			5. [kind of done - NEEDS IMPLEMENTATION OF NEGATION!] \b, \d, \s, etc [from JavaScript RegExp]
				But NOT the negative forms [excepting `\B` here...], which are supposed to be replaced by: 
					1. \S == ^[\s]
					2. \D == ^[\d]
					...
			24. [kind-of done - sketched out, to be finished] [abc-z...] - non-negative character classes - same as JavaScript + allows for ^-negation inside	: 
			2.  [kind-of done, put it on hold for now...] '^...' - negation of '...'
				NO ELEMENTARY [^...], replaced by composite ^[...]. 
				In /^ab+/, same as /(:^a)b+/
			7. [kind-of-done, put on hold for now...] Backslashing AS-IS-NECESSARY [like a simple '(\=...)', or, generally, `\^` for instance...]
			22. [kind-of-finished; leave alone for now - first need general structure of the parser] Capturing groups [besides (:...)]: 
				1. unnamed [regular]: (...)
				2. named: ({name}...)
				3. non-capturing: (:...) - same as (?:...) in JavaScript [replacement]
				4. lookahead: (=...) - same as '(?=...)' in JavaScript [replacement]	
				5. lookbehind: (<...) - same as '(?<=...)' in JavaScript [replacement]	

			10. [kind of done - finish the errors later]\u{HHHHHH} (1-6 digits, REPLACES the \c, \x and multitudes of \u in JavaScript RegExp); 
				1. NEW ERROR - must verify that the resulting 6-digit value IS HEXIDECIMAL! 
				2. NEW ERROR - must verify that there is STRICTLY between 1 and 6 values! 
				3. REMINDER: a new stream [general - for the library] - `AssertionStream`: 
					1. Takes a given underlying-stream
					2. Runs a predicate on it
					3. Puts the predicate in question through `assert(...)`
					4. [Expected] The resulting error (if there is any) gets picked up by a higher-level `ErrorStream`
		

CURRENTLY BEING DONE [screw the previously chosen order - going after the `TokenizerStream`]: 
	TO2. [new IStream] `TokenStream` - based off `Regex`: 
		IMPORTANT NOTE: 
			must be CAPABLE of allowing the functions in question to OPERATE upon the `.pos`
				of the stream that is BEING TOKENIZED. 
			Reason: oftentimes, one will not want the contents of the thing, BUT THE POSITION, instead. 
				Thus, the functions in question should (additionally, optionally) be given the `.pos` of the match; 

		1. takes in a table of `[Regex, StreamHandler]` 
		2. iterates the table for each `.curr`
		3. calls `StreamHandler` on the `.value` [takes in a `.value: IStream`]
		4. repeat 1.-3. until `.value` is finished [tokens are produced as `this`]

		This is immensely useful for tokenization via regular expressions; 

		More specifically, it takes: 
			1. an `IndexMap` [or another kind of table... maybe just a `Pairs`] of tokens
				1. VITAL: as `DefaultType` has an "elseFunc"  handler (useful for providing error-messages, etc)
					The "elseFunc" is MANDATORY, since it dictates what is to be done if an item cannot 
					be satisfactorily mapped [clearly, SOMETHING has to be done, `undefined/null` are non-options here]. 
			2. then - it walks through its items for a given index, trying to `.match` the respective `Regex` object
			
		IMPORTANT NOTE: 
			1. The `TokenStream` must have a candidate-elimination system. 
				More specifically, consider the following (failing) implementation strategy: 
					1. one uses `.peek()` [alone] to determine which token to accept next
					2. due to that, since `peek()` is *limited* by length of the `RotationBuffer`, 
						one has a limitation on size for tokens-'.value's
					3. consider good ol' /[a-zA-Z]/; This has arbitrary length. qed
			2. Thus, one needs a combination of `.peek()` and `.next() + .prev()`. 
				Thus, we do: 
					1. peek(n) for MULTIPLE of them
					2. keep going, until one of: 
						1. the set of those that can be acceptable is 1
							1. switch to `.next()/.prev()` [.prev() ONLY if there's an error, and this (ends up) not being a match either]; 
						2. we run out of `n` for `.peek(n)`
							1. switch to `.next()/.prev()`
							2. `.prev()` ONLY when an error is encountered. 
				
				Thus, the candidate elimination system: 
					1. no backtracking
					2. max-possible-usage of `.peek()`
						once out-used, `.next(n)` for max `n` for `.peek(n)`, 
						and then continue with `.next(1)`
					3. if no matches are found, backtrack to the beginning using `.prev(k)`, with total chars `k`, 
						and herald error
					4. else, continue until there's only 1 viable candidate
					5. IF there are 2+ viable candidates, and one is fully reachable sooner than others, then IT is chosen
					6. IF no single viable candidate can be produced as per rules 1-5. (id est, there is 2+), 
						throw an exception [this is a PARSER error, not the input error, end-user shouldn'e need to worry about it, 
							the parser-maker (direct library user) DOES
						]
	
	TODO [*important*]: About `TokenStream` and `Regex`: 
		1. need an INTERMEDIATE abstraction - a `MatchList`
		2. a `MatchList` is a "table" of `handler-Regex` to be walked, and USED by the `TokenStream` to MATCH one of avaiable `Regex`-es
		3. a `MatchList` is an `IndexMap`; Functions in it have access to `this` - THE CURRENT TokenStream, and `input` - first arg, and 'parser.table' - second arg, LIKE with `TableMap`; 
		4. a `TokenStream` is just a `(matchList: MatchList) => StreamClass(TableMap(matchList))`
		5. since `TokenStream` is JUST a `StreamParser`, it ALONE cannot provide one with full `laziness` in all cases (ex: recursion)
		6. FOR ENABILING full laziness, one NEEDS choosers, since they allow one to parse items of arbitrary depth: 
				b + a + (b - (...)), ... # and so forth - only characters are: a, b, +, -, (, ) and ','
			can be parsed like: 

				# very rough sketch - incomplete, need to solve the issue of DEALING with the `recursive ()`-aspect of it all. 
				1. ReadableSource
				2. InputStream
				3. SkipSpaces [in `samples`]
				3. TokenStream(+, -, a, b, (, ), ,) - provides TokenNode abstractions
				4. chooser0:	
					1. TRY FINDING a matching pair for '/(/': 
						input.next() # skipping '('
						return [
							SingletonStream(BRACKETED_RecursiveNode_func), # a RecursiveNode, with 1 element - the sum; 
							chooser1, # "gathering up" all the "+", etc
							chooser0, # recursion - bracket-expression INSIDE the bracket
							LimitedStream(EndBracketTokenPred),  # predicate of '.type === EndBracketType'
						]	
					2. all else - PASS AS-IS
				5. chooser1: 
					1. based off TableMap:
						1. based off an MatchList:
						
							0. MATCHING - Peek(1) (approx, in `.type`s) -- 1/[+-]/: 
								const elem = input.next() # a | b | (...) - last one hadled by `chooser0`
								const sign = input.next() # - | +

								# the `Summand_Curried(elem, sign)(elem1) = { type: sign, children: [elem, (elem1.type == sign ? ...elem1.children : elem1)] }`
								# we are TURNING the `Stream`-s into a SEQUENCE of ONE-ELEMENT LAYERS TO BE EVALUATED LATER; 
								# which are THEN - flattened to obtain a single `.children` array for the arithmetic expression; 
								return [
									SingletonStream(Summand_Curried(elem, sign)), 
									chooser1,
								]

							1. MATCHING - .curr 0/[ab]|#[BRACKET_TYPE]/ [with no prev. match of 'peek(1)']
								const elem = input.next() # skipping; NO +- AFTER the current one; Has to be end on the next [or, an ERROR]; 
								return [
									SingletonStream(Summand), # `Summand` - a RecursiveNode with `.children.length === 1`
									chooser1 # end-recursion - 'IS_END'/Error ahead [NOTE: this needs to be SUBSET - break into `chooser1` and `chooser2` (lesser - without recursion)]
								]


							3. IS_END: 
								return []

							4. is ',' - return a `SingletonStream(id)` - passing those as-is; 

							5. ERROR/ERROR-RECOVERY - invalid token provided

					2. EACH TIME that the `chooser1` is FINISHED, we have successfully parsed AN OPERATION
				6. PredicateStream - pick those that are not a ','
				
	RE2. `Regex` description:
		Then: 
			3. write proper docs for the class

		It will improve upon the "simple" JS regex in the following ways: 
			7. get rid of the flags: 
				1. 'g': 
					replace the different behaviour with METHODS; make it non-exclusive

				6. 'y' (????): 
					find out what this even is (and what it's used for)
					seems very much like a hack for 'regex.match/exec/search/whatever(string.slice(x))'...
				 
			IMPORTANT NOTE[1]: 
				1. `Regex` works with BOTH: 
					1. string
					2. IStream<string>: 
						reason - to allow implementations for faster/lazy algorithms

			IMPORTANT NOTE[2]: 
				1. .matchAt(x: Indexed<string>, i: number)
					Checks that AT THE GIVEN `i` IN `x`, 
						there is AN IMMIDIATE MATCH. 
					This is important, as it allows the "tabular" work that the TokenStream does
				
				2. .toString(): 
					Returns the original 'source' that was given for parsing...
				
				ALSO: the `Regex` implements an NFA

IMMIDIATE [first - order 'import's, then - do 'docs']: 			
	CURRENT AGENDA: 			
		P0: 
			2. Types: 
				1. Since we have made it that `IStream` contain the `Iterable` [and `IInitializable`] properties OPTIONALLY: 
					1. Take it further, and REMOVE the mention of ALL the "Partial" properties (that includes `ICopiable`). 
						The 'ICopiable' stretches across THE WHOLE of the library...
						This way, one would have: 

							// THE PERFECT SOLUTION [~1yr worth of suffering around that bloody thing...]
							export interface IStream<T> {
								readonly isEnd: boolean
								readonly curr: T
								next(): void
								isCurrEnd(): boolean
							}
							
					2. Remove the `.prev()` method and all related functionality. 
						Backing up is NEVER NECESSARY when parsing. It is much simpler
						and more efficient to use a `.peek(n)`-based lookahead. 
					3. Make the presence of desired properties on specific implementations EXPLICIT via interfaces. 
						Introduce LOTS 

		P1:
		[this is just an array-like strucutre, nothing unusual to see here]
		1. LiquidMap
		1. RetainedArray
			TODO: refactor this [ugly]: 
				1. NO MORE 'MixinArray'
				2. add sub-objects [composition] to represent respective business logic. 
		2. `RotationBuffer` - test it

		P2:
		[Single 'ClassTest']
		38. `ArrayMap`, 
			`BasicMap`, 
			`CharCodeMap`, 
			`PredicateMap`, 
			`RegExpMap`, 
			`SetMap`, 
			`ObjectMap`
		[Single `ClassTest`]
		8. `BasicHash`, 
			`LengthHash`, 
			`TokenHash`, 
			`TypeofHash`:
				1. DO NOT MOCK! [real dependencies are far too simple for that]
		6. WrapHandler, TableHandler

		P3: 
		17. `ResourceManager`: 
			1. USE MOCKS FOR THIS ONE!
			2. ALSO - the `.manager` properties of `WritingDestination` and `ReadableSource`
		2. ReadableSource
		3. WritingDestination
		4. Encoder[8, 16, U8, U16]
		5. Loaded[8, 16, U8, U16]

		P4. STREAMS - none of the "hard" stuff yet... Just data manipulation

	TODO: classes from /internal to test: 		
		[easy, but longer]
		6. SwitchArray: 
			1. USE A MOCK HERE (`RecursiveRenewer`)
			2. Do only after `Switch` has been thoroughly tested
		
		[not easy, but not too hard - do (slightly) later]
		5. Switch [from RecursiveList.ts]
		
		[HARD - do later]
		4. StreamList
		8. TreeWalker: 
			1. USE A MOCK HERE (`TreeLike`)

	TODO: public classes/functions/utils/etc to be tested: 
		-1. ParseError [abstract]: 
			Test will use a descendant to check that: 
				1. '.name' is correct
				2. '.message' is correct [for various inputs]

		[easy]
		0. TableMap
		2. `Autocache`: 
			1. USE A MOCK HERE! [ISettable & IIndexable]
		
		[hard]
		3. `Decoder8`, `Decoder16`, `DecoderU8`, `DecoderU16LE`: 
			1. DO NOT use mocks - instead, create REAL FILES [encoded appropriately]
		4. `WritingDestination`: 
			1. DO NOT use mocks - instead, utilize ACTUAL `IEncoder` classes from the library. 
			2. Use only 2 different classes' implementations - assume that this is equivalent 
				to it being sufficient. REASON not to use mocks - very hard to do here, since 
					that would (in essence) demand one to actually re-implement one of the 
						already present `IEncoder`-implementing classes of the library (since
							the encoder must be A WORKING ENCODER). 
			3. TEST `IEncoder` classes BEFORE TESTING THIS
		5. `WritingDestination.manager`: 
			1. This is tested DESPITE `ResourceManager` being tested as well
			2. Use REAL-LIFE files with this thing
		7. `Encoder8`, `Encoder16`, `EncoderU8`, `EncoderU16`

		[very hard]
		6. `DynamicParser`: 
			0. Do ONLY once `TokenStream` and `Regex` has been finished and tested
			1. Test ONLY as a part of "demo"-testing: 
				1. +/-, () and [a-zA-Z]-variables (no self-modification)
				2. A Self-mofidying parser
			2. This is an *INTEGRATION TEST*
			3. VITAL: besides this, ADD a "demo"-style integration test 
				for `SourceBuilder + IndexMap + DepthStream + INode + NodeSystem` for 
				GENERATION of the "+/-, (), [a-zA-Z]-vars (no-self-mod)"
				grammar sources BASED OFF a Node-based AST. 
		
		[easy, but tedious]
		8. `BasicHash`, 
			`LengthHash`, 
			`TokenHash`, 
			`TypeofHash`:
				1. Test each independently
				2. Use DIFFERENT `IPreMap`s for each case
				3. DO NOT MOCK! [use real `IPreMap`s instead]: 
					reason - as always, too simple
					this would require re-implementing A LOT of stuff for no good reason...
		
		[easy, but tedious]
		9. `IndexMap.ts` stuff: 
			1. The functions - test each one
			2. Each one should be given a SINGLE case of an `IMapClass/whatever` to work on
			3. Each one given SHOULD BE REAL!!! [no mocks]

		[semi-easy]
		10. `LoadedU8`, `LoadedU16`, `Loaded8`, `Loaded16`: 
			1. Use real files
			2. NO MOCKS

		[medium - no sooner than 'ObjectPool']
		11. `TokenNode`, `ContentNode`, `RecursiveNode`: 
			1. Use each one with a 'string/number/boolean' '.type': 
				1. create several instances for each one created class
				2. they should handle edge (if any) cases and generic behaviour
			2. type `T` of '.type: T' should be different each time
			3. CHECK FOR MEMORY-SAVINGS [`.prototype.type` and NOT `this.type`!!!]

		[semi-easy - no sooner than `Node`]
		12. `NodeSystem`:
			1. Use `samples.Nodes.PlainNodes` for this test
			2. NO MOCKS
			3. Test ONLY after the `TokenNode`, `ContentNode`, `RecursiveNode` have already been

		[easy]
		13. `ObjectPool`: 
			1. Create a mock class
			2. Use `ObjectPool(MockClass)` on it
			3. Create some instances - test it...

		[easy - no sooner than `ObjectPool`]
		15. `TypedPoolKeeper`

		[easy]
		16. `PropDigger`: 
			[VITAL: add these as TEST CASES in `PropDigger/cases.ts`!!!]
			1. `ownerDigger`: 
				1. on an `IStream`
				2. ONLY AFTER an `ILinkedStream` has been tested
			2. `resourceDigger`: 
				1. on an `IStream`
				2. ONLY AFTER an `ILinkedStream` has been tested

		[easy]
		17. `ResourceManager`: 
			1. USE MOCKS FOR THIS ONE!
		18. `RetainedArray`

		[above-medium]
		19. `ReadingSource`: 
			1. Use real files
			2. DO NOT use mocks for `IDecoder` - use real ones: 
				1. 2 classes, specifically
		
		[above-medium]
		20. `ReadingSource.manager`: 
			1. Test this also, DESPITE `ResourceManager` tests

		[easy]
		22. `TableHandler`, `WrapHandler`: 
			1. Test this NO SOONER than the respective `indexable`s
			2. Use REAL `IIndexable`s of the library: 
				1. MapClass-based
				2. ModifiableMap

		[medium-hardish]
		23. ModifiableMap: 
			1. plain index test
			2. modify + index tests: 
				1. one for *EACH* of the methods of `TableMap`.
			3. DO NOT test with mocks - use REAL objects: 
				1. `TableMap`
				2. One `MapClass`
				3. REMINDER: per `MapClass` - only ONE suite!
		
		[easy, tedious]
		23. `aliases/`	- test everything
		24. `utils/` - test everything
		25. `is/` - test everything
		26. `aliases.ts` - test everything
		27. `constants.ts`: 
				1. THESE HAVE SEMANTIC MEANING!
					1. `MissingArgument` is supposed to be behaving like `undefined` ALWAYS
					2. `isGoodIndex(BadIndex) === false`, `isGoodIndex(BadIndex + 1) === true`
					3. Test NOTHING MORE HERE
		28. `is.ts` - test everything
		29. `utils.ts` - test everything
		30. `samples/` - test everything (except for `samples/regex/refactor.ts`)

		[easy]
		31. `modules/Stream/utils/StreamPosition.ts` - test everything

		[easy, tedious]
		38. `ArrayMap`, 
			`BasicMap`, 
			`CharCodeMap`, 
			`PredicateMap`, 
			`RegExpMap`, 
			`SetMap`, 
			`ObjectMap`
		
		[hard-ish]
		39. CompositeStream
			1. cases: 
				1. many diverse streams [fewer; for greater confidence] 
					1. This one would be simple and include only data-manipulation Stream-s
				2. many diverse streams [more]	
					1. This one would include a `FreeStream`
					2. This one would include a `WriteStream`
				2. one stream only 
				3. no streams 

		[easy-ish; varies wildly]
		40. ConcatStream
		41. DepthStream
		42. FilterStream
		43. FiniteStream
		44. FreeStream: 
			0. [See below] DO NOT Use REAL `ObjectPool` here [*manual* mock-objects instead...]
			1. NO SOONER than `ObjectPool` and `IPoolGetter` are tested
			2. [maybe???] Add a framework for 'spying' on methods of objects here?
				Reason: one needs to ENSURE that the `.free` method *GETS CALLED*, 
					no matter what. Reason being: if we DON'T know that, we DON'T 
						know if the thing is actually *effective*. 

				Thus, possible solutions to the dilemma: 
					1. add a library [with existing like 'Jest' - good for resume]
						This'll additionally require: 
							1. accessing of `ObjectPool.prototype`, saving reference
								To save the '.freeMethodObservable = jest.spyOn(ObjectPool.prototype, "free")'
							2. checking the presence of the call on each iteration inside 
								a test of `FreeStream`, via the `.freeMethodObservable.mock.calls[i]`
					2. creating a fake `ObjectPool`-like object, which would register the `.free/.create` calls, 
						which would be then employed as-desired
						Then, one would need to: 

							1. create an interface for `IObjectPool`
							2. use it EVERYWHERE instead of the "ordinary" 'ObjectPool' classs objects: 
								1. That includes `FreeStream`
							3. write a dummy-mock-class for a specific type of "poolable" objects, create-d/free-d inside the `FreeStream`
							4. use the dummy-mock-class's object with `FreeStream`, executing the necessary methods appropriately
				
				CONCLUSION: do the 2. - interfaces + mock objects here would be *truly* WONDEROUS. 
		45. FreezableStream
		46. HandlerStream
		47. IndexStream
		48. InputStream
		49. InterleaveStream
		50. LazyStream
		51. LimitStream
		52. LoopStream
		53. MarkserStream
		54. PeekStream
		55. PosStream
		56. SingletonStream
		57. WriterStream
		58. BasicErrorStream [abstract]: 
			1. will define a child-class
			2. will check that it works as-intended:
				1. sample 1 - with a case-specific `ParseError`-descendant
				2. sample 2 - a (simple) fault-resistant parser
		59. DefaultErrorStream: 
			1. with bare ErrorStream
			2. with `BasicErrorStream`

	TODO: BENCHMARK: 
		1. RotationBuffer vs 'shift/unshift'-based solution: 
			0. Generalize the `PeekStream` to a common "abstract" ancestor
				1. which would enable one to "insert" an instance of the interafce used by `RotationBuffer`
			1. Make a FakePeekStream, that utilizes a `shift/unshift`-based solution [put in `src/benchmarks`]
			2. Compare the two on a large [and relatively simple] input that utilizes `TokenStream`, with : 
				0. benchmark 0 - 0-1 lookaheads [not very ambigious at all]
				1. benchmark 1 - 0-3 lookaheads [somewhat intensive/ambigious - many times - doesn't require any lookaheads]
				2. benchmark 2 - 3-12 lookaheads [IS intensive/ambigious - many times - DOES reuqire lookaheads]

				IF there is *any* performance benefit, either: 
					1. replace the 'RotationBuffer'-based version with 'shift/unshift'-based
					2. make it so that the `PeekStream` uses appropriate 

	TODO: test each (public) export: 
		AUTOMATICALLY - let they be imported via the "."-path notation. 
		Like `import {samples}` + `samples.Node.PlainNodes`, instead of just `import {PlainNodes}`

	TODO: Tests
		1. TEST the internal/ (only what makes sense)
		2. Test the public exports: 
			1. ALL classes
			2. ALL utils
			3. ALL samples
		3. Test that the LIST of library exports is correct: 
			1. EMPLOY the user-end `deep import` for EACH tested export of the library FOR its tests
				Ensures that ALL the needed exports are available
			2. WRITE tests that verify the lists of exports for each DEEP export (export-tree-test, let's call it...); 

		VITAL NOTE: employ the new `ClassTest + MethodTest(s)` model. 
			Reason: allows for using the `ICopiable` to write tests in a much more concise fashion, 
				treating each class instance as a "test starting point", from which one can test various methods. 
					Saves A LOT of boilerplate that is (usually) written when doing tests. 

NEXT UP:
	TODO: `.copy()`-fixing [IStream-implementations]: 
		1. fix the `.copy()` methods' implementations that employ the `.set*`-methods/custom initializers
			
			2. BasicStream
			4. ConcatStream
			5. DelegateStream
			6. DepthStream
			7. FilterStream
			8. FiniteStream
			9. FreeStream
			10. FreezableStream
			11. HandlerStream
			12. IndexStream
			13. InputStream
			14. InterleaveStream
			15. IterableStream
			16. LazyStream
			17. LimitStream
			18. LoopStream
			19. MarkerStream
			20. PeekStream
			21. Position
			22. PosStream
			23. SingletonStream
			24. TrivialStream
			25. WrapperStream
			26. WriterStream

			Reminder: for `CompositeStream` - DO NOT IMPLEMENT!!!
				Reason: *unbelievably* complex...
				
	TODO: [IStream 'mixin's] FIX the `mixin`s that INHERIT other mixins/classes, to USE 'this.super.constructor' MANUALLY!!!
		One cannot stress this enough. 
		Some don't need it. 
		Some do. 
		Either way, WHENEVER this is necessary - one MUST make this explicit...

	TODO: in alll the `mixin`s - FIX THE (old) `super` calls!!! 
		They MUST become 'this.super[ParentName][propName][?...].call(this, ...args)'; 

	TODO: about `annotation`s. 
		CHANGE them so that boilerplate due to mixins is MINIMIZED. 
		Let *most* influential ones be inherited INSTEAD, and others be copied. 
		[Walk through all of them - fix this...]; 

	TODO: in 'mixin's - ENSURE that constructors are set as 'constructor: function () {}', 
		and NOT `function () {}`

		OR [yes - do that instead...]: 
			TODO: 'mixin' module (`sealed` class): 
				1. ALLOW setting `constructor` WITHOUT needing a prototype. 
					In such cases, CREATE a prototype, via the `constructor = {}` INSIDE the `sealed_mixin`
						class...

				AFTER you do that, REPLACE all the 'constructor: function (...) {...}' with 'constructor(...){...}'

	TODO: REMOVE unnecessary 'Args' generic type parameters [sometimes they are just better replaced with '[]']; 

	TODO: on `let smallCaseStream: ... = PreStreamName(...); .generic = ...`; 
		REPLACE THESE in places where you use the FACTORIES instead with `const localSmallCaseName = BuildSomeStreamName<T>(...)`
		Reasons:
			1. less boilerplate [ergo - simpler code]
			2. same typing

		THIS GOES FOR '_SomeName' AS WELL: 
			1. HandlerStream
			... OTHERS [walk through the project - FIND ITHEM...]

CURRENT TODOS [top-down order...]:		
	TODO: *remove* the `prev()` as method! [on IStream *and* implementations, *and* annotations!]
		1. Just a useless piece of functionality. 
		2. Adds more things to test 
		3. Not used in practice (due to expensiveness of backtracking)
		4. [Key Reason] All benefits are Replaceable by `.peek(n)`

	TODO: Docs [initial, new, final]
		Documentation order: 
			2. fix wiki [add new stuff, delete old]
				1. LET `jsdoc` and `wiki` be THE SAME, except 'wiki' has more info. 
					COPY BLATANTLY from the `jsdoc` into 'wiki', completing this stuff with new information. 
					Also contains the per-method documentation [CLASSES ONLY, not for interfaces], 
						and 
				2. add documentation to PUBLIC/PROTECTED methods/properties *ONLY*!!!
					ONLY on public classes [exports]
					
				3. add SPECIAL pages for `IStream`: 
					1. on mixins and "annotation" classes
						1. on differences between the JS and TS APIs, specifically
						2. vital: make sure it's *ABUNDANTLY CLEAR* that this is PRIMARILY a JS-API
							Reasons: 
								1. TypeScript is a horrible language
								2. TypeScript is a HORRIBLE language
								3. Just look at all the nonsense we had to invent to make this work. 
									TypeScript is a horrible language! 
								4. Too much work to get this working...
								5. Not cutting it out entirely due to how beautiful the TypeScript's API is otherwise...

						3. CONCLUSION: 
							1. do the '.annotation/.generic'-related changes in `v0.3`: 
								DO NOT move to v0.4; 
								Reason - purity of exports. 
								The `.annotation`s ARE necessary, however, they are also: 
									1. an implementation detail
									2. necessary only to avoid exposing annotations to the user via `export`-keyword
							2. DO NOT add the TypeScript-specific docs: 
								Too much of a mess. 
								Not worth it. 
								Treat the `IStream`-classes in the docs as if they were the "real thing". 
							3. REWRITE the projectr later in your TypeScript-replacement 
								[it will share qauite a lot of the syntax, so very likely, one will not need to change all that much of the code]; 
					2. on how to extend the "annotation"-based classes [.generic, .annotation, etc]
					3. on the HIERARCHY [make an image via 'Excalidraw']

		2. Ensure NOTHING is missing from the wiki: 
			1. utils
			2. interfaces
			3. classes
			4. permalinks
			5. self-referencing: 	
				1. provide links to interfaces for class pages
				2. provide links ot implementations for interfaces to interface pages

		[later]
		4. Finalization: 
			1. piece-by-piece DOCUMENT any new exports: 
				1. finished `samples` 
				2. the `.annotation`s [leave them undocumented for the moment...]: 
					1. NO JSDOC [reason: pointless]
					2. wiki [brief explanation what these things are even]
				3. Regex, TokenizerStream, ...

MINOR TODOS: 			
	TODO: EXPORT the current (AND NEW - create them) "wrapper-functions" around classes as '(...) => T & new (...) => T' 
		via the 'const NAME = OTHER_NAME as any' type-trick. PARTICULARLY, do this for `Stream`s
		1. In particular - ONLY add "function-wrappers" for the `class _StreamClassName {....}`for: 
			1. classes that are INTENTIONALLY to remain 'sealed'
			2. classes that are using the Factory pattern 
			3. public classes that AREN'T INHERITED-FROM ELSEWHERE 
			
	TODO: make the 'MapClass' and 'HashMap' BOTH operate on a "local class" [instead of an 'inner-function class']
		Replace Configurator Pattern with 'Generalized Factory-Pattern' [or, weaker form of `Configurator Pattern`]. 
		Again - same thing as with the 'Stream'-s - for JIT's sake + to save (some) memory, 
			since the user will (likely) want to be using the MapClass for their own goals. 

	TODO: [`mixin-IStream`] MAKE the (remaining) `Annotation` classes *INTO EXPORTS*
		Also - MAKE them part of the NAMESPACES that are the "main" export-functions. 
		[Let they be available under `.annotation`, SAME WAY as `.generic`]
		Reasons: 
			1. the user MIGHT want to use them for their own (technically legal) TypeScript-machination purposes
			2. some annotations are REQUIRED by other annotations...
			3. [main reason] too much work moving them to /internal

	TODO: simplify the `MapClass` code. 
		Specifically, all the excessive `.prototype` manipulations. 
		They are ugly and unnecessary (well, except maybe ` = plainGetIndex` thingy). 
		
	TODO: ENSURE that ALL THE EXPORTS ARE ON THE SPOT [this particualrly applies to IStream - LOTS of new classes has been added...]; 
	
	TODO: [IStream] regarding the `.generic!`...
		REMOVE, replace with just `.generic` [i.e. A NON-NULL PROPERTY is what is must be...]; 

	TODO: [IStream] ENSURE that the `Args` generic arg [in `Stream` module] IS NOT 'any[]', but '[]' BY DEFAULT!!!

	TODO [docs]:
		ADD a GUIDE on how to CORRECTLY inherit the `Stream` classes [TS-only].

		Reason: 
			1. the library is written in TypeScript, so PROPER inheritance [with generics] must work well!
			2. JavaScript-usage has no use for this insanity
			3. TS-support is good!

	TODO: ENSURE the 'Args | []' THING in ALL the `IStream`-implementing classes...

	TODO [docs]: make a NOTE that the library is a PRIMARILY JavaScript API!
		Meaning - that the stuff with `.generic/.annotation` is the price to pay 
			for being JS-native [id est - being "natural", without forcing patterns
				from "conservative" OOP languages, such as Java]

	TODO: replace the `x: (...args: ...) => ...` with `x(...args: ...): ...`; 
		Where relevant only [most places]. 
		Reasons: 
			1. better typing...
			2. less confusion between the two 
				[former is ONLY necessary for function-properties, 
					whilst the latter is ONLY used for methods, 
						so marking methods with the former, while possible, 
							is neither sensible nor recommended most of the time]

	TODO: find *THE* largest class in the library 
		Use it as a sort of a: 
			1. quality check [that all the classes "fit" within a certain size bound]
				And that there are NO MORE "big" classes that need refactoring. 
			2. for-fun statistics thing [knowing what is the largest library's class currently is]

		TRY to have everything in under 140-150 lines [rare], 
			and most classes in under 80-90 lines [ideal], 
				with them being as small as possible - encapsulation 
					of even ONE (independent) variable is enough cause for 
						a separate class. 
	
LATER TODOS:
	TODO [VITAL]: FIX the `ICopiable` presence in interfaces! 
		It is EXCESSIVE, and is only required within SPECIFIC cases, when the user 
			THEMSELVES may want to copy the class in question. Otherwise put: 

				1. test CLASS IMPLEMENTATIONS, not interfaces
				2. the user SHOULDN'T HAVE TO IMPLEMENT THE `.copy` METHOD!!!

		This specifically touches on the `IStream`. 
		DO NOT add the `ICopiable` on ANYTHING that isn't supposed to be 
			ACTUALLY COPIED by the user [which it might]. 
		Instead, let the [MUTABLE] TEST-CLASSES demand `ICopiable` 
		from specific class-cases. 

	TODO [maybe?]: change all the `<... = any>`s in the code to `<... = unknown>`s. 
		Reasons: 
			1. works FAR better with the `&`-role-composition	
			2. SAFER GENERIC TYPES!
		NOTE: *do not* do it everywhere. 
			Reason - may break some code. 	
			
	TODO: TRANSFORM "excessive" 'mixin's INTO `abstract class`es
		These: 
			1. are convertible to ordinary/abstract classes
			2. have 0-1 parents
			3. very unlikely to change

		POST-TODO: later - FIND NEW ONES... [if any]

	3. FINALE OF THE V0.3 DEVELOPMENT: 
		1. Regex
		2. TokenStream
		3. finish 'samples'

	NOTES:						
		NOTE [for later]: docs - 'protected abstract' properties...
			DOCUMENT THOSEAS WELL. 
			Specifically, for classes. 
			These are INTENDED to providing the user with ability to customize EXTENDABLE behaviour
				[example: IterableStream, DelegateStream, IdentityStream]; 

		NOTE [for future]: the [Symbol.iterator] inside the `SwitchArray` COULD be a minor bottleneck. 
			Benchmark later...
			Possibly - replace with a callback-utilizing strategy..; 
				[Reason this was not done at the beginning is: it's ambigious which should be faster, 
					since it's hard to control whether or not the passed callback is INLINED or not 
						(so we may end up spending more time due to needing to create the callback)]; 
			
		2.76. [parsers.js] Set up a GitHub Actions -> npm pipeline; ALSO - ADD A GitHub Workflows FILE for updating the latest version, and publishing to npm... + publishing the latest docs [separate]
			ALSO - create a list of presently maintained projects, for which to add such a pipeline: 

		NOTE: About Tests [ADD NEW, REMOVE OLD - scorched earth... again]: 
			1. New test cases bear form:
					suite("ClassName (case #1)", () => {
						ClassTestObject.withInstance(
							new Class(), 
							(instance) => {
								test("ClassName.prototype.methodName1 (case #1), () => 
									instance.methodName1(...)	
								)
								
								test("ClassName.prototype.methodName1 (case #2), () => 
									instance.methodName1(...)	
								)

								...
								
								test("ClassName.prototype.methodName2 (case #1), () => 
									instance.methodName2(...)	
								)

								...
							}
						)
					})

			TODO [maybe?]: create
				2. `TestChain` class, 
					which has an internal `TestCounter`, 
						has a `handler` function, and 
							"inputs: any[][]" arrays of arguments to be 
								supplied to the `handler`

				USE the `TestChain` to express the chains of tests thusly. 
				
	TODO: OPTIMIZING `TypeScript` property definitions on classes: 
			When [in TypeScript], one does: 
				class C {
					prop: P
				}	 

			This DOESN'T ACTUALLY create a property of 'prop', however, doing this: 

				class C {
					props: P = ...
				}

			DOES. UPON CREATION OF THE OBJECT ['constructor' call]; 
			When one needs a PROTOTYPE PROPERTY, use the first variant (type-level only), 
				where it's an INSTANCE PROPERTY, use the second, as doing so wil avoid the 
					necessity for type-transitions in code. 

	2. Workflows [GitHub Actions]: 
		1. make-tag [like in one.js]: 
			1. updates 'package.json' version; via `npm version`
			2. makes tag
			3. commits
			4. pushes
		2. npm-publish [like in one.js]: 
			0. runs the run-tests as a Reusable Workflow
			1. publishes to npm
		3. run-tests [not quite like in one.js, but similar]:
			0. builds the project and tests
			1. runs the tests

		1. AFTER you copy/write those, ADD a GitHub Gist for them: 
			1. make-tag
			2. npm-publish

			__NOT__ the `test` command. 
			Reason: too different from project-to-project. 
			These are the same. []
	
[TOTAL LIST - final + leftovers...] TODOS for v0.3: 		
	Primary: 
		EL1. Eliminator: 
			Replacement for the old `Eliminator`, based off `Regex`; 
			Lazy, doesn't need several passes. 
			Takes in the `Regex` list [walks, for given "character", each of them, matching, eliminating what was matched...]; 
				Eliminates ONLY the first thing....; 	
													
		TODO: `samples`: 
			3. SkipSpaces - TODO: *add* into `samples.Stream` ['Stream' for skipping space characters]: 
				1. Based off `FilterStream`
			4. ALSO - DelimiterStream
					0. based off 'FilterStream'
					1. uses a "has"-based predicate created from a list of possible delimiters/items, THAT ARE SKIPPED
			5. IMPORTANT NOTE: 
					after adding `TokenStream`, introduce: 
						1. 'isCRLF' utils - returns if `new Regex("\r\n")` matches AT CURRENT `stream` POSITION (this should be easy to do...)
							Put together with the `isLFSkip` util. 
							More exactly: 
								1. add the `isCRLF = (x: IStream<string>) => new RegexStream("\r\n").matchCurr(x)` [or something...]; 
								2. add the `isCRLFReplace = (x: IStream<string>) => isCRLF(x) ? "\n" : x.curr`
						2. 'isNewlineSkip = or(isLF, isCRLF)'
							This is for the user to be able to instead of transforming "CRLF -> LF", 
								just use this as-is [count the newlines]; 
						3. `LFStream` - a stream returning, for an `isNewline` predicate 
							[not INewlinePredicate, instead (stream: IStream<string>) => string], 
								INSTEAD of items that fit the `isNewline`, the result of the `isNewline`. 
							
							Something like: `return isNewline(x)`
							It's a `StreamParser`
							Put under 'StreamParser/classes'
							Purpose: to use with `Regex("\r\n")` [isCRLF], and `isLF`
								More specifically, to be able to do: 
									'toLF = (x: IStream<string>) => return isNewline(x) ? "\n" : x'
								MAKE this a proper util.

							Reason to introduce - LF/CRLF-independent parsers...
								Immensely useful for negelecting platform-specific LF/CRLF differences. 
			
			6. scavenge old projects + add common-all-over-the-place type-of patterns: 	
				1. Tokens (names for INode-classes): 
		
					1. Take them from various parsing projects of yours already existing: 
		
						1. xml
						2. selector
						3. regex
		
						Unite, choose best string-names; 
						Create appropriate names for sample-token classes (via TokenInstance) [
							example: 
							
							Opbrack - (, 
							Clbrack - ), 
							OpSqBrack - [, 
							ClSqBrack - ], 
							OpBrace - {, 
							ClBrace - }, 
		
						...]; 
		
				2. arrays with common items (USED throughout...): 
		
					'binary = [0, 1]'
					'decimal = [0, 1, ..., 9]'
					'hex = [0, 1, ..., F]' - TAKE OUT of the global '/utils.ts'
					'ascii = [0, 1, ...]'
					'alphanumeric = [0, 1, ..., 9, a, ..., Z]'
		
					Also - the sets in question should be provided AS-ARE (without the 'inSet'...); 
				
				3. ALSO - join the 'samples' with the 'constants.ts' file 
					[they are pretty similar in that they BOTH provide frequently recurring ambigious constant entities with meaningful "static" names and uses]; 

	Secondary: 			
		RE1. 'regex' - allow for methods for creation of regular expressions that use the extended regex-syntax (enabled by the 'v' flag); 
			Namely: 
				1. intersection: 	/[[...]&&[...]]/v
				2. subtraction: 	/[[...]--[...]]/v
				3. string literals: /[q{...}]/v
		
	Minor: 	
		TY2. Type simplification - the `array.Pairs` overuse: 
			Rely more upon the `Iterable<[KeyType, ValueType]>`, where possible. 
			Specifically - when the [Symbol.iterator] is the only actually needed property.
				
	IMPORTANT: clean the git history - keep only the commits for each individual version; 
		Squash all the others + change the respective commit messages; 
		[do this via `git rebase -i` + `git push --force`]

	BEFORE WE LEAVE [vital]: 		
		TODO: JIRA for v0.4
			WHEN you do the FINAL 'git rebase -i' of the thing, 
				remove all the *old* `todos.txt` files (to make the repo seem presentable). 
		
			Let the *new* notes for v0.4 now be inside of:
				1. JIRA 
				2. `GitHub Projects` [as a backup only...]
		
	AS for the `Dungeon Librarian` - it's JIRA there, AS USUAL. 

	TODO: WRITE the CHANGELOG
	
[v0.4]

TODO: 
	0. General notes (see these before anything else): 
		PR1. Move all this to GitHub Issues: 
			1. The library's issue-tracking IS A MESS
				It ought to be organized...
			2. Put each of these pieces into a seaprate Issue
			3. Remove 'todos.txt'; 
		TE1. Do TDD this time
		JD1. [IMPORTANT] Expand JSDoc: 	
				Read more about `JSDoc` in TypeScript at:
					https://www.typescriptlang.org/docs/handbook/jsdoc-supported-types.html	

				1. use it more thoroughly (give higher priority for classes and so forth - not just a brief description + Wiki link)
				2. expand its contents (argument types, @type, self-reference, etc)
				3. [IMPORTANT] document early - another one of the banes of parsers.js: 
					Due to the fact that development was (mostly) content- and feature- oriented, 
						one very much forogt about importance of spending appropriate amount of 
							time on documentation. Hence, one (slowly) started to forget what one 
								was doing, and whether it even (sometimes) made any sense. 
					Had there been documentation, certain specific poor choices may have been more avertable, and 
						some little time could have been saved. 
						
	1. Primary [new ideas, modules, interfaces/classes, algorithms]	
		TS1. DepthStream: create a new version - children-first traversal [call it `ChildrenStream`]; 
			The 'DepthStream' (currently) is the pre-order tree traversal algorithm Stream-implementation; 
			There should be ANOTHER - the post-order tree traversal; 

			1. Create a new 'IStream' implementaiton for this - 'PostStream'; 
				This is (particularly) useful for some types of interpreters; 

				The Pre visits the node, then its children one-by-one starting from the beginning, then the siblings; 
				The Post visits first the children, then the siblings, only THEN the parent [when there are NO MORE children - then the parent is visited]; 

				Basically, Pre (parent-to-children) returns the nodes 'as they go' in the left-to-right traversal in the tree,
					whereas Post returns them in 'reversed' order: for the parent to return, children must all return first; 

				[Example usage: transform the binary '+' -- a + (b + c * (d + (k + f * r))) -> a + b + c * (d + k + f * r)]; 
				
		TC1. Create an 'ASTAnalyzer' class [under new 'Node' module]: 
			1. Takes in an `INode`
			2. Traverses the tree
			3. Returns information: 
				1. [Buckets-Categorization] Arrays of '.trivial', '.content' and '.recursive' nodes
				2. [Type-Categorization] Returns a 'HashMap' of arrays of nodes, filtered by their '.type'; 
					NOTE: the 'HashMap' is, TOO, completely configurable by the user [user-defined (if provided), else - default being: when 'string's are used for 'type's - ObjectHashInternal, otherwise - HashMapInternal]; 
						(another function-argument for returning the final ASTAnalyzer class)
	
			4. Provides functionality [ASTAnalyzer]: 
				1. mapTypes((type_name: string) => string): 
					creates a new 'ASTNode', by means of mapping the '.type'-s
						of the current one using the given function. 
					Immensely powerful for changing between different data formats. 
	
				2. mapValues((x: any) => any)
					creates a new 'ASTNode', by means of mapping the '.value's of each 'ContentASTNode', 
						while preserving all else
				
				3. .find(type: ..., pred: (x) => boolean): 
					Seeks an item in the tree with a type '.type', obeying the predicate '.pred';
					Optimizes to look specifically in a '.type'-bucket of the given 'type'. 
					Can be radically faster for large inputs. 
				
				4. .find(pred: (x) => boolean): 
					if the 'type' is not passed, the entire tree is searched; 
					Optimization - uses prior obtained collections for increasing cache locality of large searches. 
				
				5. .iterate(type) - returns a Stream, filled specifically with values of type 'type'; 
					Namely, it returns an 'InputStream', wrapped around the respective '.type'-bucket; 
					The items are listed on a from-beginning-to-end of the 'Stream' used to construct the ASTAnalyzer; 
	
				6. .filter(pred) - returns a new tree, such that it ONLY contains elements that obey the given predicate: 
					HOWEVER, one can also mandate that in order for a sub-tree (RecursiveASTNode) to remain, 
						at least one of its children must be 'true' as well. 
						This is done by means of returning `null`, instead of `true`/`false`.  
				
				7. .search(type: any) - searches, and returns for multi-index information for each of the elemenets of a given type. 
					Optimization: uses ONLY JUST the information from a given '.type'-bucket; 
					This allows one to: 
						1. Iterate a portion of the tree INSTEAD of needing to start "at the beginning
						2. Know exactly when to stop (seeing the *last* item that needs to be iterated over)
	
				8. .map((x: ContentASTNode | TrivialASTNode) => any): 
					Maps all the non-RecursiveASTNode parts of the tree to a different 'Tree'. 
					Useful for creation of generalized non-AST trees, to be used with 'TreeStream': 
						Example, passing a function that returns 'string', then - CONCATENATING all 
							the items inside the obtained Tree, via: 
	
								// [sketch - no types]
								function GenerationFunction(F) {	
									return function generate(ast) {	
										return array(TreeStreamPre(ast.map(F)), new UnfreezableString()).get()
									}
								}
								
								// this is to create either a SUBSET defined by a recursive IndexMap-like function, *or* via working through a '[type]'-defined set of "superior" nodes; 
								// NOTE: 'ast.types[type]' is an *ARRAY*
								function GenerationRecursive(F, type) {	
									return function generate(ast) {	
										return array(InputStream(ast.types.index(type).map(F)), new UnfreezableString()).get()
									}
								}
		
					Also - '.map(F)' OPTIMIZES, so, it expects the given values to be returning FUNCTIONS, for plugging in their children's 'F(x)', 
						so: 
	
							1. A -> B
							2. F(A) -> X(K): X(B) == string
							3. F(B) == string
								F(A)(B); 
							
					The optimization is - SPLITTING the '.types'-buckets via doing '.types[name].map(...)'; 
				
				9. RecursiveASTNode: 
					1. Contains various copying .value-Array-delegate methods: 
						1. .map(f) - creates a new RecursiveASTNode via 'new RecursiveASTNode(this.type, this.value.map(f))'
						2. .filter(f) - creates a new RecursiveASTNode via 'new RecursiveASTNode(this.type, this.value.filter(f))'
	
			5. Optimization information (additional): 
				1. IDEA: add a new 'ASTHierarchy' information object, which: 
					1. Specifies, what '.type'-s CAN be found within which '.type'-s of RecursiveASTNode-s: 
						IMMENSELY powerful for search-optimization inside a given Tree 
							[limits the '.type'-buckets in which one needs to look]; 
					2. Specifies maximum "depth" of a certain particular 'RecursiveASTNode': 
						1. Allows one to replace a continuous check of 'isGoodIndex(.lastChild)' with 'isGoodIndex(.lastChild) && i < MAX_DEPTH_COUNT'; 
							Guarantees a performance bound. 
							When the bound is PRECISE, the 'isGoodIndex' can be dropped
					3. Specifies maximum length for '.children' of a given 'RecursiveASTNode': very useful for iteration! 
						1. Replaces '.children.length' check with a 'counter < FIXED_PRECOMPUTED_LENGTH'; 
					4. Specifies expected structure for a given 'RecursiveASTNode': 
						1. May permit one to be going one-by-one downwards for it, EXPECTING the given '.type's; 
						2. Eliminates checks for '.isSiblingAfter'
	
					'ASTHierarchy' is optional, and its purpose is solely to: 
						1. provide algorithm optimizations
						2. enforce bounds for a given format
	
					NOTE: 'ASTHierarchy' is ONLY good for optimization, when IT IS PRECISE 
						[id est, we can SKIP certain function-calls/checks]. 
	
				NOTE: optimizations work via: 	
					1. ASTStream-s: 
						0. Alternative to 'TreeStream': less generic, provides better optimizations for AST: 
							1. ASTStreamPre - alt. of TreeStreamPre
							2. ASTStreamLevel - alt. of TreeStreamPre
							3. ASTStreamPost - alt. of TreeStreamPost
						1. Accepts an 'ASTNode' instead of a 'Tree'
						2. Optimizations: 
							1. No '.lastChild' presence check in '.isChild()' method - instead, just '.lastChild > -1'	
								This is a good optimization for TreeStream-iteration of large trees ('.isChild()' is called ON EVERY '.next()' call); 
							2. Algorithm Configurable via 'ASTHierarchy':
								1. Optimizations are present ONLY when ASTHierarchy is PRECISE
								2. Expects a VALID abstract syntax tree
	
				2. For '.type's field values, one uses EITHER an 'ObjectHashInternal' (if strings), or a 'MapHashInternal' (if not strings); 
		
		ME3. [IStream] Iterator Helper functions - '.filter', '.take', ...: 
			Provide them. 
			Use them in the library definitions that actually EMPLOY them [example: PositionalValidator would benefit greatly from a single '.filter()' call]; 
			These will (each) require individual internal-class implementations + public interfaces to use.
				Reason for "internal" nature of these classes: one doesn't want to allow user to create them on their own
			
	2. Secondary [new utils, methods]
		UT1. bufferize(tree, size) - util [Tree/utils.ts]
			Given a 'Tree', returns an array of values of size `size`: 

				{
					childNumber: number,
					value: any
				}

			The `size` is a fixed number >= 0, 'childNumber' is the number of children, ahead of current node in the array.
			To be used for creating a persistent linear data structure for working with the given AST. 
			Good for cases, when the number of items is known, and is large (part of the the "big input" optimizations); 

			Would permit user to store a '.state.size: Pattern<number>' property (Pointer(number)) on the ParserState-s, 
				for counting the size of the input in terms of nodes, then re-use it with 'bufferize', 
					and use the tree for evaluation; 

			If the 'size' is unknown, implements a simple DFS (namely - conversion of a given 'Tree' into an 'Array')
			
		UT2. util - enumerateTree [Tree/utils.ts]
			Given an array of "types" of nodes, it recursively converts their properties (given in the 'shapes' array), 
				to single-array-siblings form (the current ChildrenTree); 	

			This would permit a more "variable" set of trees (those that have properties with fixed names), 
				to be enumerated accordingly...; 

			Example [1]: 

				{
					type: any, 
					a: any
					b: any
					c: any
				}

			Becomes [1]: 

				{
					type: any, 
					value: [
						// ... a, b, c
						// ... or b, a, c
						// ... or any other order
					]
				}

			Example [2]: 

				{ type: any }; remains the same
			
			Example [3]: 

				{ type: any, namedProp: any }; becomes { type: any, value: any }; by changing the property name to 'value'

			For this, define a 'TreeKind'/'TreeShape' - with a list of properties to be used/transformed; 
			More specifically, it creates a tree of `Node`s, from a tree with a given shape. 
			
		UT3. Add more complex 'Node' utilities. Based off previous 'one.js' v0.3.1 [the removed methods], add: 

			1. deepSearch - searching inside the given Tree for a value with a given 'prop'
			2. depth - calculating the depth of the given Tree
			3. treeCount - counts the values of a given predicate for all items in the tree

			ALSO: Add new methods to 'RecursiveNode': 

				1. .reverse() - recursively reverses the given tree
											
		CU1. Add a 'binarySearch' utility: 
			1. For IndexBuffer - as it keeps 'number's in an ordered fashion, there is no reason not to employ it...; 
			2. This is generic (meaning, it provides a map 'f', and then compares via 'f(a, b)', with default being '(a, b) => a < b'); 

		MC1. MapClass: fix types
			More precisely, the "shady" types of `.extend/extendKey`, and of `.index(x: ???, y: ...any)`; 
			These are supposed to make the distinction between the OUTER KEY type and the INNER KEY type ['K']. 
			The inner key type matters during construction, but the outer key type matters during other usage. 
			Likewise, FIX the `IIndexable`. Let it specify the type of the PRIMARY ARGUMENT 'x: ???'; 
							
	3. Unrelated/separate module/grand refactoring: 		
		DP1. [Unrelated - later, spread to appropriate note-files] Deployment Pipelines for other maintained projects: 
			1. draw-text [install-npm + prod-to GitHub Pages]
				Also - ADD THE NPM DEPENDENCIES THAT IT REALLY REQUIRES - *INCLUDING* 'parsers.js'
			2. selector [to npm]
			3. regex [to npm]
			4. xml [to npm]

	4. Docs: 
		WE1. CREATE A proper website with documentation for the library. 
			Do benchmarks, et cetera...; 
			After doing GitHub Wiki for v0.3, see if it cuts it (spoiler - it likely won't one STILL wants the ravishing types for the library's docs to be present!)
				For this: 

					1. Learn a new CSS-based tool (either for generating/hosting docs/docs-css-styles like TypeDoc/readthedocs , 
						OR a new library to create one's own doc-styles like 'Tailwind CSS'); 
					2. Create a JSON parser in parsers.js and COMPARE it against the [https://chevrotain.io/performance/], with ops/sec. 
						See, WHICH libraries have managed to beat parsers.js v0.4., and WHAT can one do about it (see their optimizations, specifically...); 
						2.1. ALSO: create a benchmark for MEMORY USAGE [more concretely - compare parsers.js with the others...]; 
							The memory usage should be noticeably better than speed...; 

		JD1. JSDoc-s: add references
			Particularly - add the links to documentation website INSIDE the JSDoc (currently, lacks, the user has to look up themselves). 
			Reason (that it lacks currently, as of v0.3): time concerns

		JD2. Use a more complex (high-quality) JSDoc
			Current one is primitive (time concerns). 
			Add tags: 
				1. @returns
				2. @type [JSDoc-level self-reference for types]
				3. ...

		IN2. REMINDER: about the `init` method output type: 
			After it is generalized to a SINGLE interface, make FOR ALL `.init`-having method
			so that `init` returns `this`. 
	
		WI1. Add usage examples: 
			Current API is quite complex considering the need for generic functions (in many cases). 

		WI2. Add proper Guides: 
			Currently, the API code is pretty brutal. 
			Doesn't explain stuff, only contains crucial-to-be-used notes, 
				and general explanations. 
			The API *IS* simple (ultimately), but it is better to be explained,
				because otherwise quite a lot of things may seem cryptical and/or 
					unnecessary

			1. Separate the API documentation from Guides
			2. Guides would include: 
				1. Examples of code for mini-projects: 
					It's going to be one of:

					1. A JSON parser [using 'DynamicParser' - non-self-modifying]: 
						Reasons: 
							1. simple to implement
							2. simple to test
							3. can unify the code
							4. not really useful (as builtin JSON.parse exists; e.g. no reason to refactor to another repo, besides tidyness...)
				2. Terminology pages: 
					Originally intended to be part of v0.3, these have gotten way out of 
						hand, and no longer feasible without some considerable effort (for which there really isn't much time...); 
				3. Samples page: 
					This is a Guide to the `samples` module. 
					Will list the "out-of-the-box" cases for usage 
						and link to respective `samples` module 
							exported definitions. 

		WI3. Remove links to old docs (v0.3 and prior...): 
			It's good, but it's old, and... 
				well, the library wasn't quite formed yet. 
			Constant compatibility breakages, no firm direction of development, 
				constant change of idioms employed, 
					it's basically (almost) like a whole new project now
						(difference being - now it's got a lot more types, and good swe practices: tests, SOLID, etc...)

		WI4. about `fault tolerance`, 
			the user can (themselves) create special "Error" types, 
				which are respectively handled by HashMap/IndexMap-based TableMap-functions; 

			Add a Guide for that.

		WTJD1. Combined note on self-references
			1. add wiki references: 
				1. to jsdoc
					Now, the wiki (which is still just a glorified version of jsdoc), 
						will also be given references to by the JSDoc [user no longer has to search for it themselves]. 
				2. into wiki [self-references]
			2. complete jsdoc: 
				1. for classes: 
					1. add documentation ON A PER-METHOD basis
						[note: for 'annotated' classes - add docs to the FAKE METHODS, not the real ones]; 
					2. add documentation to PUBLIC/PROTECTED methods *ONLY*!!!
						Must be synced with `wiki`. 
						LET the `public/protected` docs for these methods be THE SAME. 

	5. Testing: 
		1. introduce mocks [use Jest]
			Reason: complexity. 
				There's still v0.4 to go (proper BFS implementation, for instance...); 

	6. Research (Chevrotain - source, capabilities, optimizations): 
		1. Compare performance with Chevrotain benchmark (the JSON parser example): 
			1. *possibly*, if the libraryperformance is really bad, do something about it...
		2. [yes, that thing also...] https://github.com/Chevrotain/chevrotain/blob/master/packages/utils/src/to-fast-properties.ts#L2
			Supposedly, it (significantly) speeds up property access on classes. 
			Benchmark...
			3. [building v8] d8 - USE IT to profile the optimizations (lack of) of the used library code [reflect upon, and change the library respectively]
				Do research (V8 performance): https://floitsch.blogspot.com/2012/03/optimizing-for-v8-introduction.html

FUTURE: 
	1. REFACTOR [some] of the `internal/exported` APIs into SEPARATE npm PACKAGES: 
		0. `copiable` package: 
			A TypeScript package. 
			Contains the `ICopiable` trait. 
			Make a dependency of this. 

		1. `test` package [already developed]: 
			1. add the current 'ClassTest', 'MethodTest' 
			2. make `copiable` a dependency of this: 
				1. reason - relies upon copiability of a given class for its instances to be 
					'ITestable' - a new interface to export
			3. [unrelated] add serialization of tested items [interfaces for it, etc]
			4. [unrelated] add 'Snapshot' API (similar to `Jest`)

		2. `autocache` package [make a dependency of parsers.js]: 
			1. Autocache
			2. IIndexable
			3. ISettable
		
		3. `serializable`: 
			1. `ISerializable` interface
			2. `ISerializableObject` interface
		
		4. 'mixin.ts': 
			1. a new package - the 'mixin' module here
				It should be USED IN `parsers.js` as BOTH: 
					1. a dependency
					2. a re-export [since there are mixins CURRENTLY exported]

		5. `object_pool`: 
			This is the `ObjectPool` (and `IInitializable` interface) class. 
			It is far too independent to be just left here. 
			Far too useful to be allowed to be "herded" together with the domain-specific 
			stuff. 

			'FreeStream' REMAINS PART OF `parsers.js`

			Export this from here as a dependency. 

	2. [maybe? later, definitely] A Java rewrite: 
		the project is a very fine one, a very fine one indeed... 
		Howeeeverr...
		One cannot (for instance) utilize it for projects that do not allow the usage of JavaScript. 
		Solution: 
			1. rewrite in *name-of-THAT-Big-project* [once finished]
			2. decompile the JS sources [obtained from TypeScript]
			3. change the sources
			4. transpile to Java [that is the alternative language of choice]
				That is without the tests of course. 
				Those you'll need to write anew (using something like JUnit, maybe?)
				THE `.copy()` aspect WILL STILL REMAIN!!!
			5. write new tests [Java]
			6. publish
		
		Also, that is NOT to notice that the `Java` version would leverage the OOP principles far 
			better than the current JS code (since it's been transpiled from TypeScrtipt). 
		
		NOTE [VITAL]: remember that the complex `IStream` hierarchy was (actually)
			supposed to have been done entirely using 'mixins' 
				(which are absent from TypeScript, and, in a formalized variant, from JS)...

	3. Reminder: do you rmember the stack.js? 
		Old (partially abandonned) project for using "stackless" code?

	4. (maybe) later - ensure JS-runtime-independence: 
		Verify that the package has compatibility with: 
		1. Deno [v2.0+]
		2. Bun [v1.0+]

	5. Rewrite this using your NEW language [later the "TS Replacement"]. 
		Reasons: 
			1. natural runtime-static-time combination
			2. better + simpler type-system
			3. [big one] natural support for mixins