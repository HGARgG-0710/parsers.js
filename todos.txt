[v0.3]
	
5. Source code TODO-s: 
	17. USE the optional '.pos' property [Stream] on EVERY possibility inside the method implementations 
		(namely - passing to member-functions/predicates, if it's not there - it's just not there, if it is - the user gets a new feature);
	
	2.62. PROBLEM - with LayeredFunction: 

		TAKEAWAY: add another abstraction OVER IT, that would permit one to pass 
			a 'new (e: EndableStream, state: Summat) => StreamParser', which would [then], 
				be used to create the StreamParser instance USING: 

					1. the 'input' [dynamically given, as first argument]
					2. the initial 'state' [static - later can be changed from inside 'table']

				PROBLEM: consider doing 'StreamParser()' WITH A '.buffer'?! 
					Then - INSTEAD, create a function [separate - put in one.js/functional; NAME - 'argFiller'; SPECIFICALLY - have an 'export const stateFiller = argFiller(new Set(1))' as *A UTIL*]: 	
							
					[ADD THIS TO DOCS!!!]
					Then, one can do: 
						
						const layers = new LayeredFunction(...)
						const singlePrecompute = precompute(new StreamParser(...), new Set([1, ... (other inds)]))
						layers.layers = [singlePrecompute({ parser: layers }), InputStream] // ... example ... 

					Presence of this in one.js would be a PERFECTLY good excuse for keeping 'LayeredFunction' HERE (for the moment, at the very least... - one did plan to refactor it out somewhere, is needed for other porjects as well...); 

	2.73. Do research (V8 performance): https://floitsch.blogspot.com/2012/03/optimizing-for-v8-introduction.html
		SPECIFICALLY - *inlining*. The library assumes its usage HEAVILY to remain efficient. 

	2.74. Do research (Chevrotain - source, capabilities, optimizations): 
		It is *the* fastest JS parsing library out there. See what it can do...:

			1. [basics] http://chevrotain.io/docs/tutorial/step0_introduction.html
			2. [important!] http://chevrotain.io/docs/guide/introduction.html

			[Post parsers.js v0.3; ADD this as parsers.js v0.4 todo...]; 
			3. [yes, that thing also...] https://github.com/Chevrotain/chevrotain/blob/master/packages/utils/src/to-fast-properties.ts#L2
			4. [building v8] d8 - USE IT to profile the optimizations (lack of) of the used library code [reflect upon, and change the library respectively]
			5. [Stream-specific] see if the '.curr/.next/.prev'  method re-binding optimizations were an overkill [elimination of 'this.isEnd = true/false/whatever-for-each-call']; 

	2.76. [parsers.js] Set up a GitHub Actions -> npm pipeline; ALSO - ADD A GitHub Workflows FILE for updating the latest version, and publishing to npm... + publishing the latest docs [separate]
		ALSO - create a list of presently maintained projects, for which to add such a pipeline: 

			0. one.js [to npm]
			1. draw-text [install-npm + prod-to GitHub Pages]
				Also - ADD THE NPM DEPENDENCIES THAT IT REALLY REQUIRES - *INCLUDING* 'parsers.js'
			2. selector [to npm]
			3. regex [to npm]
			4. xml [to npm]

	2.80. WALK through the types, determine IF they are desired or not... [once again...]; 
		This time, one has found that (a lot) of types, currently are (somewhat) orphaned/not used, or plainly aren't desired within the library. 

		To Fix [comprehensive list of issues with types]: 
			1. Add 'BasicStream<Type>' Generics to *ALL* the Stream-related utils 
			2. See whether the 'stream.curr' return arguments in things like 'uniRewind' is good or not (maybe, returning 'stream' would have been better?)
			3. Find all the 'as'-thingies, where polymorphism is expected: 
				Specific examples: 
					1. ParentTree: 
						DESPITE the 'IParentTree' interface, it still IS, in practice, non-polymorphic (requires storage of 'ParentTree'-s DUE TO 'instanceof' in the 'constructor'); 
						SOLUTION: 
							1. Provide [in a constructor] a 'isParentTree' function, which defaults to 'x instanceof ParentTree'			

			
			4. DefaultType - for IndexMap-types: 
				It's pretty important, as it determines about half the return types of the type's methods. 
					Put those up! [in templates, and as a dependency of *EVERYWHERE*]
					ALSO - make it a 'NoInfer'! 
			5. 'this'-returning: 
				Stop with the ': any'-nonsense, or overly-specific-this-return-type-nonsense. 
				Just do ': this', AS INTENDED...

			6. StreamClasses/interfaces.ts: 
				1. 'abstract new' types - REMOVE!
					These (instead) have to be written inline [same way as '6.2.']
				2. 'BufferizedPositionalVeryBloodyLongStringMyGoodness...' - replace with laconic '&'-s (this includes the 'abstract new' cases); 
					AND USE: 
						1. Bufferized
						2. Posed
						3. Stateful
					IN THEIR INITIAL FORM (meaning, for instance, for the "maximum" case - 'Posed<number> & Bufferized<Type> & Stateful & StreamClassInstance<Type>')
				3. refactor "small" types via 'Pick', 'Partial' and 'Omit': 
				4. Add the 'LocatorStream' interface (and other SPECIFIC interfaces, to indicate where and which optional flags are present): 
					These are just 'A & B'-type expressions [marker interfaces, mostly]... ;
					IDENTIFY them with the respective classes/functions

	2.77. Do research [learn LR/LALR-parsing properly]: https://en.wikipedia.org/wiki/LR_parser#
		Compare with the stuff the library currently has, (possibly) implement 
			some of the stuff from there... (specifically - the LALR parsers, if they aren't already present)
		Also, See: https://stackoverflow.com/a/24177215
		Also, See: https://en.wikipedia.org/wiki/Bottom-up_parsing
			ESPECIALLY: https://en.wikipedia.org/wiki/Shift-reduce_parser
				This describes how precisely the Shift-reduce parsers (most popular Bottom-Up parsers out there) work. 
				An LR-parser is a kind of a Shift-reduce parser. 
			And, finally: https://en.wikipedia.org/wiki/GLR_parser
			Note: this is the thing that's currently missing from the library. 
				It's extremely easy and opportune to make Top-Down recursive parsers with it, 
					however the 'Bottom-Up' approach is still missing

Order of TODO-elimination: 
	2.80.	
	
	5.17.

	5.2.62.
	
	JSDoc: 
		Add it BEFORE doing the tests. 
		One must first understand PRECISELY how a certain thing is to work, and what to expect from it...;
		Tests are far too tiresome and long to write 
			(as it is now "too late" to set the requirements based off them and do proper TDD, docs provide a much better/simpler/faster alternative)
		
		These are short and concise, don't get too deep into the implementation details. 
		[The "big" full GitHub Wiki will be for this...]
	
	TESTS [work all anew... too much has changed]: 
		1. enumerate the things to test; 
		2. walk through tests, verify them for being satisfactory; 
		3. re-do where necessary, add new cases; IMPLEMENT MISSING;
		4. re-do the 'import' tests as well; 
		5. log the 'run.ts' thingie into a special temp text file; 
			1. [make into an npm command - 'test-run' and 'test: test-compile + test-run']
			2. add to '.vscode/launch.json' [to simplify the debugging process... we'll need it]; 
			3. WORK on the failed tests from there...; 
		
	5.2.73.
	5.2.74.	
	5.2.76.	
	5.2.77.

	[fix test compilation errors]
	[run the tests]

	B. 
		
	C.
	
B. documentation (wiki - change/fix it...);
	Hosted on the project's GitHub Wiki. 
	WHEN WRITING DOCUMENTATION, 
		
		0. make a note that the '.hash' and '.extension' functions SHOULD BE ABLE TO HANDLE null-values!	
		1. Note in the 'UnfreezableArray' docs that AFTER '.unfreeze()' is called, 
			the underlying Stream behaviour *DOES NOT CHANGE* (meaning - one has to create a NEW Stream...); 

	EXAMPLES: 
		1. Currently, this is the code to take a certain `input` Stream, 
			then - take its portion and transform it: 

				// TODO: add this code as example for THE DOCS later...! 
				parserStream = new StreamParser(...)()
				limStream = new LimitedStream(...)()
				convertPortion = (input) => {
					limStream.init(input)
					parserStream.init(limStream)
					parserStream.finish()
					return parserStream.buffer.get()
				}

		2. Faster/more-elegant version of 1.: 
		
			parser = transform(...)
			limStream = new LimitedStream(...)()
			convertPortion = (input) => {
				limStream.init(input)
				return parser(limStream, new CollectionClass()).get()
			}
	
C. rewrite the CHANGELOG [too much has been altered since the "original" v0.3]; 
	C.1. Walk through the last commit's state of v0.2.1, noting changes and writing them down [destructive]; 
	C.2. Walk through the v0.3 changes, and write them down [constructive];

[v0.4]

0. Check if this works with Deno; 	
	
2. New Streams/Parsers + some minor stuff/code-quality/minimalism: 

	2.1. bufferize(tree, size) - util

		Given a 'Tree', returns an array of values of size `size`: 

			{
				childNumber: number,
				value: any
			}

		The `size` is a fixed number >= 0, 'childNumber' is the number of children, ahead of current node in the array.
		To be used for creating a persistent linear data structure for working with the given AST. 
		Good for cases, when the number of items is known, and is large (part of the the "big input" optimizations); 

		Would permit user to store a '.state.size: Pattern<number>' property (Pointer(number)) on the ParserState-s, 
			for counting the size of the input in terms of nodes, then re-use it with 'bufferize', 
				and use the tree for evaluation; 

		If the 'size' is unknown, implements a simple DFS (namely - conversion of a given 'Tree' into an 'Array')

	2.2. TreeStream: create a new versions; 

		The 'TreeStream' (currently) is the 'LL' pre-order tree traversal algorithm Stream-implementation; 
		There should be ANOTHER - the 'LR' post-order tree traversal; 

		1. Rename the 'TreeStream' to 'TreeStreamPre', and create an implemnetation for the 'TreeStreamPost'; 
			This is (particularly) useful for some types of interpreters; 

			The Pre visits the node, then its children one-by-one starting from the beginning, then the siblings; 
			The Post visits first the children, then the siblings, only THEN the parent [when there are NO MORE children - then the parent is visited]; 

			Basically, Pre (parent-to-children) returns the nodes 'as they go' in the left-to-right traversal in the tree,
				whereas Post returns them in 'reversed' order: for the parent to return, children must all return first; 

			[Example usage: transform the binary '+' -- a + (b + c * (d + (k + f * r))) -> a + b + c * (d + k + f * r)]; 

	2.3. Implement a 'level-by-level TreeStream' ('TreeStreamLevel'): 
		It would get an initial given 'level' (tree root), walk through all of its '.children', 
			then walk each of the '.children' themselves as levels (one after another...), THEN descending lower...; 
		Would do it via calling '.getJointLayer' function (then - caching for later '.prev'); 
		This (basically) takes the nodes from the previous 'joint layer', gets their respective children,
			CONCATENATES THEM into a new layer, then iterates it; 

	2.4. Add a module SPECIFICALLY for working with strings/identifiers/Sequence-s (currently: the 'Indexed' interface); 
		In particular, functions/abstractions: 

			1. shorten(names); 
				Given a sequence of "names" (strings/sequences),
					and a 'comparison' predicate for each of its elements,
						it would implement a name-shortening algorithm, 
							that would preserve the name uniqueness;
				The return value is an IndexMap; 

			2. renameable(names);
				Given an IndexMap of names (which could come from, for instance, 'shorten'), 
					the method: 

					1. Re-orders;
					2. Creates new name-map entries;

				In a fashion that would allow straightforward implementation of a sequential "exact replacement algorithm" based on the output: 
					1. Loop through a list of name-maps; 
						2. [In the loop] Rename current encounters of a name with its mapped value;	
					
				There are 2 operations that may be necessary: 
					1. creation of temp-names (when collisions in present names are far too high to re-order);
					2. re-order; 
				
				The algorithm for the 'renameable' function: 
					0. Keep the cached (met) names in a 'Set'; 
					1. Loop through the 'names' given: 
						1.0. If the name is in the cached Set, continue; 
						1.1. If the current 'value' of the 'name' is already present amongst 'keys' (another loop): 
							1.1.1. put the keys in question BEFORE the current value;
							1.1.2. '.swap' the two indexes;
							1.1.3. go one position back (because now one needs to check the "others" now); 
							1.1.4. cache the current amongst the checked names in a 'Set', (so that one doesn't get into an infinite loop);

				This is useful when implementing things like mass renamings/name shortenings in code mangling software; 

		Put identifier, and Indexed-related stuff there (organization, overall tidyness); 

	2.5. [Idea?] Create an error-handling API for the 'v0.4';
		Create means of: 

			1. Stacking different levels of exceptions (the 'try-catch' blocks); 
			2. Assigning relevant debug information to them;

		This is (primarily) for the development process of the parsers; 
		
		Define a Catcher [rough sketch]: 

			function Catcher (info: DebugInfo, logger: Function) {
				return function (thing: Function, thisArg: any, args: any) {
					try {
						thing.call(thisArg, ...args)
					} catch (e) {
						logger(info)
						throw e // NOTE: THIS here is to represent DEPTH [as parsers can be VERY recursive indeed, it may be needed to eliminate the recursive errors on a case-by-case basis]; 
					}
				}
			}

		And a ToplevelCatcher [rough sketch, can use to generalize the above, with 'quit = (e) => throw e']: 

			function ToplevelCatcher (info: DebugInfo, logger: Function, quit: Function) {
				return function (thing: Function, thisArg: any, args: any) {
					try {
						thing.call(thisArg, ...args)
					} catch (e) {
						logger(info)
						quit(e, info) // this is here to represent continuation of a program AFTER the exception
					} 
				}
			}

		Although... This is rather general. Perhaps, better implement as a separate package; 
		Likewise, there'd be ways to count levels of recursion, and assign types to them [specialized signature]; 

	2.6. [maybe?] BreakableStream - a major generalization of the library's patterns: 
		Returns a class for producing Stream-s: 

		1. Have an '.input' (another Stream); 
		2. Has an 'Indexable' of 'BreakPattern's (they are objects) pre-given, which define how the Stream instances behave: 
			2.1. type: string - one of: 
				2.1.1. limit - works like LimitedStream; 
				2.1.2. nest - works like NestedStream; 
				2.1.3. halt - stops the stream [no more items after that]; 
				2.1.4. navigate - finds: 
					2.1.4.1. Either the next item [or the first item AFTER THE BEGINNING] that obeys a given property (PredicatePosition)
					2.1.4.2. Goes to a static position [relative or absolute]; 
					
					ALSO, has a boolean arg 'isRelative' (default: true, basically - whether the values for 'navigate' are to be handled absolutely or not...); 
				2.1.5. transform - applies a given transformation on the '.input' (calls a function); Returns the result;
			2.2. args: object - the way that the 'type' is resolved (equivalent of constructor-arguments/.init-arguments); 
			2.3. buffer: boolean | () => boolean - this one corresponds to whether the current value should be bufferized; 
				When a function is passed, it is called on the 'BufferizedStream' in question to get the value; 
		
		The result of matching 2. to the '.input.curr' is (effectively) evaluated to be the Stream's '.curr'; 

		This todo is a 'maybe?' because all the said things can (pretty much) already be done by the user using StreamTokenizer and the rest
			- this really only just provides a minor abstraction over the whole thing (frees the user from needing to use the exports of the library); 
		This kind of thing is (somewhat) against the library's "all-minimialism" approach; 

		On the other hand, it would permit one to use a very domain-specific language to describe operations on streams conventionally, 
			thus - simplifying semantics + unifying a very large number of different abstractions that are creatable via the library using just this one; 	
		Conclusion: think about it, more of a YES, tha a NO currently;
	
	2.7. utility - 'nestedInfo'; 

		1. Finds a depth of a given Stream, based off 'inflate' and 'deflate'; 
			NOTE: the MAX depth achievable; 
		2. Finds its length - how many positions (in 'number') must one walk before the current nestedness ends, from the initiated position; 

		Returns it as a pair; 

	2.8. Idea for a method: ProlongedStream.prolong; 	

		This (utilmately) modifies the 'this.streams' [modifying the '.isEnd' accordingly]; 
		Also, there'd be '.shorten(n: number)' - it'd delete a certain number of elements from '.stream'-s, 
			optionally changing '.curr', and set '.isEnd = true' (if the .curr is in one of the deleted 'Stream'-s); 
	
	2.9. [DEFINITELY!] Create a 'samples' directory; 
		This is for 'common-case' parsing - generally, walk through the various syntaxes that were 
			PRESENT within the previous parsers, use them?
		
		Or, better, make this into a separate mini-project called 'parsing-samples'; 
		[EXAMPLE: handling the 'escaped' strings/sequences all the time];

		Think about it [this particular refactoring is VERY fine and case-specific, though, it can come in useful...]; 

		No, if one is to do it, the thing should: 

			1. Work with general parsing patterns, that would be CONFIGURABLE for every case; 
			2. Provide low-level Token-s with specialized given names; 

		1. Tokens: 

			1. Take them from various parsing projects of yours already existing: 

				1. xml
				2. selector
				3. regex

				Unite, choose best string-names; 
				Create appropriate names for sample-token classes (via TokenInstance) [
					example: 
					
					Opbrack - (, 
					Clbrack - ), 
					OpSqBrack - [, 
					ClSqBrack - ], 
					OpBrace - {, 
					ClBrace - }, 

				...]; 

		2. 'inSet' functions: 

			'isBinary = inSet(0, 1)'
			'isDecimal = inSet(0, 1, ..., 9)'
			'isHex = inSet(0, 1, ..., F)' - TAKE OUT of the global '/utils.ts'
			'isAscii = inSet(0, 1, ...)'
			'isAlphanumeric = inSet(0, 1, ..., 9, a, ..., Z)'

			Also - the sets in question should be provided AS-ARE (without the 'inSet'...); 
		
		3. ALSO - join the 'samples' with the 'constants.ts' file 
			[they are pretty similar in that they BOTH provide frequently recurring ambigious constant entities with meaningful "static" names and uses]; 
		
		4. MOST IMPORTANT - generation functions, stuff for working with AST-s: 
			1. Based off ASTAnalyzer, ASTStream-s, ASTHierarchy, etc... Generally, the 'Tree.AST' module
		
		5. Stuff for Stream-transforming: 
			Namely, nested-stuff via the 'StreamParser' [WITH '.buffer']: 

				// A VERY common pattern...
				// Generalize to an arbitrary 'X <- LimitedStream() [here]', with 'StreamParser.init(X())'
				const l = new LimitedStream()()
				const t = new StreamParser()()
				const parse = (input) => {
					l.init(input)
					t.init(l)
					t.finish()
					return t.buffer.move()
				}
	
	2.10. v8-specific optimizations: 
		Having done some benchmarking on some typical Array methods, one arrived at a...
	
		CONCLUSION: 
			1. Handwritten methods are *often* __MUCH__ faster than the builtins ON LARGE PIECES OF DATA; 
				On the more common-place cases, however, they are inferior in time performance; 
			2. THEREFORE, one should do: 
				Re-organize the library (how?) to permit usage of the current methods on LARGE pieces of data; 
				Prior - do more benchmarking; 

				About re-organization: 
					1. Think - whether to introduce explicit low-level 'size-controls' [possibly costly, when run a lot]; 
						Although, it's likely to be optimized/negligible; 
					2. Or, whether to split the thing onto 2 chunks [where the 'size' is precomputed...]; 
						This is less flexible (and thus, error-prone), plus will be more difficult to re-structure; 
					
				Current vote is for 1.; 

				ALSO, about bounds - those should be CAREFULLY BENCHMARKED; 
		
		MORE GENERALLY - try to adapt the library for work on LARGE datasets; 
		It'll make it useful for working with big projects/pieces-of-text; 

		THINGS OF ISSUE TO THINK ABOUT IN PARTICULAR: 	

			1. What "LARGE" sizes are practical? [meaning - sources of what sizes can occur in the wild?]
				Take them as large as one can. At least 10000000 symbols (~30MB, ~100000 lines of 90-100 chars), maybe try more; 
			2. How much of a performance difference can one get by doing these optimizations?
				In particular - try to sketch out a parser for something (the smtf format of one's own? it's simple to parse), perform 
					profiling on a REALLY big auto-generated file, then: 
						
						1. take overall time measurements (performance.now()); 
						2. profile, break the execution down on several tiny pieces; Then - benchmark and optimize them accordingly; 
							For this, use results from samples from one's benchmarks library;  
			3. Optimize for smaller sizes ONLY when it doesn't impair the memory severely; 
				When it impairs it at all, choose whether or not to do it based off the chosen memory/speed difference measurements...; 
				
				3.1. FOR THIS [maybe] - have specialized "Heuristics" constant-space in 'constants.ts'; 
					3.1.1. IF doing them - create the heuristics for DIFFERENT ENGINES; 
					3.1.2. IF oding them - make the 'Heuristics' only the "default value" for transition of sizes; 
						THE USER must be able to set their own boundries; 
					3.1.3. IF DOING THEM - check how to "bounrdies" for the increase in efficiency when using 
						the custom implementation "shifted" as the versions of Node progressed - TRY IT with different node, v8 
							versions; 
					3.1.4. IF DOING THEM - other engines to test: 
						3.1.4.0. V8 (Chrome)
						3.1.4.1. SpiderMonkey (Firefox)

					[maybe?] Try building the engines from nil, running the benchmarks for heuristics standalone...; 
	
	2.11. [maaayyybe??] Bring the old planned stuff for v0.3 back; 
		That is the 'Parsers/utils.ts' - 'transform', 'delimited', 'consume' [equivalent of 'LimitedStream', uses 'Collection'-s]; 
	
		Do only if there are code aspects suffering from this significantly: 

			1. readability/accesibility; 
			2. performance; 
		
		Primary cause for this todo is that, despite being "slow" (memory allocation), 
			they did still have the benefit of semantic simplicity; 

		The only cause for performance issues with OOP Stream-s API is overhead from re-structuring the original loop so much
			[need for calling the '.isCurrEnd()' two times more]; 
		Again, do this ONLY if the performance difference is SIGNIFICANT!
	
	2.12. util - enumerateTree; 
		Given an array of "types" of nodes, it recursively converts their properties (given in the 'shapes' array), 
			to single-array-siblings form (the current ChildrenTree); 	
		This would permit a more "variable" set of trees (those that have properties with fixed names), 
			to be enumerated accordingly...; 

		Example [1]: 

			{
				type: any, 
				a: any
				b: any
				c: any
			}

		Becomes [1]: 

			{
				type: any, 
				value: [
					// ... a, b, c
					// ... or b, a, c
					// ... or any other order
				]
			}

		Example [2]: 

			{ type: any }; remains the same
		
		Example [3]: 

			{ type: any, namedProp: any }; becomes { type: any, value: any }; by changing the property name to 'value'

		For this, define a 'TreeKind'/'TreeShape' - with a list of properties to be used/transformed; 
		MORE SPECIFICALLY, it creates a 'ASTAnalyzer'- and 'ASTNode'-fit tree, from one that is 
			too complex (not homogenous enough nodes, more complex node categorization). 

		2.12.1. Also: 

			Create a "fast" 'TreeIterator' class; 
				This is to replace the WalkableTree+TreeWalker+TreeStream combination due to: 

					1. They are slow (require FIRST converting the source to a 'Tree'); 
					2. They are memory-costly (creating a new Tree-structure is somewhat demanding); 
				
			This would (instead) approach iteration directly - it would use PARTICULAR properties for a given Tree;
			Also, (as it's not a PERSISTENT type), it'd accept a 'converter' (so as to be able to "iterate" AND "convert" a given Tree); 

			The interface works like: 

				TreeIterator(TreeKind(...))(childIterator, converter)
			
			Requires NO copying for long-term storage (it's "inlined"), hence - no time needed for unnecessary allocations; 

			The 'childIterator' is (basically) an IndexMap/function that returns for any given item its' sub-iterated items
				(an array of children); 
			The 'childIterator' is used for walking through the items INSIDE the 'TreeIterator'; 
		
			BENCHMARK THIS [compared to the 'WalkableTree'+'TreeWalker'+'TreeStream' approach to code-generation]!
		
	2.13. [generally - see if/where applicable] About generics - relax them; 
		Use ONLY WHEN NEEDED; 
		For example: a method makes use of X generics, out of which Y < X is needed [id est, used somewhere beyond the 'this: ...' in the parameters list]; 
			REMOVE the unneeded ones; 
			RE-ORDER THEM, if need be; 
			Try to make them more minimal...; 

	2.14. [maybe?] Create a module for randomized Stream-generation (`random`);
		(given, say, an InputStream with an Array of expected values inside of it [that can be picked for Nested], 
			it will choose a pair of them for different [distinct] boundries); 
		Similarly - one could give an Array of limiting characters for a LimitedStream to be defined by...; 
	
	2.16. Add a fast implementation of '.finish' to the 'TreeStream'; 
		When called for the first time - it SAVES the "last index", THEN - re-uses the computed one on any subsequent '.finish()' calls; 
		Calls the '.index' on the saved '.lastIndex'; 

	2.17. Re-structure the tests to be made on a per-method basis? 
		Current per-class suites are rather verbose (don't do this until v0.4, already spent too much time on the tests...); 
	
	2.18. RE-INTRODUCE the '.copy()' method and 'Copiable' interface; 
		Comes inside the 'StreamClass' (optionally) via the 'hasState' property;
		Will additionally be supported by: 

			1. 'StreamClass(...).prototype.copy()' method; 
				Solution for '.copy': 
					1.1. By default, copy ONLY: 
						1. the user-defined variables under '.state'; 
						2. the '.pos', '.buffer', and so forth - predefined "inherited" properties;
							Any CUSTOM copying/initialization logic can be defined by the user in DERIVED CLASSES; 
				
				Definition [loose, has to have overloads, depending on presence/absence of '.pos']: 

					function uniCopy (stream: ...) {	
						const copy = new (Object.getPrototypeOf(stream).constructor)()
						copy.uniInit(initSignature(this, this.bits))	
						return copy
					}

				NOTE: here, 'this.bits' is the '.prototype'-level BitArray with different properties that ARE PRESENT; 
					Look below for pseudocode explanation

		Then, the various 'Indexable'-s can use the '.state' inside the '.index' call; 

		ALSO [regarding '.init']: 
			[Predefined] things like '.value', '.buffer', '.pos' and so-forth, DO NOT become '.vars'; 
				To simplify the process, store (on each StreamClass), a 'BitArray' - to indicate, which are present; 
					Depending on this - the user will be able to call the '.init' with "universal" signature, like so: 

						// note: stored on the '.prototype', no memory waste
						stream.uniInit(initSignature(this, this.bits))
					
					The 'initSignature' is a util, returning the "maximum" '.init'-signature, that is possible; 
					The '.uniInit' is a method that initializes the given Stream BASED OFF THE "maximum" signature; 
					This version of 'initSignature' is "from-bits", one that isn't is done via an object like: 

						stream.uniInit({
							state: this.state, 
							vars: this.vars, 
							...
						})

	2.19. For future: improve the 'constructor' tests. 
		Refactor them, permit the ability to properly test the '.copy()'-ed instances using the constructor
			tests (namely - the "prototypeProps" and "ownProps" thing - missing currently); 
			
	2.20. 'regex' - allow for methods for creation of regular expressions that use the extended regex-syntax (enabled by the 'v' flag); 
		Namely: 
			1. intersection: 	/[[...]&&[...]]/v
			2. subtraction: 	/[[...]--[...]]/v
			3. string literals: /[q{...}]/v
		
	2.21. [prototypes, generality] make all the prototype 'value's (in particular - methods) OVERRIDABLE! 
			via doing: 

				Object.defineProperty(_class.prototype, {	
					methodName: { value: ..., writable: true }
				})
			
			Currently, they are ALL non-overridable; 
			This will enable the user to more liberally use the library classes [they are, presently, highly single-purposed (intentionally)]; 
	
	2.22. Create a new 'IndexMap' implementation: 'SetMap'; 
		This would be a: 
			1. IndexMap interface implementation; 
			2. generalized wrapper around a Set (in a way that HashMap is a wrapper around Object and Map); 
				2.1. Would have its own 'extension'; 
				2.2. For 'values' would contain true/false; 

		This is useful for working with cases that require the 'or'-predicate; 
		Example: 'nested' utility; 
		How it is done [pseudo-code]: 

			const SpecialCaseSetMap = SetMap(...)
			const processNested = (x) => nested(TableMap(new SpecialCaseSetMap([...])), TableMap(new SpecialCaseSetMap([...])))(new WhateverStream(x))

	2.23. refactor the 'imports' tests; 
		1. more use of 'specificChildImports'
		2. the 'prefixedNames' + 'namesCapitalized' - appears lots more than once, take out; 
		3. Make better use of the 'namesCapitalized'; 

	2.24. Add the 'export default' from 'classes.ts' and 'interfaces.ts' files for their "main" stuff; 
		Example [1]: a 'classes.ts/interfaces.ts' with THE ONLY export will have it as default, besides referencing by name; 
		Example [2]: when a 'classes.ts/interafaces.ts' has an export with THE SAME name as the module, the export is default (besides being present as of itself); 
				
	2.25. [large input optimizations] Investigate whether retaining the indexes information from 'matchAll' in 'tokenizeString' may not be faster on large inputs; 
		This is largely a question of how fast '.split' is compared to building the array manually using the indicies information from 'matchAll' (that is, a large sequence of '.slice()'-es); 
		Potentially, this could be a large time saving (if '.slice()'-es are relatively short), but otherwise - could be a waste of memory (additional strings allocated without need); 
			Then, of course comes the question of whether the strings are allocated at all; 
			BENCHMARK IT; 
	
	2.26. Introduce a new Position class - MultilinePosition
		Basically, a pair of numbers: 
			(linesNum, symsNum)	
		Create a generalization of it (where 'newline' is a context-defined term, like a particular "separation token", or something...): 
		Which correspond to prior number of newlines and prior number of symbols; 
		Used frequently throughout for things like debug information; 
		The '.convert()' would play very nicely with it...; 

	2.27. refactor the '(x: any, y: any) => boolean' comparison into a 'Comparison' type; 
		This is (most often) is used inside the library's testing mini-framework; 

	2.28. About validation and parsing: 
		This is a somewhat worrysome part of the library, currently, as:

			1. Validation happens separate from parsing, despite using the same abstraction (ex: ValidatablePattern and TokenizablePattern); 
			2. '1.' causes the entire process of 'validate + parse' to take 2x the time (need another loop); 
		
		[SOLUTION 1] Still, for more complex cases (in particular - those that require a per-Stream-element identification - this will not suffice); 
			For this reason, a TODO: 

				Create a new abstraction that would encompass BOTH parsing and validation; 
				Granted, it would take more time than a simple parsing procedure, 
					but (and this is CRUCIAL) it must take LESS time than split parsing + validation (in exchange for increased memory usage); 
				
			This is already doable (manually) using 'StreamTokenizer', but one would love to make it a part of the library (general abstractions + interface + algorithm); 
			From the noted above, 'PatternTokenizer' and 'PatternValidator' are a no-go here 
				(potentially, an infinite set of possible valid/invalid expressions with convoluted structure); 
			It should be a case of 'StreamTokenizer'; 

			Ideas: 
				0. It is a StreamClass (a configurable class of StreamTokenizer-s, more precisely); 
				1. Let stream elements be '<OutType>[valid, token]: [boolean, OutType]'; 
				2. Contains a field of '.validator', which returns '[boolean, OutType]', where 'OutType' is the UPDATED token (based off current one and 'input'); 
					entire return value is, then: 
						
						return this.validator(this.handler(this.input), input)
					
		[SOLUTION 2] Alternative solution [most natural approach in practice, although forces using more general grammars]: 
			1. StreamParser; 
			2. Use a table (HashMap) to see whether a given token can be (in-order) parsed to anything; 
			3. If it cannot be matched - an error/exception, one needs a specialized ErrorHandler; 
			4. Otherwise - part of the Stream; 

			Benefits: 
				1. Least possible overhead
				2. Trivial to implement
				3. Natural 
			
			Cons [problems]: 
				1. Little-to-no custom validation logic: 
					SOLUTION: add it; 
						the 'ErrorHandler' be passed the given Stream; 
				2. Forces stopping of the parsing:	
					SOLUTION: do not; 
						Add the ability to return an additional value from the 'ErrorHandler': 
							either '[false, true]' - ContinueParsing (error), or '[false, false]' - HaltParsing (error). 

			
		[SOLUTION 3] ASTValidator - for 'ASTAnalyzer/ASTNode' APIs: 
			1. Due to homogeniety, it is extremely convinient/fast to check for the "right" '.lastChild' value 
			2. Due to homogeniety, it is 
			3. [Summary] The 'ASTHierarchy' idea could be used to verify the tree post-parse

			YES, DO THIS
		
	2.29. Later - improve the tests: 
		2.34.1. Add generics to the class-tests signatures [just for the sake of it - very pretty and give a lot more sense to using TypeScript for tests]; 
		2.34.2. Create a way to print out the class tests' instances (via a special 'instance' library function, based off 'it'); 
	
	2.30. Implement a new interface for the library - FiniteEnum: 
		This, basically, creates an 'EnumSpace', that consists of all the given items, 
			BUT requires finding out their uniqueness beforehand (using the 'new Set', or the 'one.js' 'norepetitions'). 
			Thus, for instance, these are equivalent: 

				const a = new FiniteEnum(["800", "elorian", "800", 998])
				const b = new FiniteEnum(["800", "elorian", 998])
			
			Thus, the '.size' of the given EnumSpace is actually fluid; 
			Likewise, its' '.add()' would NOT be a number (hence - one NEEDS A NEW INTERFACE); 

	2.31. IDEA: how to speed up the tokenization algorithm [attempt]:
		Way: 
		1. Split the "tokenized" and "untokenized" parts with 2 arrays
		2. Keep the 'indexes' of the tokenized (on a per-item basis) and untokenized (on a "linear" - read below - basis) parts
		3. Split the final result (tokenized) into BATCHES	
			3.1. The underlying is a NEW structure called 'BatchArray'; 
				It keeps its values as a list of lists, which HAVE THEIR INDEXES; 
				The second-level lists are all ordered as the original array is; 

				Operations: 
					
					0. bind(array) - binds the given array
					1. newBatch() - creates a new last batch, to have values written into
					2. merge() - combines the batches into a single array, removing the indexes; 
						For this - use Merge-sort (it's a natural choice here - the structure is precisely what the algorithm requires); 
						Due to quality of data, the complexity (in this case) is O(nlog k) [considerably better than 'O(nlog n)'];
					3. replace(i, subarr, subinds) - replaces a given index with a value of 'subarr'
						This has to be fast, therefore: 

							1. Store the '.bound' NOT as 'any[]', but as a special 'ContinuedElement[]'; 
								It keeps elements as "referenced" (ContinuedElement), and 
									items that are referenced can have: 
										1. a '.value' (when no array is used); 
										2. an '.array' (when one is used, otherwise - null); 
									
								Thus, the replacement-with-array operation becomes O(1);

							2. When this happens, after changing the 'ContinuedElement' one: 

								2.1. Finds the batches that have the affected elements; 
									To make it fast (for general case), store batches NOT just as 'any[]', 
										but also have '.minIndex' and '.maxIndex' properties; 
									Thus, it's possible to determine (in a single check) whether 
										they are affected ('.maxIndex < .changedIndex')
										in O(1) time; 
								2.2. Finds the elements inside the batches; 
									Done via binary search over the indexes; 
									Then - as they are linearly ordered, walks the remainder of the batch-list, 
										doing 2.3.; 
								2.3. Changes the elements; 
									Add the 'length-of-the-replaced-subarray - 1'; 
									O(1) already, no need to change anything; 

								IMPORTANT: for this to be FAST (algorithm becomes O(k^2 logn) and NOT O(nlogn)), 
									one needs to do this step is done in a SEPARATE 
							
							3. When 'replacement' occurrs recursively (inside an existing ContinuedElement): 
								3.1. It's the same, only difference is - one has to calculate the index manually (which can be mildly cumbersome, but is still O(n)); 
							
							4. The '.indexes' list, then, is ALSO altered; 
								The old 'untokenized' index is "deleted", new UNTOKENIZED items' indexes (subinds) are put in its stead; 
								It is ALSO kept as a 'ContinuedElement[]'; 
								When an index is JUST deleted (that is, it's tokenized wholy and singl-y), 
									one replaces the value with 'null' (no '.splice' necessary); 

								To find it, one has to: 

									1. Take the first 'ContinuedElement' x: 
										1.1. Increase the value by (if array has no) 'x.array.length', or (if no '.array' is used, just '.value') 1
											To make checking for presence of a 'ContinuedElement' inside 'ContinuedElement' faster - add a flag-property ('isRecursive: boolean'); 
											BETTER IDEA: 
												Use a 'BitArray' (a new data structure - a UInt8Array/UInt32Array, whatever, that keeps flags as bits); 
												ANOTHER IDEA: 
													Use a 'BitArray' to store a number [together with a boolean]; 
													This way, it can have less bits, and more appropriate for a given usage ("cram it" with the flags - these go first); 
													Then, one can also KEEP TRACK of the first 'ContinuedElement' [helps, when it's > 0]; 

													ANOTHER number stored - count of recursive 'ContinuedElement'-s [allows to sometimes finsh early]; 
													IDEA: 
														Keep not the FIRST 'ContinuedElement', but the one that is THE FARTHEST! 
														Then, what we have is - the largest possible speed increase; 
														This also requires an ADDITIONAL number getting stored: separate BitArray (for the index that is skipped); 
														Remainder are linearly walked-through; 

														BETTER IDEA STILL: 	
															Store it upon the 'CountedElement'-s that are using '.value'; 
															THEN - there is no need for the flag, as one ONLY works with 'the next' value; 

										1.2. Proceed until the happenstance that it becomes equal to the sought-after index, OR 
											the difference between the current one, and the next becomes "too far" to be used as an intermediate-point; 
											From there - one does a literal jump (as there are no more "nested" indexes left); 
										
									Therefore, our '.indexes' is capable of getting the current index at about O(k), where 'k' is the table size (number of tokenizations); 

									IMPORTANT: the '.indexes' is actually A 'ContinuedElement' ITSELF; 
										Thus, all techniques can be used on it also...; 
						4. indexSync() - see '3.->2.->IMPORTANT'; 
						5. getIndex(prevIndex: number): number - for iteration, see 'Fields->2.'; 

				Fields: 

					1. .bound: ContinuedElement[] - the array of items; 
					2. .indexes: number[] - the list of indexes that are NON-REMOVED [used for iteration]
						IDEA: instead of keeping it as a list of numbers, use 'BitArray'-s, with a changing memory outfit; 
						This permits to (greatly) economy the memory, because to see if an index is "present" one just 
							marks it as 1/0; 
						The "nested" ones will require more memory; 

						(
							Possibly... One could save *some* memory with a complex bit-layout?
							Thus, for instance, having, 01 mean 'non-nested, present', 00 - 'non-nested/nested, missing', 
							'1' - 'nested' [only a single (first) bit used]; 

							Also - there'd be another - 'GrowingNumber' (new library data structure; this time - a single-element UInt8Array/UInt16Array/UInt32Array as a base, not UInt8Array); 
							This one'll keep a single number; NOTE: the thing is needed at all even because of the '2^53 - 1' number-bound (namely, that one doesn't like it...)
							This is (basically) a wrapper around a number to allow for quick manipulations that are ALSO memory-efficient

							'Nested' ones will USE THE SEPARATE INDEXES inside the '.indexes: (...)[]' array; 

							Then - one'll be able to have ANOTHER method on this, called 'getIndex'; 
							It'll return the next non-missing iteration index, based off the current one; 
						)

						Also - regarding the 'automatic' empty return from 'tokenization' function - this won't work here (too much per-element overhead); 
						Conclusion - one has to check the return-array length FROM THE START; 

						PROBLEM: consider this - '.indexes' becomes PARTITIONED! 
							More importantly - a given portion now becomes more about "missing" than "present" 
								non-nested bits; 
							SOLUTION: reverse the bits inside the given 'nested' bit; 
							To determine WHEN to do it - count the "missing" and "present" parts; 

					3. .batches: [any, number][] - the list of indexed elements inside the array
					4. .syncInfo: [number, number, number] - the list of REPLACEMENTS that has been done; 
						Used for increasing speed (namely - DELAYING the slow index-sync operation); 
						Has form of '[batchHappen, replacementLength, replacementIndex]'
						
						1. Because one KNOWS that for all things that are AFTER the 'batchHappen', the index is "fixed", we also know
							that only 'batchHappen' batches need be checked at all - O(k^2), where k^2 is the size of the table; 
						2. Because one KNOWS that there are '.minIndex' and '.maxIndex' for a given batch (and it's ordered), 
							one can find out whether one has the desired changed indexes - O(1)
						3. Because one has the indexes as NUMBERS, one can use binary search - O(log n)

						IDEA: for an even BETTER optimization - to have better memory consumption, a more complicated structure: 

							1. STORE THE INDEX-ADDITION INFO on a per-batch-basis' (buckets)
							2. Store them in a linearly ordered fashion (id est, so as to PRESERVE the values...); 
						
							QUESTION: how much will this book-keeping piece cost? (the 'number' triple only requires O(1))
								To do this, however, one is in need of: 

									1. get the triple; 
									2. put it into one of the 'batch-baskets' of the '.syncInfo' (based off 'batchHappen'); 
									3. in the chosen basket - '.push([index, length])'; 

									[post-creation, when calling '.sync']
									4. later - get the value from the appropriate bucket (they are ITERATED from 0 to 'curr-batch-index' for '.batches'); 
									5. apply all the transformations to the given batch; 
								
							ANSWER: not much in terms of performance; 
								ALSO - it's actually better in terms of cache-locality (because there's less jumping around); 

							ALSO: the two numbers are kept as GrowingNumber-s! 
		Result [theory]: 
		1. O(n) additional memory usage (where 'n' - number of final tokens)
		2. O(1)-O(n) performance gain (depends on number of tokenized items, least waste in cases when there's a lot)
			The current "speedy" O(n^2) (O(n) - walk through the items; O(n) - the '.splice' in replace due to index-keeping) 
				can be replaced with a "slowish" (meaning - one with a possibly significant factor) O(n).
			This ability is important, and in certain applications can be considerable; 
			Only questions are: 

				1. When can this be used? (use-cases)
				2. [Most important] Can use-cases be identified prior? 
			
			Generally, for: 

				1. Large inputs; 
				2. Large (> 15 items) tokenization-tables; 
			
			This could make some sense. 

				1. The '.splice' requires one to go SEVERAL TIMES through the 
					ALREADY TOKENIZED elements (the reason for inner O(n))
				2. The 'isType' requires performing redundant checks; 

			In expense of memory and: 

				1. Book-keeping cost (a noticeably more complex algorithm that runs in O(n) linear time + lots of index-keeping); 
			
			this implementation would permit to remove these; 	

			Final performance is: 

				O(nk^2log n), where 'n' - number of tokenized elements, k - table size

		CONCLUSION [post micro-benchmarking]: 
			1. After some little fiddling with benchmarks, one found out that (actually), 
				'.splice' is optimized to be extremely fast; 
			2. One'll need to check the optimization difference betwee this and '.splice'
				(very well may be that this turns out to be slower regardless)
			3. (Theoretically), the '.splice' STILL copies the array, even though it happens on a 
				MUCH lower-level (C++, Asm); IF it's faster (which is quite likely, indeed), 
					that is only due to the purely numeric part; 
			
			4. Important: 
				(From what is known insofar) The only "really" slow part of the algorithm, 
					that genuinely would ruin the relative performance to repeated '.splice' (which, for a large enough input would take literally forever...), 
						is the "setting up". 

				Namely, the process of "guessing" the number of 'nested' indexes [their tree]; 
				One would be required to "grow" it accordingly, with a relatively few size-increases for it...; 

			5. Before adding ANY memory-saving data structures 
				1. Benchmark for memory; 
				2. Benchmark for speed; 

				Because if either is absent, the whole thing could simply collapse due to speed/memory assumptions going horribly wrong; 

			6. FINAL: if it's slower, REMOVE; 
				First - implement and benchmark performance for some large inputs (see if faster at all, likely not); 
				Second - see the 'time-difference/memory-difference' ratio, see if it's worth it (very likely not); 
				Third - either delete, or keep; 

	2.32. Performance concern: '.push(...elems)' FAILS for large arrays; 
		Problem - with the '...elems' spread operator: it creates a new array; 

		1. Do '.push()'-es in batches, or avoid them completely....; 
			[Idea: use ContinuedList-s instead? - new data structure
				Same thing as 'ContinuedElement', but without the '.value' to store, 
					and that references ANOTHER ContinuedList, instead of just an array]; 
		2. Better idea - where possible, use "mixed signatures"
			(example: 'string[] | ContinuedList | string[][]' for 'StringCollection.push'); 
			Then - the methods in question have a higher versatility; 

	2.33. [new type] BitArrayHash; 
		Accepts a BitArray, returns a value from it; 
			A hash based off bit-shifts (<<, >>); 
		Use for defining the '.init', '.curr' and other choice functions in the StreamClass; 
		BitArray - a UInt8Array (or other) array. Look into the '2.35' (pattern-tokenizer speedup) for details

	2.34. [TypeScript; clarity] Add the 'type ForcePresence<T, K extends keyof T> = {[x in K]-?: T[]}'
		This (and similar ones), for adding/removing various type features, are handy for refactoring (appears at least in StreamClass); 
		ALSO: 

			1. Doing this COULD allow one to be switching from the "composite interfaces" to polymorphic ones with optional properties; 
				(Even create a SINGLE RIGID 'Stream' interface! [NOTE: the OLD ones AREN'T GOING ANYWHERE - they're just getting re-formulated...]; 

					interface Stream<Type = any> extends Summat {
						pos?: number
						buffer?: FreezableBuffer<Type>
						state?: Summat
						prev?: () => Type
						isStart?: boolean

						init?: (...x: any[]) => Stream<Type>
						navigate?: (pos: Position) => Type
						finish?: () => Type
						rewind?: () => Type

						curr: Type
						next: () => Type
						isEnd: boolean
					}

					Plus, one COULD start using the OPTIONAL per-element interfaces [then, using the 'ForcePresence' with a "stricter" interfaces]; 
					This, in particular givest the benefit of being able to recognize properties on a "lower" type WITHOUT requiring an 'as' (no significant type downgrading); 

					2.49.1. ALSO  - create a single RIGID 'StreamClass' interface! [for the implementation - write down the optional '.pos, .buffer', and so forth... Missing currently...]; 
				)
	
	2.35. Use C preprocessor with this code; 

		Getting tired of typing "optionalValue(this, value)", and other such things in repetative situations (like 'init.ts'); 
		Abstract those away with a good old '#define' + '#include'; 
			Then - introduce C preprocessor tools to the project's build; 
				The build process, then, would then be : 

					1. steps: 
						prep/ (with-preprocessor dir, via 'make') -> ts/ (pure TypeScript) -> dist/ (pure JavaScript)
					2. extension: 
						change from '.ts', to stop the VSCode from doing its thing; 
					3. create a basic theme for this "mixed" TypeScript + C-preprocessor thingie [VSCode extension idea]; 
						For syntax highlighting

		Due to library's (intended) reliability in therms of name-usage, it ouhgt to be ok; 
		ALSO - do this only AFTER everything else in the library was ALREADY DONE! 
		Due to the fact one doesn't want to touch its code again after the v0.4, this particular "hybrid" 
			language shouldn't become a thing to cause problems [that would need separate solving] during development; 
	
	2.36. Liberate the 'Parser/' directory as well? 
		Due to the reformulation of the library's key terms through 'StreamParser', it has become depressingly empty; 
		Also - introduces some unnecessary higher-level "bunching"; 
		This way - there'd be global functions that are exported from the package - PatternTokenizer, PatternValidator, for starters...; 
		Although - there MAY be come new things coming into the 'Parser' (the new memory-hungry patter-tokenization algorithm, for instance?)

	2.37. About the continiously occurring '[A, B][]' types in code: 
		Fix those; Replace with 'Pairs'; 
	
	2.38. Increase the Library's natural debuggability: 
		ADD names to all the nameless functions. 
		This is due to the fact that in error logs, they would appear A LOT more clearly if one did do it. 
		ALSO - do the same for 'one.js'; 
	
	2.39. [maybe?] Add another 'hash' function for the 'HashMap'? 
		In particular, use the 'extension' for 'set', 'replaceKey', 'delete' (by-original-key), 
			and another hash (the 'indexHash') for 'index' [this one will be optional and default to 'extension']; 
		This is in parallel with how the 'IndexMap' has it.
		Motivating example: 

			1. Indexes are objects with a field 'T: keyof X'; 
			2. Indexed are the objects with a field 'R: keyof X'; 
			3. One wants to create a HashMap with 'N = HashClass(...)', such that '(new N(...))(T) == smth'; 
				Problem here is that one EXPECTS the same hash to work for both the 'index' and 'delete'. 
				In general, the 'HashClass' does NOT support the categorization (even though in the case of 'SimpleTokenType', it works); 
	
	2.40. [new class] Traveller, a point on a Stream; 

		Prior sketches of the v0.3 included a '.navigate' method that implemented an absolute positioning inside a Stream.
		This was scratched in favour of relative positioning, which is more widespread. 
		The absolute positioning, however, has a very powerful use case, provided more things are added: 

			1. StreamClass - a new flag '.size', for remembering the precise size of a Stream, in terms of the last position that 
				is visitable; 
				This is useful for: 

					1. Remembering a Position, at which the portion of the Stream starts; 
					2. Traversing the precise distance until the end; 
						The hard numeric bound gives a speedup due to the lack of need for checking for '.isCurrEnd()'
							(more precisely, one can replace it with a 'this.pos < this.size'). 

						For this, the 'isCurrEnd' has to be __'.writable'__; 

				For efficient implementation of this, one must be able to get to the beginning of the portion in question; 
				Therefore, a custom implementation of '.travel' is necessary. 
				Although, one can define a default that works by an efficient implementation of '.navigate()' and '.rewind()': 

					[sketch]
					travelDefault = function (pos: Position) {
						if (isNumber(pos) && this.pos - pos < pos)
							return this.navigate(pos - this.pos)
						else {
							this.rewind()
							return this.navigate(pos)
						}
					}

				Then, one can have: 

					1. A WrapperStream, on a given Stream, with '.size = true'; 
					2. Remember a Position, where WrapperStream starts on a Stream using a 'Traveller'; 
					3. Define an efficient '.rewind()' on the WrapperStream, that would use the 'Traveller' in question, after fully iterating through it; 
						Another reason for formalizing the WrapperStream; 
				
				This is absolutely priceless for lazy parsing (skipping pieces of data in order to gain information about it first 
					- then doing the parsing, on a need-to-know basis). 

				List of Stream-creation classes to make into WrapperStream-s: 

					1. LimitedStream; 
					2. NestedStream; 
					3. PredicateStream; 
				
				Like StreamClass (on which they'd be based), these are all generated classes;  

			2. In accordance with '1.', a new default method for StreamClass - '.travel' (does an absolute navigation around the Stream)
			3. To remember particular Positions on a particular Stream, that may need to be recalled later (as per '1.'), one has a new 'Traveller' class: 

				[sketch]
				Traveller extends PositionObject<Position> {
					value: Position
					stream: TravelableStream
					convert() {
						return positionConvert(this.value)
					}
					travel () {
						return this.stream.travel(this.value)
					}
				}
			
	2.41. Problem: non-reactive '.isEnd' property; 

		[Note: WrapperStream - a Stream with '.value' being another Stream]
		This is the issue with keeping it as a property, and not a getter. 
		Consider this: 

			1. A Stream is made; 
			2. A WrapperStream around it; 
			3. The underlying Stream is modified (example: .travel() to the middle of it)
			
			Result:

				1. The underlying Stream's '!.isEnd', whereas the WrapperStream '.isEnd'; 
			
		Conclusion: one needs another (more general) interface to express dynamic relationships of Streams' '.isEnd' property. 
			In particular: 

				1. The scenario in question ONLY occurrs when one needs to be able to SAVE a WrapperStream around a given Stream; 
				2. Therefore, one can safely say when it is reused, and (therefore), when one needs to update the '.isEnd' for veracity; 
					Conclusion [1]: add a new method for this '.updateIsEnd()'; 
						It does precisely this [in StreamClass implementation]: 

							this.isEnd = this.value.isEnd

					Conclusion [2]: For this, one'll need to formalize the 'WrapperStream' interface (because, otherwise there's no guarantee that the 'value' is indeed a Stream); 

	2.42. Refactor creation of PropertyDescriptor-s; 
		They are all over the place, order it, make an ordered, simple and maintainable signature,
			that would allow one to express PropertyDescriptorMap-s as objects; 
	
	2.43. [refactoring] Add a new util: 
		tokenCompare = (tt: TokenType) => (x: any, y: any) => tt.is(x) && tt.is(y) && value(x) === value(y); 

	2.44. TreeStream: flag optimizations; 
		Allow 'TreeStream' to be a Stream-class-generation function also; 
		Permit the TreeStream to have overloads for: 	

			1. navigate (work with number-indexes, when '.buffer' is present); 
			2. next		(same thing - '.buffer' + '.pos')
			3. prev		(same thing - '.buffer' + '.pos')
			... [and so forth]
		
		Will allow to have the benefits of flag-presence via the overloads; 
	
	2.45. [static method] MultiIndex.fromPosition(stream: TreeStream, position: Position): MultiIndex
		This is a method for converting a 'position' from the given TreeStream to a 'MultiIndex'; 
		Does NOT '.rewind()'; 
	
	2.46. [Big one] Add support for proper PatternMatching;
		Serves as an abstraction over the 'RegExp'; 
		Allows one to match 'Token'-s using 'MatchablePattern'-s; 
		Would have: 

			[1. matching functions]
			1.1 StreamMatcher - works on Stream-s of items that are '.match(...)'-able [as arguments for 'MatchablePattern.match']; 
				Expects as input a 'MatchableStream', or a similar thing; 

			[2. MatchablePattern-s]
			2.1. [StreamClass] MatchableStream 	- walks through a given Stream of items, producing new 'MatchablePattern' on each '.match()' [ex: calls '.next()' on every '.match()']; 
				Depending on the output of the '.match' ('true', 'false', 'null', number, or PredicatePosition), either: 

					1. 'true' 					- goes forward 1 position; 
					2. 'false' 					- halts (mismatch)
					3. 'null' 					- goes backward 1 position;
					4. number   				- goes forward/backward specified number of positions
					5. PredicatePosition 		- goes forward/backward until specified condition on a 'MatchablePattern' is met; 

			2.2. FlexibleMatchable				- a 'MatchablePattern' defined by a particular user-given function; 
			2.3. [ChildrenTree] MatchableTree	- a 'ChildrenTree', that chooses one of the matching "paths" depending on the result of the '.match' function of its own; 
				Useful in combination with 'TreeStream'+'MatchableStream' - can create "matching trees"; 

			[Possibly, add more stuff... LOOK FOR CASES OF APPLICATIONS!]

	2.47. [codebase consistency] The '.init' methods for StreamClass DO NOT return 'this': 
		Fix this - let them. All the other '.init' methods do it; 
		(This, in particular, generally permits usage like: 

				const t = new InitializedClass().init(...)
			
		In cases, when the constructor is NOT the same as the '.init(...)';
		Likewise, a different semantics [copying] for '.init', when a partial copy is necessary: 

				const t = new InitializedClass(...)
				const r = t.init(...)
				t === r // false
		)
		
	2.48. [maybe?] Turn the 'TableMap' and 'MapWrap' into 'FlexibleFunction' derivatives? 
		This would [pro]: 

			1. introduce some consistency into the library; 
			2. reduce "dynamic" addition of properties to objects (particularly - functions); 

		However [con]: 

			1. may cause a performance penalty (check); 
		
		IF using FlexibleFunction is NOT slower (in terms of function calls), then do just that...; 
		The 'FlexibleFunction' is always going to be slower on creation (due to: 1. constructor; 2. runtime-parsing of FlexibleFunction), 
			but this shouldn't be too costly overall (besides, when string is static, it well may be cached to improve performance); 

	2.49. Change the tests using one's the 'tests.js' library; 

	2.50. Add more complex 'Tree' utilities. Based off previous 'one.js' v0.3.1, add: 

		1. deepSearch - searching inside the given Tree for a value with a given 'prop'
		2. depth - calculating the depth of the given Tree
		3. treeCount - counts the values of a given predicate for all items in the tree

		2.51.1. Add new methods to 'ChildrenTree': 

			1. .reverse() - recursively reverses the given tree

	2.51. [maybe?] Do some amazing TypeScript jiggery-pokery:
		Sometimes, the type can be more complex to predict [but STILL might be possible via templates, type-inferences and special functions like 'Parameters'...]: 

			[LinearIndexMap/methods.ts]
			1. extend
			2. extendKey

	2.52. Type Simplification: 

		Library boasts some pretty complex types. 
		Particularly, this applies to: 

			1. StreamClass
			// ... [expand the list]
		
		Use utility types to make life easier. 
		Also - REMOVE some of the excess types. 

	2.53. Naming conventions - the post-name-'Type' doesn't look NEARLY as good as the classic prefix-'I'; 
		Replace [note: with LOCAL imports *only* - things like 'PointerType', say...]. 

	2.54. CREATE A proper website with documentation for the library. 
		Do benchmarks, et cetera...; 
		After doing GitHub Wiki for v0.3, see if it cuts it (spoiler - it likely won't one STILL wants the ravishing types for the library's docs to be present!)
			For this: 

				1. Learn a new CSS-based tool (either for generating/hosting docs/docs-css-styles like TypeDoc/readthedocs , 
					OR a new library to create one's own doc-styles like Tailwind CSS); 
				2. Create a JSON parser in parsers.js and COMPARE it against the [https://chevrotain.io/performance/], with ops/sec. 
					See, WHICH libraries have managed to beat parsers.js v0.4., and WHAT can one do about it (see their optimizations, specifically...); 
					2.1. ALSO: create a benchmark for MEMORY USAGE [more concretely - compare parsers.js with the others...]; 
						The memory usage should be noticeably better than speed...; 

					2.54.1. One MAY want to adopt a new Tokenizer: 
						Specifically, a more "traditional one", which CAN use regular expression 'RegExpTokenizer' and/or 'RegExpLexer' (for num-positions...). 
						Reasons: 
							1. StreamParser as a tokenizer SHOULD be performant (if used correctly - with 'match' util)
							2. PatternTokenizer is SHOULD be SLOW AS FUCK because of how simple its algorithm is...
								PROFILE IT LATER [again, when creating new parsers...]
							
	55. MultiMap - [IndexMap-like] this returns ALL the matched items, and while running multiple results 
		[with REQUIRED function-values, and only unless early termination has been demanded by a return value]; 

	56. Create an 'ASTAnalyzer' class [under new 'Tree.AST' module]: 
		1. gets an 'AST', defined by: 
			1. Trivial nodes - 'TokenInstance'-like
			2. Content nodes - 'Token'-like or 'TokenType'-like
			3. Recursive nodes - 'TokenType'-like, with arrays for values
		2. Traverses the tree: 
			1. New abstraction: ASTNode - class, with '.from' method: 
				1. The '.from' method takes a JavaScript object (JSON), 
					with values as described above, and transforms it INTO A PROPER TREE, 
						using: 
							1. ChildlessTree
							2. SingleTree
							3. MultTree
				2. The ASTNode is a 'ChildrenTree' [see Tree/classes.ts]: 
					1. Namely, it is based off a 'ChildrenTree' descendant (it's a class-generating function), already provided ones are: 
						1. ParentTree [for quick iteration via 'TreeStream' - better for subsequent reads]
						2. TrivialWalkableTree [for better memory footprint - better for initial writes]
				3. ASTNode can be created via a constructor, like any other tree [uses the 'constructor' of the given parent class]: 
					1. Namely, it employs a specific 'converter' function [one that converts the elements "simply"]
					2. Keeps a '.type' property on itself (it's a TokenInstance)
					3. There are *3* types of ASTNode-s [3 distinct classes - for optimization purposes]: 
						1. RecursiveASTNode - without '.value' and '.children: ASTNode[]'; NOTE: here 'ASTNode' is an interface
						2. ContentASTNode - with '.value: any', without '.children'; This is just a 'TokenType' with additional prototype-methods
						3. TrivialASTNode - with '.value', without '.children'; This is a 'TokenInstance', with additional prototype-methods
			2. By doing 'ASTStream(TreeWalker(ASTNode.from(...)))', one can iterate a given generated JavaScript AST object! 
				2.1. This is what the ASTAnalyzer (actually) is doing; 
				2.2. The 'Stream' is, then, applied a 'StreamParser' upon; 
				2.3. NOTE: that the 'TreeStream' used is CONFIGURABLE [namely - 'ASTAnalyzer' is, too a function, taking in and returning A CLASS!]
				2.4. About 'ASTStream' - see below...

		3. Returns information: 
			1. [Buckets-Categorization] Arrays of '.trivial', '.content' and '.recursive' nodes
			2. [Type-Categorization] Returns a 'HashMap' of arrays of nodes, filtered by their '.type'; 
				NOTE: the 'HashMap' is, TOO, completely configurable by the user [user-defined (if provided), else - default being: when 'string's are used for 'type's - ObjectHashInternal, otherwise - HashMapInternal]; 
					(another function-argument for returning the final ASTAnalyzer class)

		4. Provides functionality [ASTAnalyzer]: 
			1. mapTypes((type_name: string) => string): 
				creates a new 'ASTNode', by means of mapping the '.type'-s
					of the current one using the given function. 
				Immensely powerful for changing between different data formats. 

			2. mapValues((x: any) => any)
				creates a new 'ASTNode', by means of mapping the '.value's of each 'ContentASTNode', 
					while preserving all else
			
			3. .find(type: ..., pred: (x) => boolean): 
				Seeks an item in the tree with a type '.type', obeying the predicate '.pred';
				Optimizes to look specifically in a '.type'-bucket of the given 'type'. 
				Can be radically faster for large inputs. 
			
			4. .find(pred: (x) => boolean): 
				if the 'type' is not passed, the entire tree is searched; 
				Optimization - uses prior obtained collections for increasing cache locality of large searches. 
			
			5. .iterate(type) - returns a Stream, filled specifically with values of type 'type'; 
				Namely, it returns an 'InputStream', wrapped around the respective '.type'-bucket; 
				The items are listed on a from-beginning-to-end of the 'Stream' used to construct the ASTAnalyzer; 

			6. .filter(pred) - returns a new tree, such that it ONLY contains elements that obey the given predicate: 
				HOWEVER, one can also mandate that in order for a sub-tree (RecursiveASTNode) to remain, 
					at least one of its children must be 'true' as well. 
					This is done by means of returning `null`, instead of `true`/`false`.  
			
			7. .search(type: any) - searches, and returns for multi-index information for each of the elemenets of a given type. 
				Optimization: uses ONLY JUST the information from a given '.type'-bucket; 
				This allows one to: 
					1. Iterate a portion of the tree INSTEAD of needing to start "at the beginning
					2. Know exactly when to stop (seeing the *last* item that needs to be iterated over)

			8. .map((x: ContentASTNode | TrivialASTNode) => any): 
				Maps all the non-RecursiveASTNode parts of the tree to a different 'Tree'. 
				Useful for creation of generalized non-AST trees, to be used with 'TreeStream': 
					Example, passing a function that returns 'string', then - CONCATENATING all 
						the items inside the obtained Tree, via: 

							// [sketch - no types]
							function GenerationFunction(F) {	
								return function generate(ast) {	
									return array(TreeStreamPre(ast.map(F)), new UnfreezableString()).get()
								}
							}
							
							// this is to create either a SUBSET defined by a recursive IndexMap-like function, *or* via working through a '[type]'-defined set of "superior" nodes; 
							// NOTE: 'ast.types[type]' is an *ARRAY*
							function GenerationRecursive(F, type) {	
								return function generate(ast) {	
									return array(InputStream(ast.types.index(type).map(F)), new UnfreezableString()).get()
								}
							}

					IDEA: ADD a set of 'samples' to the library - see above...

				Also - '.map(F)' OPTIMIZES, so, it expects the given values to be returning FUNCTIONS, for plugging in their children's 'F(x)', 
					so: 

						1. A -> B
						2. F(A) -> X(K): X(B) == string
						3. F(B) == string
							F(A)(B); 
						
				The optimization is - SPLITTING the '.types'-buckets via doing '.types[name].map(...)'; 
			
			9. RecursiveASTNode: 
				1. Contains various copying .value-Array-delegate methods: 
					1. .map(f) - creates a new RecursiveASTNode via 'new RecursiveASTNode(this.type, this.value.map(f))'
					2. .filter(f) - creates a new RecursiveASTNode via 'new RecursiveASTNode(this.type, this.value.filter(f))'

		5. Optimization information (additional): 
			1. IDEA: add a new 'ASTHierarchy' information object, which: 
				1. Specifies, what '.type'-s CAN be found within which '.type'-s of RecursiveASTNode-s: 
					IMMENSELY powerful for search-optimization inside a given Tree 
						[limits the '.type'-buckets in which one needs to look]; 
				2. Specifies maximum "depth" of a certain particular 'RecursiveASTNode': 
					1. Allows one to replace a continuous check of 'isGoodIndex(.lastChild)' with 'isGoodIndex(.lastChild) && i < MAX_DEPTH_COUNT'; 
						Guarantees a performance bound. 
						When the bound is PRECISE, the 'isGoodIndex' can be dropped
				3. Specifies maximum length for '.children' of a given 'RecursiveASTNode': very useful for iteration! 
					1. Replaces '.children.length' check with a 'counter < FIXED_PRECOMPUTED_LENGTH'; 
				4. Specifies expected structure for a given 'RecursiveASTNode': 
					1. May permit one to be going one-by-one downwards for it, EXPECTING the given '.type's; 
					2. Eliminates checks for '.isSiblingAfter'

				'ASTHierarchy' is optional, and its purpose is solely to: 
					1. provide algorithm optimizations
					2. enforce bounds for a given format

				NOTE: 'ASTHierarchy' is ONLY good for optimization, when IT IS PRECISE 
					[id est, we can SKIP certain function-calls/checks]. 

			NOTE: optimizations work via: 	
				1. ASTStream-s: 
					0. Alternative to 'TreeStream': less generic, provides better optimizations for AST: 
						1. ASTStreamPre - alt. of TreeStreamPre
						2. ASTStreamLevel - alt. of TreeStreamPre
						3. ASTStreamPost - alt. of TreeStreamPost
					1. Accepts an 'ASTNode' instead of a 'Tree'
					2. Optimizations: 
						1. No '.lastChild' presence check in '.isChild()' method - instead, just '.lastChild > -1'	
							This is a good optimization for TreeStream-iteration of large trees ('.isChild()' is called ON EVERY '.next()' call); 
						2. Algorithm Configurable via 'ASTHierarchy':
							1. Optimizations are present ONLY when ASTHierarchy is PRECISE
							2. Expects a VALID abstract syntax tree

			2. For '.type's field values, one uses EITHER an 'ObjectHashInternal' (if strings), or a 'MapHashInternal' (if not strings); 

	57. PreallocArray - a new 'Collection' type: 
		0. new property - 'lastInd: number = this.value.length'
		1. extends ArrayCollection
		2. new method - '.prealloc(n)': 
			1. If 'this.value.length > n', goto 5, else - continue
			2. this.value.length = n
			3. this.lastInd = this.value.length
			4. Finish
		3. change method - '.push()': 
			1. this.value[this.lastInd++] = this.value
		4. change method - '.get()' [as 'readonly number[]']: 
			1. return this.lastInd < this.value.length ? this.value.slice(0, this.lastInd) : this.value
		5. change method - '.move(n)': 
			1. this.value.length = this.lastIndex
			2. const lastValue = this.value
			3. this.value = Array(this.n)
			4. return lastValue

		Purpose: 
			1. Providing an interface for user-level preallocation of an array
			2. Providing 1. together with an integration with the 'Collection' interface

	58. Add "move semantics" to 'Collection's: 
		1. '.move' method - for a 'MoveCollection extends Collection': 
			1. Sets the 'this.value = []', or to a similar "default"/"empty" value. 
			2. '.move()' returns 'this.value'
		
		This permits to achieve the 'PreallocArray' idea WITHOUT the need to copy/change the contents of THE SAME array. 
		CONCLUSION: YES, add the '.move' to SOME collections via 'ArrayMoveCollection' with '[]'; 

	59. [types] Maybe - split the "specific" composed 'X extends A, B, C, ... {}' types from the elementary 'A, B, C, ...': 
		Currently, they are being exported within the same 'type'-module as the first (original/closest) usage. 
		However, one could also (instead) export them at the special 'interfaces.mixin' level [which, basically, contains the "small" interfaces]. 
		
		One COULD otherwise put them into a single 'refactor.ts' file, HOWEVER, then becomes the problem of needing to actually export some of them 
			[ex: Indexable]. 
		
		Thus, divide them on: 
			1. "unused" (src/refactor.ts) - those that DON'T appear in any function definitions
			2. "used" - export them IN THE SAME module as their ontology best demands

	60. FlexibleFunction - make a dependendency, a "mini-package" named "callable"; 
		Export the function there as 'Callable'

	61. Iterator Helper functions - '.filter', '.take', ...: 
		Provide them. 
		Use them in the library definitions that actually EMPLOY them [example: PositionalValidator would benefit greatly from a single '.filter()' call]; 

	62. *Add* ability to store indexes of various AST nodes within the original 'FreezableBuffer'
		object passed to the InputStream: 
			Rather a curious problem here, as the Stream that issues the error HAS TO KNOW what index 
				the original input 'String' has that the parsing starts at. 
		
		SOLUTION [idea/attempt]: an 'IndexBuffer' (a FreezableBuffer) wrapper: 
			1. This accepts a 'number[]'
			2. Requires making the given 'Token' an 'IndexAssignable': 
				1. Create a new class 'IndexedToken', one that would also be having an '.assignedIndex' property; 
				2. '.assignedIndex' stores the INDEX inside the '.indexBuffer', that the user can store on (say) .state of the given StreamParser;
					1. IndexBuffer has THE SAME length as an actual '.buffer' would, EXCEPT that it *doesn't* get to store the values, ONLY just the 'indexes': 
						1. For this, USE '.pos'; 
						2. [big maybe...] Enable storing the .indexBuffer as an optional 'stream.indexes' property? 
							1. Would automatically enable '.pos' property
							2. Would keep the '.pos' associated with each object
							3. Would assign '.assignedIndex' to the thing...;
								3.1. Would expect 'IndexAssignable's as its items; 
							
							4. Then - create an 'RecoverableStream' [from StreamParser], which would do that sort of thing...: 
								1. keeping positions for future error-reference
			3. The 'IndexBuffer' stores FINAL indexes (in the original 'InputStream')

		Then, one can do a setup: 
			0. All the Stream-s have a running '.pos' that is being '.push'ed to the respective '.state.indexBuffer'

			1. InputStream - contains the original '.buffer' (no '.state')
			2. [first] StreamParser - tokens contain indexes inside the initial parsed string - 'InputStream.buffer'
			3. [higher-level] StreamParser - tokens contain indexes within the '.value.state.indexBuffer': 
				This way, the chain goes: 

					// rough sketch: TODO - add this full-on as a 'util'; 
					function findErrorOrigin (errToken, stream) {
						let index = errToken.assignedIndex
						do index = stream.value.state.indexBuffer[index]
						while ((stream = stream.value) && stream.value)
						return stream.buffer.get()[index]
					}
				
				Purpose of keeping the '.assignedIndex' on EVERY level, is that IN CASE that this is a parsing error 
					that can be discovered early, IT WOULD [alternatively - during later validation, one may want to provide a more precise error information]

	63. Add a 'binarySearch' utility: 
		1. For IndexBuffer - as it keeps 'number's in an ordered fashion, there is no reason not to employ it...; 
		2. This is generic (meaning, it provides a map 'f', and then compares via 'f(a, b)', with default being '(a, b) => a < b'); 

	64. 'InputStream' - provide a '.peek(n)' method: 
		1. looks "forward" n items; 
		2. add a new interface - PeekableStream
			2.1. Add a general 'StreamClass' implementation: uses a 'PreallocArray' (empty by default), which 
				keeps the 'next n' items, to be looked up. 

				1. For when '.buffer' is '.isFrozen === true', this IS NOT necessary, one can (instead) just 
					index appropriately [reflect this...]; 
		3. Permits making more complex parsers

	
	65. [Much-much later; big maybe] A Stack-less CPS-based version of the library: 
		0. TypeScript
		1. InfiniteStack [from stack.js]
		2. CPSArray [from stack.js]
		3. Implements all the current items within the library using continuation-passing style

		Note: keep this for v0.5 ONLY. *Not* as a part of v0.4

	66. A new utility function (Parser/utils.ts) - match(word, stream): 
		For a given 'Indexable', it does: 

			// rough sketch - requires the '.peek(n)' method; 
			// NOTE: the '.peek(0) === .curr' is ALWAYS a requirement...
			function matchWord(word, stream) {	
				for (let i = 0; i < word.length; ++i) 
					if (word[i] !== stream.peek(i)) 
						return false
				return true
			}

	67. Remove (replace):
		1. PatternTokenizer + RegExpMap
		2. PatternValidator + RegExpMap

		With a NEW approach - a: 
			1. 'RegExpTokenizer'
			2. 'RegExpValidator'
		
		These are function-creation functions that is based off *THE SAME SIGNATURE* as 
			the 'RegExpMap', with the difference that these get to be used with 'StreamParser/LocatorStream/PositionalValidator/...'; 
		
		Reasons: 
			1. PatternTokenizer, PatternValidator use THE SAME code
			2. PatternTokenizer, PatternValidator are (both) FAR too generic
				Used only once. 
				It's a waste of generality. 

		NOTE: the 'RegExpMap' REMAINS: 
			Reason: it can still be useful to someone (as a boilerplate saver...)