[v0.3]

Due to TODO swamp, one has decided to SPLIT all the tasks into:
	1. actually relevant (current), which are BEING DONE at the moment
	2. minor ones that WILL get done, but slightly later
	3. the "general", which may get removed/deleted/never-get-done/changed, or (maybe) done
		At worst - they get done later due to being less significant. 
	4. the "leftovers" for v0.3 - this is definitely getting done, and definitely done last - before the publishing
	5. the "v0.4"
	6. the (possibly, not so distant) "future"

NEXT UP:
	TODO: Generic-parameters - STYLE-CHANGE: `Type -> T`. 
		Reason - simplicity of names...
		WALK: 
			1. through classes/ files
			2. through internal/ files
			3. through interfaces/ files
			4. through utils/ files

	TODO [minor]: let the `PreTokenNode` inherit from `Initializer`
		LET these classes provide the `.initializer` property. 
		Reason - style. 
					
CURRENT TODOS:	
	TODO: `SourceU16`, `LoadedU16`, `EncoderU16`: 
		Problem: these are LITTLE-ENDIAN, not BIG-ENDIAN. 
			RENAME these to 'U16LE'
			
	TODO [maybe?]: FreeStream
		IDEA [big change]: [try to] CHAGE the interface of `IStream` to INSTEAD have `.next(): void`
			THEN, we'll be able to INSTEAD keep memory for JUST TWO Shticks, and not 3!

	TODO: make the `.isStart` MANDATORY on `IStream`
		1. REMOVE the `IBackward` interface - completely unhelpful. 

	TODO: `samples`: 
		1. new `Stream` for `samples` [DO add in `v0.3`]. 
			`JSONStream = StreamParser(JSON.stringify)`; 
		2. relocate `Pairs` into `samples` from `classes`. 
			1. It's not a class, it's a function
			2. It doesn't do any allocation [as a rule], just a semantic wrapper
			3. It is used frequently with IndexMap. 
		3. SkipSpaces - TODO: *add* into `samples.Stream` ['Stream' for skipping space characters];
		4. Relocate the `DelimitedStream` to `samples.MarkerStream` 
			1. This is an ".init-composition" (TO BE USED AS A SINGLE STREAM!!!) of: 
				1. `boolean`-based MarkerStream, which relies upon a user-given predicate to determine the `.currMarked` of the given item; 
				2. A `FilterPredicate`, ABOVE which provides the 
			2. ALSO - DelimiterStream
				0. based off 'DelimitedStream'
				1. uses a "has"-based predicate created from a list of possible delimiters/items
		5. ADD NEW ".init-compositions" [NOT `CompositeStream`-s, although that is what is indended elsewhere...] (old pseudocode)	
			1. trivialCompose(IndexStream, InputStream)
			2. trivialCompose(IndexStream, LFStream, InputStream)
			3. trivialCompose(IndexStream, LFStream, LazyBuffer)	
			4. trivialCompose(IndexStream, LazyBuffer)

			IMPORTANT NOTE: 
				after adding `TokenStream`, introduce: 
					1. 'isCRLF' utils - returns if `new Regex("\r\n")` matches AT CURRENT `stream` POSITION (this should be easy to do...)
						Put together with the `isLFSkip` util. 
						More exactly: 
							1. add the `isCRLF = (x: IStream<string>) => new RegexStream("\r\n").matchCurr(x)` [or something...]; 
							2. add the `isCRLFReplace = (x: IStream<string>) => isCRLF(x) ? "\n" : x.curr`
					2. 'isNewlineSkip = or(isLF, isCRLF)'
						This is for the user to be able to instead of transforming "CRLF -> LF", 
							just use this as-is [count the newlines]; 
					3. `LFStream` - a stream returning, for an `isNewline` predicate 
						[not INewlinePredicate, instead (stream: IStream<string>) => string], 
							INSTEAD of items that fit the `isNewline`, the result of the `isNewline`. 
						
						Something like: `return isNewline(x)`
						It's a `StreamParser`
						Put under 'StreamParser/classes'
						Purpose: to use with `Regex("\r\n")` [isCRLF], and `isLF`
							More specifically, to be able to do: 
								'toLF = (x: IStream<string>) => return isNewline(x) ? "\n" : x'
							MAKE this a proper util.

						Reason to introduce - LF/CRLF-independent parsers...
		
		6. scavenge old projects + add common-all-over-the-place type-of patterns: 	
			1. Tokens (names for INode-classes): 
	
				1. Take them from various parsing projects of yours already existing: 
	
					1. xml
					2. selector
					3. regex
	
					Unite, choose best string-names; 
					Create appropriate names for sample-token classes (via TokenInstance) [
						example: 
						
						Opbrack - (, 
						Clbrack - ), 
						OpSqBrack - [, 
						ClSqBrack - ], 
						OpBrace - {, 
						ClBrace - }, 
	
					...]; 
	
			2. arrays with common items (USED throughout...): 
	
				'binary = [0, 1]'
				'decimal = [0, 1, ..., 9]'
				'hex = [0, 1, ..., F]' - TAKE OUT of the global '/utils.ts'
				'ascii = [0, 1, ...]'
				'alphanumeric = [0, 1, ..., 9, a, ..., Z]'
	
				Also - the sets in question should be provided AS-ARE (without the 'inSet'...); 
			
			3. ALSO - join the 'samples' with the 'constants.ts' file 
				[they are pretty similar in that they BOTH provide frequently recurring ambigious constant entities with meaningful "static" names and uses]; 

		7. INode: 
			1. a `default` NodeSystem factory: 
				Uses THREE CATEGORIES. 
				These are defined by types provided by THE USER (it's a factory, not a single object), 
					AND they are [given in that order]: 
						1. TokenNode
						2. ContentNode
						3. RecursiveNode
				Reasons: 
					1. These three are sufficient
					2. As the number of categories becomes lesser, the tree becomes simpler
					3. These three are minimalistic
					4. These three are absolutely necessary (always - in one form or another)
						Most forms (typically) break these three categories on sub-categories, 
							when they need to: 
								1. provide multiple kinds of information [flags, etc]
								2. "flatten" the tree [since greater simplicity leads to greater depth]

		8. WriterStream: 
			1. JSONWriter - a "composition" of `WriterStream` and `JSONStream`, taking in a IStream
				for producing JSON-serializable "shticks". 
				ALSO: it's not a "pure" composition in that it requires: 
					
					1. FOUR new Stream-s: 
						1. FiniteStream(...v): iterates the finite collection 'v[0], v[1], ...'
							An inline, more "tame" version of `InputStream`
							Also - no 'navigate/prev/rewindvA on it '
						2. ConcatStream(...streams): 
							Concatenates the outputs of given streams sequentially 
								like 's_11, s_12, ..., s_1n, s_21, ..., s_2n, ...'
						3. LoopStream(...v): 
							An infinite stream that loops over the finite list of items 'v': 
								'v1, v2, ..., vn, v1, v2, ...'
						4. InterleaveStream(...streams)
							Combines the streams by "interleaving" their values in the following fashion: 
								's_11, s_21, ..., s_n1, s_12, s_22, ..., s_n2, ..."

					2. TWO new methods [on IRecursiveNode - A NEW INTERFACE WITH THESE TWO]: 	
						1. jsonInsertablePre(): 
							For an IRecursiveNode, returns 
								[
									'{type: ..., children: [', 
									'a1, a2, ...]}'
								]
								
						2. jsonInsertablePost(): 
							For an IRecursiveNode, returns 
								[
									'{type: ..., children: [a1, a2, ...', 
									']}'
								]

	TODO: order imports [Shift + Alt + O]

	TODO: Docs
		BEFORE YOU DO THAT: 
			1. finish the `samples`
			2. decide the ultimate fate of `utils`
		
		1. Ensure that ALL the exports are AS-DESIRED (relevant)
			Finalize the library exports, that is
		2. Ensure NOTHING is missing from the wiki: 
			1. utils
			2. interfaces
			3. classes
			4. permalinks
			5. self-referencing: 	
				1. provide links to interfaces for class pages
				2. provide links ot implementations for interfaces to interface pages
		3. finish the JSDoc
			1. change/update old
			2. ADD FOR: 
				1. classes
				2. ALL public exports
					IF a 'util' - a proper short function description. 
					IF a 'class/interface/related' - a brief description + a link ot the wiki

	TODO: Tests
		BEFORE YOU DO THAT: 	
			1. finish the docs [COMPLETELY!]

		0. Remove old tests
		1. TEST the internal/ (only what makes sense)
		2. Test the public exports: 
			1. ALL classes
			2. ALL utils
			3. ALL samples
		3. Test that the LIST of library exports is correct: 
			1. EMPLOY the user-end `deep import` for EACH tested export of the library FOR its tests
				Ensures that ALL the needed exports are available
			2. WRITE tests that verify the lists of exports for each DEEP export (export-tree-test, let's call it...); 

		VITAL NOTE: employ the new `ClassTest + MethodTest(s)` model. 
			Reason: allows for using the `ICopiable` to write tests in a much more concise fashion, 
				treating each class instance as a "test starting point", from which one can test various methods. 
					Saves A LOT of boilerplate that is (usually) written when doing tests. 
		
MINOR TODOS: 			
	TODO: EXPORT the current (AND NEW - create them) "wrapper-functions" around classes as '(...) => T & new (...) => T' 
		via the 'const NAME = OTHER_NAME as any' type-trick. PARTICULARLY, do this for `Stream`s
		1. In particular - ONLY add "function-wrappers" for the `class _StreamClassName {....}`for: 
			1. classes that are INTENTIONALLY to remain 'sealed' (like, say, `DynamicParser` and `Enum`)
			2. classes that are using the Factory pattern 
			3. public classes that AREN'T INHERITED-FROM ELSEWHERE 
			
	TODO: make the 'MapClass' and 'HashMap' BOTH operate on a "local class" [instead of an 'inner-function class']
		Replace Configurator Pattern with 'Generalized Factory-Pattern' [or, weaker form of `Configurator Pattern`]. 
		Again - same thing as with the 'Stream'-s - for JIT's sake + to save (some) memory, 
			since the user will (likely) want to be using the MapClass for their own goals. 

	TODO: simplify the `MapClass` code. 
		Specifically, all the excessive `.prototype` manipulations. 
		They are ugly and unnecessary (well, except maybe ` = plainGetIndex` thingy). 
	
LATER TODOS: 		
	UT4. [maybe???] REMOVE the `utilities`: 
		Since we're going to be adding the `samples`, perhaps it's wise to unite the two???

	TODO [maybe?]: change all the `<... = any>`s in the code to `<... = unknown>`s. 
		Reasons: 
			1. works FAR better with the `&`-role-composition	
			2. SAFER GENERIC TYPES!
		NOTE: *do not* do it everywhere. 
			Reason - may break some code. 	

	3. FINALE OF THE V0.3 DEVELOPMENT: 
		0. DO TDD [since tests will now be more-or-less established...]; 
		1. Regex, TokenStream, ErrorHandler, Eliminator, WriterStream, etc - SEE EXTENSIVE NOTES BELOW

	TODO [one.js] - MAYBE...: LATER, when finishing new version:
		Replace all occurences of `numbers(...).map(...)` with `array.from`; 
		Do via Search: 'numbers(...)'; 
		There is at least one example in `Enum`. 

	NOTES:						
		NOTE [for later]: docs - 'protected abstract' properties...
			DOCUMENT THOSEAS WELL. 
			Specifically, for classes. 
			These are INTENDED to providing the user with ability to customize EXTENDABLE behaviour
				[example: IterableStream, DelegateStream, WrapperStream]; 

		NOTE [for future]: the [Symbol.iterator] inside the `SwitchArray` COULD be a minor bottleneck. 
			Benchmark later...
			Possibly - replace with a callback-utilizing strategy..; 
				[Reason this was not done at the beginning is: it's ambigious which should be faster, 
					since it's hard to control whether or not the passed callback is INLINED or not 
						(so we may end up spending more time due to needing to create the callback)]; 
			
		2.76. [parsers.js] Set up a GitHub Actions -> npm pipeline; ALSO - ADD A GitHub Workflows FILE for updating the latest version, and publishing to npm... + publishing the latest docs [separate]
			ALSO - create a list of presently maintained projects, for which to add such a pipeline: 

		NOTE: About Tests [ADD NEW, REMOVE OLD - scorched earth... again]: 
			1. New test cases bear form:
					suite("ClassName (case #1)", () => {
						ClassTestObject.withInstance(
							new Class(), 
							(instance) => {
								test("ClassName.prototype.methodName1 (case #1), () => 
									instance.methodName1(...)	
								)
								
								test("ClassName.prototype.methodName1 (case #2), () => 
									instance.methodName1(...)	
								)

								...
								
								test("ClassName.prototype.methodName2 (case #1), () => 
									instance.methodName2(...)	
								)

								...
							}
						)
					})

			TODO [maybe?]: create
				1. `TestCounter` class
					for *counting* tests-being-run within the given test suite
				2. `TestChain` class, 
					which has an internal `TestCounter`, 
						has a `handler` function, and 
							"inputs: any[][]" arrays of arguments to be 
								supplied to the `handler`

				USE the `TestChain` to express the chains of tests thusly. 

			2. remove the old `lib.ts` (make current `_lib.ts` into the new `lib.ts`); 
				
	TODO: OPTIMIZING `TypeScript` property definitions on classes: 
			When [in TypeScript], one does: 
				class C {
					prop: P
				}	 

			This DOESN'T ACTUALLY create a property of 'prop', however, doing this: 

				class C {
					props: P = ...
				}

			DOES. UPON CREATION OF THE OBJECT ['constructor' call]; 
			When one needs a PROTOTYPE PROPERTY, use the first variant (type-level only), 
				where it's an INSTANCE PROPERTY, use the second, as doing so wil avoid the 
					necessity for type-transitions in code. 

	2. Workflows [GitHub Actions]: 
		1. make-tag [like in one.js]: 
			1. updates 'package.json' version; via `npm version`
			2. makes tag
			3. commits
			4. pushes
		2. npm-publish [like in one.js]: 
			0. runs the run-tests as a Reusable Workflow
			1. publishes to npm
		3. run-tests [not quite like in one.js, but similar]:
			0. builds the project and tests
			1. runs the tests

		1. AFTER you copy/write those, ADD a GitHub Gist for them: 
			1. make-tag
			2. npm-publish

			__NOT__ the `test` command. 
			Reason: too different from project-to-project. 
			These are the same. []
	
[TOTAL LIST - final + leftovers...] TODOS for v0.3: 		
	Primary: 
		UP2. A new utility function (Parser/utils.ts) - match(word, stream): 
			For a given 'Indexable', it does: 

				// ! PROBLEM: lacks ability to check for MULTIPLE WORDS: 
				// 		* Solution: replace 'matchWord' with a more complex hand-written 'RegExp'-matching engine for the library... [uses 'Stream's instead of JS 'string's]: 
				// 			ALSO - use the *same* engine for the 'RegExpTokenizer'; 
				// rough sketch - requires the '.peek(n)' method; 
				// NOTE: the '.peek(0) === .curr' is ALWAYS a requirement...
				function matchWord(stream, word) {	
					const positions = word.length
					for (let i = 0; i < positions; ++i) 
						if (word[i] !== stream.peek(i)) 
							return 0
					return positions
				}

				// IDIOM: for skipping a word, call it 'function consume(stream, word, wrapper)' - a 'Parser/utils.ts' utility; 
				const matched = matchWord(stream, word)
				skip(stream, matched)
				if (matched) return wrapper(word)

		TO2. [new IStream] `TokenStream` - based off `Regex`: 
			IMPORTANT NOTE: 
				must be CAPABLE of allowing the functions in question to OPERATE upon the `.pos`
					of the stream that is BEING TOKENIZED. 
				Reason: oftentimes, one will not want the contents of the thing, BUT THE POSITION, instead. 
					Thus, the functions in question should (additionally, optionally) be given the `.pos` of the match; 

			1. takes in a table of `[Regex, StreamHandler]` 
			2. iterates the table for each `.curr`
			3. calls `StreamHandler` on the `.value` [takes in a `.value: IStream`]
			4. repeat 1.-3. until `.value` is finished [tokens are produced as `this`]

			This is immensely useful for tokenization via regular expressions; 
				
			IMPORTANT NOTE: `TokenStream` MUST ALLOW FOR ERROR-HANDLING!!! 
				More specifically, it takes: 
					1. an `IndexMap` [or another kind of table... maybe just a `Pairs`] of tokens
					2. then - it walks through its items for a given index, trying to `.match` the respective `Regex` object
					3. IF none fit, IT PROVIDES AN `error-handling` callback, which the user may employ AS THEY SEE FIT! 
						More specifically: 
							1. an instance of an `IErrorHandler`-implementing class, contains a `nextItem()` [optional] and `error()` [mandatory]
								methods, which work like:
									1. if `nextItem(stream, info: IErrorInfo)` is present, it passes the respective `tokenStream`
										to it, and `nextItem` skips the respective nodes until an acceptable one can be 
											obtained via `stream.curr`. 
										
										The return value of `nextItem` is a new interface - `IPositionRange`:
											it's a pair of positions: [IPosition, IPosition], like [for instance]: 
												1. [ILineIndex, ILineIndex]
												2. [MultiIndex, MultiIndex]
												3. [number, number]
												4. [IPositionPredicate, IPositionPredicate]
												...

											TODO: pass an `IPositionRange` to the `LimitedStream`; 

										The returned `IPositionRange` would be correspondently treated 
											by the interface in question. 

										For the case of `TokenStream`, 
											it is [basically] a `[ILineIndex, ILineIndex]`. 
											The `IPositionRange` is then passed to an (optional)
												`.handleErrRange(this.value.buffer, range: IPositionRange)` method on the 
												Also, the `this.value.buffer` is the UNDERLYING 
													buffer on the underlying [used by the 'TokenStream'] `IStream<string>`; 
													It (basically) allows one to process the given range of positions 
														"the right way". 

										The `IErrorInfo` is a `number` - error code, 
											for one of possible errors that can ocurr. 
											[It's intended that the user will use an `enum` of `number`s for this]
									2. call error()

							[data access]
							1. is kept upon a SPECIAL class/interface-instance called `ErrorHandler`: 
								1. the `handler` has access to various state: 
									it can be respectively pre-configured  by the user
								2. `TokenStream` can have the `ErrorHandler` used INJECTABLE
								3. The "base" `ErrorHandler` would have EXTENSION-CLASSES: 
									1. [dynamic, via getter] `lineIndex` [use `dig + .value` - would keep the respective "top" Stream, from which to keep counting]
										Useful for error-throwing inside the `.error()` method; 
									2. [dynamic, via getter] `errorNode` [use `.value + .curr` - would keep the respective `Stream`, while also SERIALIZING the `Node`, 
										at which the error has happened and/or use its `.type`/`.value`]; 
										
										Useful for error-throwing inside the `.error()` method; 
									3. [static, via prototype - Configurator pattern] `nextItem()`: 
										This would be user-defined. 
										Allows, particularly, to work with

									4. [static, via prototyep - Configurator pattern] `handleErrRange()`: 
										This is user-defined. 

						ALSO: this behaviour MUST BE INJECTABLE. 
							Reason: the user may want to keep the tokenizer AS A PROPERTY in their own 
								`IStream`, which would be responsible for handling errors. 

					5. generalize the "TokenStream" error-handling mechanism that employs the `ErrorHandler` object, 
						and employ (generally): 
							1. uses an `ErrorHandler` with (optional) `handleErrRange`, and (optional) `nextItem`, 
								and (mandatory, if neither other are present) `error`
							2. uses an arbitrary `IStream & IBufferized`
							3. uses an `IErrorDispatcher` - any function that can return a `type IErrorInfo<Type = any> = number | Type`: 
								more specifically, these (in practice) would be `TableMap`-s, returning either: 
									1. the `IErrorInfo` (whatever it is)
									2. the `Type` (on success)
										There should be a clear `isErrorInfo` predicate to distinct one from the other; 
										Upon `IErrorInfo`, one has the `ErrorHandler`
											dealing with it (in the said fashion, working with the `buffer`-ized `Stream`, 
												which is PROCESSED by the `ErrorDispatcher`, which ITSELF is a 
													`TableMap` for the current `IStream`; 

													Then, one needs a new kind of `Stream` for this kind of thing. 
														Call it `ErrorStream(dispatcher)(buffer, pos)` [configurator pattern]
													
													Then - one can put it into a `CommonParser`, 
														and use with a `CommonParser.prototype.init(...)`; 
														That is where creation of instances of `ErrorHandler`
															would occur. 
											). 

								PROBLEM: with using `IErrorInfo = number`: can't handle `Type = number` then. 
									Solution: allow generics? Think of something, this is nigh-trivial...
				
			IMPORTANT NOTE: 
				1. The `TokenStream` must have a candidate-elimination system. 
					More specifically, consider the following (failing) implementation strategy: 
						1. one uses `.peek()` [alone] to determine which token to accept next
						2. due to that, since `peek()` is *limited* by length of the `RotationBuffer`, 
							one has a limitation on size for tokens-'.value's
						3. consider good ol' /[a-zA-Z]/; This has arbitrary length. qed
				2. Thus, one needs a combination of `.peek()` and `.next() + .prev()`. 
					Thus, we do: 
						1. peek(n) for MULTIPLE of them
						2. keep going, until one of: 
							1. the set of those that can be acceptable is 1
								1. switch to `.next()/.prev()` [.prev() ONLY if there's an error, and this (ends up) not being a match either]; 
							2. we run out of `n` for `.peek(n)`
								1. switch to `.next()/.prev()`
								2. `.prev()` ONLY when an error is encountered. 
					
					Thus, the candidate elimination system: 
						1. no backtracking
						2. max-possible-usage of `.peek()`
							once out-used, `.next(n)` for max `n` for `.peek(n)`, 
							and then continue with `.next(1)`
						3. if no matches are found, backtrack to the beginning using `.prev(k)`, with total chars `k`, 
							and herald error
						4. else, continue until there's only 1 viable candidate
						5. IF there are 2+ viable candidates, and one is fully reachable sooner than others, then IT is chosen
						6. IF no single viable candidate can be produced as per rules 1-5. (id est, there is 2+), 
							throw an exception [this is a PARSER error, not the input error, end-user shouldn'e need to worry about it, 
								the parser-maker (direct library user) DOES
							]
	
		TODO [*important*]: About `TokenStream` and `Regex`: 
			1. need an INTERMEDIATE abstraction - a `MatchList`
			2. a `MatchList` is a "table" of `handler-Regex` to be walked, and USED by the `TokenStream` to MATCH one of avaiable `Regex`-es
			3. a `MatchList` is an `IndexMap`; Functions in it have access to `this` - THE CURRENT TokenStream, and `input` - first arg, and 'parser.table' - second arg, LIKE with `TableMap`; 
			4. a `TokenStream` is just a `(matchList: MatchList) => StreamClass(TableMap(matchList))`
			5. since `TokenStream` is JUST a `StreamParser`, it ALONE cannot provide one with full `laziness` in all cases (ex: recursion)
			6. FOR ENABILING full laziness, one NEEDS choosers, since they allow one to parse items of arbitrary depth: 
					b + a + (b - (...)), ... # and so forth - only characters are: a, b, +, -, (, ) and ','
				can be parsed like: 

					# very rough sketch - incomplete, need to solve the issue of DEALING with the `recursive ()`-aspect of it all. 
					1. ReadableSource
					2. InputStream
					3. SkipSpaces [in `samples`]
					3. TokenStream(+, -, a, b, (, ), ,) - provides TokenNode abstractions
					4. chooser0:	
						1. TRY FINDING a matching pair for '/(/': 
							input.next() # skipping '('
							return [
								SingletonStream(BRACKETED_RecursiveNode_func), # a RecursiveNode, with 1 element - the sum; 
								chooser1, # "gathering up" all the "+", etc
								chooser0, # recursion - bracket-expression INSIDE the bracket
								LimitedStream(EndBracketTokenPred),  # predicate of '.type === EndBracketType'
							]	
						2. all else - PASS AS-IS
					5. chooser1: 
						1. based off TableMap:
							1. based off an MatchList:
							
								0. MATCHING - Peek(1) (approx, in `.type`s) -- 1/[+-]/: 
									const elem = input.next() # a | b | (...) - last one hadled by `chooser0`
									const sign = input.next() # - | +

									# the `Summand_Curried(elem, sign)(elem1) = { type: sign, children: [elem, (elem1.type == sign ? ...elem1.children : elem1)] }`
									# we are TURNING the `Stream`-s into a SEQUENCE of ONE-ELEMENT LAYERS TO BE EVALUATED LATER; 
									# which are THEN - flattened to obtain a single `.children` array for the arithmetic expression; 
									return [
										SingletonStream(Summand_Curried(elem, sign)), 
										chooser1,
									]

								1. MATCHING - .curr 0/[ab]|#[BRACKET_TYPE]/ [with no prev. match of 'peek(1)']
									const elem = input.next() # skipping; NO +- AFTER the current one; Has to be end on the next [or, an ERROR]; 
									return [
										SingletonStream(Summand), # `Summand` - a RecursiveNode with `.children.length === 1`
										chooser1 # end-recursion - 'IS_END'/Error ahead [NOTE: this needs to be SUBSET - break into `chooser1` and `chooser2` (lesser - without recursion)]
									]


								3. IS_END: 
									return []

								4. is ',' - return a `SingletonStream(id)` - passing those as-is; 

								5. ERROR/ERROR-RECOVERY - invalid token provided

						2. EACH TIME that the `chooser1` is FINISHED, we have successfully parsed AN OPERATION
					6. PredicateStream - pick those that are not a ','
					
		CONCLUSION [about demos]: 
			5. IMPORTANT : provide proper demos for: 
				1. ab, +-, () and , arithmetic (like the one above) - USING `DynamicParser`
				2. SELF-MODIFYING thing [using `DynamicParser`]
					THIS should be made into a *test* [demo, more precisely, an INTEGRATION test]; 

		RE2. `Regex` description:
			[maybe] - create your own flavour for 'RegExp's
			IMPORTANT NOTE: 
				1. must have a library-specific feature of targetting the `.type` property. 
					More specifically, add a '{matched_type}'-Regex-construct for matching a token with a 
						type `matched_type`. 
				2. the types here must be either STRINGS or NUMBERS. 
					More specifically: 
						1. i{...} - matches a type of number
						2. s{...} - matches a type of string

			Then: 
				1. [maybe?] add new convinience functions to `regex` to also handle your flavour...
				2. create `fromRegExp` util, which would create a `Regex` [library] from `RegExp` [JS builtin]
				3. write proper docs for the class

			It will improve upon the "simple" JS regex in the following ways: 
				0. replace the first and last items "/^$/" with /:#/
				1. add a general `^x` for NEGATING a pattern. Thus, elementary [^...] becomes composite ^[...]
					This way, for instance /^ab+/ matches "bbb", but not "ab". 
				2. add a general `(){n, k}` - limits input from 'n' symbols to 'k' inclusively
					to NOT grab it, use '(:...){n, k}' [basically, the same as the *ordinary* '{n, k}']; 
				3. remove the `\B`, `\D`, `\S`, (?!...), (?<!...), ..., other negative-classes/assertions in favour of the simpler `^x` form
				4. replace the (?:...) with (:...) [syntax simplification]
				5. provide different '\\x' characters (obviously, due to changed syntax); make the escaping redundant for the rest
				6. provide A SINGLE '\u{}' instruction instead of that horse**** with \X, \u and \c that JS has
					Will span the max possible values of the Unicode range
				7. get rid of the flags: 
					1. 'g': 
						replace the different behaviour with METHODS; make it non-exclusive

					2. 'v':
						remove, provide its functionality by default: 
							1. `P` not working - it is removed and replaced by `^p{...}`:
								By default, consumes the max possible string (remaining, if needed). 
								Limited by (...){n, k}

					3. 'i': 
						make it a new group type - that matches A GROUP regardless of the case: 
							Ideas for syntax: 
								
								1. /(#i...)/; 
								2. /(?i...)/; # more traditional...
						
						ALSO: make it possible to *inject* custom functionality 
							(utilizing the fact that `RegExpTokenizer` CAN be generic); 

					4. 's': 
						make it default
					
					5. 'u': 
						remove, included by default with `v`

					6. 'y' (????): 
						find out what this even is (and what it's used for)
						seems very much like a hack for 'regex.match/exec/search/whatever(string.slice(x))'...
				
					7. 'm': 
						remove - unnecessary
						possible to achieve the same thing using a slight modification of:
							/(?<\n)...(?=\n)/
					
					8. 'd': 
						too specific, ugly
						make these kinds of behaviours GENERIC instead
						[note: the 'RegExpTokenizer' WILL be generic...
				8. get rid of form-feed (`\f`): 
					Reason: not useful for parsing
				9. ALLOW the space in '{a,b}' (becomes '{a, b}'); 
				10. Add a possesive quantifier "*+", "?+", "++": 
					https://www.regular-expressions.info/possessive.html
				11. Add atomic groups: 
					https://www.regular-expressions.info/atomic.html
				12. Double negation inside a class: 
					^[^\w] is the same as (in JS flavour) [^\W]

					That is, it is COMLEMENT OF A CLASS of items that are NOT A CHARACTER.
					That is, it's \w. 
				13. Add relative backreferences: 
					https://www.regular-expressions.info/backrefrel.html
				14. Fix the ECMAScript behaviour of empty-matching backreferences: 
					https://www.regular-expressions.info/backref2.html
				15. Add the conditionals: 
					https://www.regular-expressions.info/conditional.html
				16. Regarding the /(#)/ - modes: 
					https://www.regular-expressions.info/modifiers.html
				17. Add subroutines (recursion): 
					https://www.regular-expressions.info/subroutine.html

				IDEA: use this regex-flavour for the HammockLang implementation later...	
				
				IMPORTANT NOTE[1]: 
					1. `Regex` works with BOTH: 
						1. string
						2. IStream<string>: 
							reason - to allow implementations for faster/lazy algorithms

				IMPORTANT NOTE[2]: 
					1. .matchAt(x: Indexed<string>, i: number)
						Checks that AT THE GIVEN `i` IN `x`, 
							there is AN IMMIDIATE MATCH. 
						This is important, as it allows the "tabular" work that the TokenStream does
					2. .sub(from: number, to: number) - returns a new Regex that is the result of LIMITING the current one
						between `from` and `to` indexes for VALID TOKENS. 

						This way, for instance: 
							(a(bc)+) has 4 tokens, ordered as: 
								1. (...) - outer bracket
								2. a 
								3. (...) - inner bracket
								4. + [relative to the outer bracket]
								5. b
								6. c

						Thus, recursion is handled BY THE FIRST OCCURENCE. 

					ALSO, add the JavaScript string-compatibility methods: 
						1. [Symbol.match]
						2. [Symbol.matchAll]
						3. [Symbol.replace]
						4. [Symbol.search]
						5. [Symbol.split]

					ALSO [imporant]: have ALGEBRAIC methods for it. 
						Meaning: ability to join them via a disjunction '|', 
							applying the quantifiers '*', '+', '{a, b}', '?', 
							applying negation '^'. 
						Each of those CREATES A NEW 'Regex'! 
						Purpose is to (basically) replace the `regex` module. 
					
					ALSO: the `Regex` implements an NFA

		EL1. Eliminator: 
			Replacement for the old `Eliminator`, based off `Regex`; 
			Lazy, doesn't need several passes. 
			Takes in the `Regex` list [walks, for given "character", each of them, matching, eliminating what was matched...]; 
				Eliminates ONLY the first thing....; 	
				
	Secondary: 			
		RE1. 'regex' - allow for methods for creation of regular expressions that use the extended regex-syntax (enabled by the 'v' flag); 
			Namely: 
				1. intersection: 	/[[...]&&[...]]/v
				2. subtraction: 	/[[...]--[...]]/v
				3. string literals: /[q{...}]/v
		
	Minor: 	
		TY2. Type simplification - the `array.Pairs` overuse: 
			Rely more upon the `Iterable<[KeyType, ValueType]>`, where possible. 
			Specifically - when the [Symbol.iterator] is the only actually needed property.
				
	IMPORTANT: clean the git history - keep only the commits for each individual version; 
		Squash all the others + change the respective commit messages; 
		[do this via `git rebase -i` + `git push --force`]

	TODO: WRITE the CHANGELOG
	
[v0.4]

TODO: 
	0. General notes (see these before anything else): 
		TE1. Do TDD this time
		JD1. [IMPORTANT] Expand JSDoc: 	
				Read more about `JSDoc` in TypeScript at:
					https://www.typescriptlang.org/docs/handbook/jsdoc-supported-types.html	

				1. use it more thoroughly (give higher priority for classes and so forth - not just a brief description + Wiki link)
				2. expand its contents (argument types, @type, self-reference, etc)
				3. [IMPORTANT] document early - another one of the banes of parsers.js: 
					Due to the fact that development was (mostly) content- and feature- oriented, 
						one very much forogt about importance of spending appropriate amount of 
							time on documentation. Hence, one (slowly) started to forget what one 
								was doing, and whether it even (sometimes) made any sense. 
					Had there been documentation, certain specific poor choices may have been more avertable, and 
						some little time could have been saved. 
						
	1. Primary [new ideas, modules, interfaces/classes, algorithms]	
		TS1. DepthStream: create a new version - children-first traversal [call it `ChildrenStream`]; 
			The 'DepthStream' (currently) is the 'LL' pre-order tree traversal algorithm Stream-implementation; 
			There should be ANOTHER - the 'LR' post-order tree traversal; 

			1. Create a new 'IStream' implementaiton for this - 'PostStream'; 
				This is (particularly) useful for some types of interpreters; 

				The Pre visits the node, then its children one-by-one starting from the beginning, then the siblings; 
				The Post visits first the children, then the siblings, only THEN the parent [when there are NO MORE children - then the parent is visited]; 

				Basically, Pre (parent-to-children) returns the nodes 'as they go' in the left-to-right traversal in the tree,
					whereas Post returns them in 'reversed' order: for the parent to return, children must all return first; 

				[Example usage: transform the binary '+' -- a + (b + c * (d + (k + f * r))) -> a + b + c * (d + k + f * r)]; 
				
		TC1. Create an 'ASTAnalyzer' class [under new 'Node' module]: 
			1. Takes in an `INode`
			2. Traverses the tree
			3. Returns information: 
				1. [Buckets-Categorization] Arrays of '.trivial', '.content' and '.recursive' nodes
				2. [Type-Categorization] Returns a 'HashMap' of arrays of nodes, filtered by their '.type'; 
					NOTE: the 'HashMap' is, TOO, completely configurable by the user [user-defined (if provided), else - default being: when 'string's are used for 'type's - ObjectHashInternal, otherwise - HashMapInternal]; 
						(another function-argument for returning the final ASTAnalyzer class)
	
			4. Provides functionality [ASTAnalyzer]: 
				1. mapTypes((type_name: string) => string): 
					creates a new 'ASTNode', by means of mapping the '.type'-s
						of the current one using the given function. 
					Immensely powerful for changing between different data formats. 
	
				2. mapValues((x: any) => any)
					creates a new 'ASTNode', by means of mapping the '.value's of each 'ContentASTNode', 
						while preserving all else
				
				3. .find(type: ..., pred: (x) => boolean): 
					Seeks an item in the tree with a type '.type', obeying the predicate '.pred';
					Optimizes to look specifically in a '.type'-bucket of the given 'type'. 
					Can be radically faster for large inputs. 
				
				4. .find(pred: (x) => boolean): 
					if the 'type' is not passed, the entire tree is searched; 
					Optimization - uses prior obtained collections for increasing cache locality of large searches. 
				
				5. .iterate(type) - returns a Stream, filled specifically with values of type 'type'; 
					Namely, it returns an 'InputStream', wrapped around the respective '.type'-bucket; 
					The items are listed on a from-beginning-to-end of the 'Stream' used to construct the ASTAnalyzer; 
	
				6. .filter(pred) - returns a new tree, such that it ONLY contains elements that obey the given predicate: 
					HOWEVER, one can also mandate that in order for a sub-tree (RecursiveASTNode) to remain, 
						at least one of its children must be 'true' as well. 
						This is done by means of returning `null`, instead of `true`/`false`.  
				
				7. .search(type: any) - searches, and returns for multi-index information for each of the elemenets of a given type. 
					Optimization: uses ONLY JUST the information from a given '.type'-bucket; 
					This allows one to: 
						1. Iterate a portion of the tree INSTEAD of needing to start "at the beginning
						2. Know exactly when to stop (seeing the *last* item that needs to be iterated over)
	
				8. .map((x: ContentASTNode | TrivialASTNode) => any): 
					Maps all the non-RecursiveASTNode parts of the tree to a different 'Tree'. 
					Useful for creation of generalized non-AST trees, to be used with 'TreeStream': 
						Example, passing a function that returns 'string', then - CONCATENATING all 
							the items inside the obtained Tree, via: 
	
								// [sketch - no types]
								function GenerationFunction(F) {	
									return function generate(ast) {	
										return array(TreeStreamPre(ast.map(F)), new UnfreezableString()).get()
									}
								}
								
								// this is to create either a SUBSET defined by a recursive IndexMap-like function, *or* via working through a '[type]'-defined set of "superior" nodes; 
								// NOTE: 'ast.types[type]' is an *ARRAY*
								function GenerationRecursive(F, type) {	
									return function generate(ast) {	
										return array(InputStream(ast.types.index(type).map(F)), new UnfreezableString()).get()
									}
								}
		
					Also - '.map(F)' OPTIMIZES, so, it expects the given values to be returning FUNCTIONS, for plugging in their children's 'F(x)', 
						so: 
	
							1. A -> B
							2. F(A) -> X(K): X(B) == string
							3. F(B) == string
								F(A)(B); 
							
					The optimization is - SPLITTING the '.types'-buckets via doing '.types[name].map(...)'; 
				
				9. RecursiveASTNode: 
					1. Contains various copying .value-Array-delegate methods: 
						1. .map(f) - creates a new RecursiveASTNode via 'new RecursiveASTNode(this.type, this.value.map(f))'
						2. .filter(f) - creates a new RecursiveASTNode via 'new RecursiveASTNode(this.type, this.value.filter(f))'
	
			5. Optimization information (additional): 
				1. IDEA: add a new 'ASTHierarchy' information object, which: 
					1. Specifies, what '.type'-s CAN be found within which '.type'-s of RecursiveASTNode-s: 
						IMMENSELY powerful for search-optimization inside a given Tree 
							[limits the '.type'-buckets in which one needs to look]; 
					2. Specifies maximum "depth" of a certain particular 'RecursiveASTNode': 
						1. Allows one to replace a continuous check of 'isGoodIndex(.lastChild)' with 'isGoodIndex(.lastChild) && i < MAX_DEPTH_COUNT'; 
							Guarantees a performance bound. 
							When the bound is PRECISE, the 'isGoodIndex' can be dropped
					3. Specifies maximum length for '.children' of a given 'RecursiveASTNode': very useful for iteration! 
						1. Replaces '.children.length' check with a 'counter < FIXED_PRECOMPUTED_LENGTH'; 
					4. Specifies expected structure for a given 'RecursiveASTNode': 
						1. May permit one to be going one-by-one downwards for it, EXPECTING the given '.type's; 
						2. Eliminates checks for '.isSiblingAfter'
	
					'ASTHierarchy' is optional, and its purpose is solely to: 
						1. provide algorithm optimizations
						2. enforce bounds for a given format
	
					NOTE: 'ASTHierarchy' is ONLY good for optimization, when IT IS PRECISE 
						[id est, we can SKIP certain function-calls/checks]. 
	
				NOTE: optimizations work via: 	
					1. ASTStream-s: 
						0. Alternative to 'TreeStream': less generic, provides better optimizations for AST: 
							1. ASTStreamPre - alt. of TreeStreamPre
							2. ASTStreamLevel - alt. of TreeStreamPre
							3. ASTStreamPost - alt. of TreeStreamPost
						1. Accepts an 'ASTNode' instead of a 'Tree'
						2. Optimizations: 
							1. No '.lastChild' presence check in '.isChild()' method - instead, just '.lastChild > -1'	
								This is a good optimization for TreeStream-iteration of large trees ('.isChild()' is called ON EVERY '.next()' call); 
							2. Algorithm Configurable via 'ASTHierarchy':
								1. Optimizations are present ONLY when ASTHierarchy is PRECISE
								2. Expects a VALID abstract syntax tree
	
				2. For '.type's field values, one uses EITHER an 'ObjectHashInternal' (if strings), or a 'MapHashInternal' (if not strings); 
		
		ME3. [IStream] Iterator Helper functions - '.filter', '.take', ...: 
			Provide them. 
			Use them in the library definitions that actually EMPLOY them [example: PositionalValidator would benefit greatly from a single '.filter()' call]; 
			These will (each) require individual internal-class implementations + public interfaces to use.
				Reason for "internal" nature of these classes: one doesn't want to allow user to create them on their own
		
	2. Secondary [new utils, methods]
		UT1. bufferize(tree, size) - util [Tree/utils.ts]
			Given a 'Tree', returns an array of values of size `size`: 

				{
					childNumber: number,
					value: any
				}

			The `size` is a fixed number >= 0, 'childNumber' is the number of children, ahead of current node in the array.
			To be used for creating a persistent linear data structure for working with the given AST. 
			Good for cases, when the number of items is known, and is large (part of the the "big input" optimizations); 

			Would permit user to store a '.state.size: Pattern<number>' property (Pointer(number)) on the ParserState-s, 
				for counting the size of the input in terms of nodes, then re-use it with 'bufferize', 
					and use the tree for evaluation; 

			If the 'size' is unknown, implements a simple DFS (namely - conversion of a given 'Tree' into an 'Array')
			
		UT2. util - enumerateTree [Tree/utils.ts]
			Given an array of "types" of nodes, it recursively converts their properties (given in the 'shapes' array), 
				to single-array-siblings form (the current ChildrenTree); 	

			This would permit a more "variable" set of trees (those that have properties with fixed names), 
				to be enumerated accordingly...; 

			Example [1]: 

				{
					type: any, 
					a: any
					b: any
					c: any
				}

			Becomes [1]: 

				{
					type: any, 
					value: [
						// ... a, b, c
						// ... or b, a, c
						// ... or any other order
					]
				}

			Example [2]: 

				{ type: any }; remains the same
			
			Example [3]: 

				{ type: any, namedProp: any }; becomes { type: any, value: any }; by changing the property name to 'value'

			For this, define a 'TreeKind'/'TreeShape' - with a list of properties to be used/transformed; 
			More specifically, it creates a tree of `Node`s, from a tree with a given shape. 
			
		UT3. Add more complex 'Node' utilities. Based off previous 'one.js' v0.3.1 [the removed methods], add: 

			1. deepSearch - searching inside the given Tree for a value with a given 'prop'
			2. depth - calculating the depth of the given Tree
			3. treeCount - counts the values of a given predicate for all items in the tree

			ALSO: Add new methods to 'RecursiveNode': 

				1. .reverse() - recursively reverses the given tree
											
		CU1. Add a 'binarySearch' utility: 
			1. For IndexBuffer - as it keeps 'number's in an ordered fashion, there is no reason not to employ it...; 
			2. This is generic (meaning, it provides a map 'f', and then compares via 'f(a, b)', with default being '(a, b) => a < b'); 
								
	3. Unrelated/separate module/grand refactoring: 		
		DP1. [Unrelated - later, spread to appropriate note-files] Deployment Pipelines for other maintained projects: 
			1. draw-text [install-npm + prod-to GitHub Pages]
				Also - ADD THE NPM DEPENDENCIES THAT IT REALLY REQUIRES - *INCLUDING* 'parsers.js'
			2. selector [to npm]
			3. regex [to npm]
			4. xml [to npm]

	4. Docs: 
		WE1. CREATE A proper website with documentation for the library. 
			Do benchmarks, et cetera...; 
			After doing GitHub Wiki for v0.3, see if it cuts it (spoiler - it likely won't one STILL wants the ravishing types for the library's docs to be present!)
				For this: 

					1. Learn a new CSS-based tool (either for generating/hosting docs/docs-css-styles like TypeDoc/readthedocs , 
						OR a new library to create one's own doc-styles like 'Tailwind CSS'); 
					2. Create a JSON parser in parsers.js and COMPARE it against the [https://chevrotain.io/performance/], with ops/sec. 
						See, WHICH libraries have managed to beat parsers.js v0.4., and WHAT can one do about it (see their optimizations, specifically...); 
						2.1. ALSO: create a benchmark for MEMORY USAGE [more concretely - compare parsers.js with the others...]; 
							The memory usage should be noticeably better than speed...; 

		JD1. JSDoc-s: add references
			Particularly - add the links to documentation website INSIDE the JSDoc (currently, lacks, the user has to look up themselves). 
			Reason (that it lacks currently, as of v0.3): time concerns

		JD2. Use a more complex (high-quality) JSDoc
			Current one is primitive (time concerns). 
			Add tags: 
				1. @returns
				2. @type [JSDoc-level self-reference for types]
				3. ...

		IN2. REMINDER: about the `init` method output type: 
			After it is generalized to a SINGLE interface, make FOR ALL `.init`-having method
			so that `init` returns `this`. 
	
		WI1. Add usage examples: 
			Current API is quite complex considering the need for generic functions (in many cases). 

		WI2. Add proper Guides: 
			Currently, the API code is pretty brutal. 
			Doesn't explain stuff, only contains crucial-to-be-used notes, 
				and general explanations. 
			The API *IS* simple (ultimately), but it is better to be explained,
				because otherwise quite a lot of things may seem cryptical and/or 
					unnecessary

			1. Separate the API documentation from Guides
			2. Guides would include: 
				1. Examples of code for mini-projects: 
					It's going to be one of:

					1. A JSON parser [using 'DynamicParser' - non-self-modifying]: 
						Reasons: 
							1. simple to implement
							2. simple to test
							3. can unify the code
							4. not really useful (as builtin JSON.parse exists; e.g. no reason to refactor to another repo, besides tidyness...)
				2. Terminology pages: 
					Originally intended to be part of v0.3, these have gotten way out of 
						hand, and no longer feasible without some considerable effort (for which there really isn't much time...); 
				3. Samples page: 
					This is a Guide to the `samples` module. 
					Will list the "out-of-the-box" cases for usage 
						and link to respective `samples` module 
							exported definitions. 

		WI3. Remove links to old docs (v0.3 and prior...): 
			It's good, but it's old, and... 
				well, the library wasn't quite formed yet. 
			Constant compatibility breakages, no firm direction of development, 
				constant change of idioms employed, 
					it's basically (almost) like a whole new project now
						(difference being - now it's got a lot more types, and good swe practices: tests, SOLID, etc...)

		WI4. about `fault tolerance`, 
			the user can (themselves) create special "Error" types, 
				which are respectively handled by HashMap/IndexMap-based TableMap-functions; 

			Add a Guide for that.

	5. Testing: 
		1. introduce mocks [use Jest]
			Reason: complexity. 
				There's still v0.4 to go (proper BFS implementation, for instance...); 

	6. Research (Chevrotain - source, capabilities, optimizations): 
		1. Compare performance with Chevrotain benchmark (the JSON parser example): 
			1. *possibly*, if the libraryperformance is really bad, do something about it...
		2. [yes, that thing also...] https://github.com/Chevrotain/chevrotain/blob/master/packages/utils/src/to-fast-properties.ts#L2
			Supposedly, it (significantly) speeds up property access on classes. 
			Benchmark...
		3. [building v8] d8 - USE IT to profile the optimizations (lack of) of the used library code [reflect upon, and change the library respectively]
			Do research (V8 performance): https://floitsch.blogspot.com/2012/03/optimizing-for-v8-introduction.html

FUTURE: 
	1. REFACTOR [some] of the `internal/exported` APIs into SEPARATE npm PACKAGES: 
		0. `copiable` package: 
			A TypeScript package. 
			Contains the `ICopiable` trait. 
			Make a dependency of this. 

		1. `test` package [already developed]: 
			1. add the current 'ClassTest', 'MethodTest' 
			2. make `copiable` a dependency of this: 
				1. reason - relies upon copiability of a given class for its instances to be 
					'ITestable' - a new interface to export
			3. [unrelated] add serialization of tested items [interfaces for it, etc]
			4. [unrelated] add 'Snapshot' API (similar to `Jest`)

		2. `autocache` package [make a dependency of parsers.js]: 
			1. Autocache
			2. IIndexable
			3. ISettable
		
		3. `serializable`: 
			1. `ISerializable` interface
			2. `ISerializableObject` interface

	2. [maybe? later, definitely] A Java rewrite: 
		the project is a very fine one, a very fine one indeed... 
		Howeeeverr...
		One cannot (for instance) utilize it for projects that do not allow the usage of JavaScript. 
		Solution: 
			1. rewrite in *name-of-THAT-Big-project* [once finished]
			2. decompile the JS sources [obtained from TypeScript]
			3. change the sources
			4. transpile to Java [that is the alternative language of choice]
				That is without the tests of course. 
				Those you'll need to write anew (using something like JUnit, maybe?)
				THE `.copy()` aspect WILL STILL REMAIN!!!
			5. write new tests [Java]
			6. publish
		
		Also, that is NOT to notice that the `Java` version would leverage the OOP principles far 
			better than the current JS code (since it's been transpiled from TypeScrtipt). 

	3. Reminder: do you rmember the stack.js? 
		Old (partially abandonned) project for using "stackless" code?

	4. (maybe) later - ensure JS-runtime-independence: 
		Verify that the package has compatibility with: 
		1. Deno [v2.0+]
		2. Bun [v1.0+]